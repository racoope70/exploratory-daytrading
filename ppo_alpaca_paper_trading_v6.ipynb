{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/racoope70/exploratory-daytrading/blob/main/ppo_alpaca_paper_trading_v6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ga_fnXP5kxE",
        "outputId": "279ec709-39c5-482e-d48d-47021a9b630a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping stable-baselines3 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping shimmy as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: gymnasium 1.2.3\n",
            "Uninstalling gymnasium-1.2.3:\n",
            "  Successfully uninstalled gymnasium-1.2.3\n",
            "Found existing installation: gym 0.25.2\n",
            "Uninstalling gym-0.25.2:\n",
            "  Successfully uninstalled gym-0.25.2\n",
            "\u001b[33mWARNING: Skipping alpaca-trade-api as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping alpaca-py as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: websockets 15.0.1\n",
            "Uninstalling websockets-15.0.1:\n",
            "  Successfully uninstalled websockets-15.0.1\n",
            "Found existing installation: PyYAML 6.0.3\n",
            "Uninstalling PyYAML-6.0.3:\n",
            "  Successfully uninstalled PyYAML-6.0.3\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m944.3/944.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires gym<=0.25.2, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.5/122.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.2/172.2 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.9/184.9 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.25.1 requires PyYAML<7.0.0,>=6.0.2, which is not installed.\n",
            "google-genai 1.64.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 16.0 which is incompatible.\n",
            "google-adk 1.25.1 requires websockets<16.0.0,>=15.0.1, but you have websockets 16.0 which is incompatible.\n",
            "gradio-client 1.14.0 requires websockets<16.0,>=13.0, but you have websockets 16.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.5/182.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m767.5/767.5 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Reset\n",
        "!pip uninstall -y stable-baselines3 shimmy gymnasium gym alpaca-trade-api alpaca-py websockets PyYAML\n",
        "\n",
        "# Match training stack (per your system_info.txt)\n",
        "!pip install -q \"stable-baselines3==2.7.0\" \"gymnasium==1.2.0\" \"shimmy==2.0.0\"\n",
        "\n",
        "# Trading deps\n",
        "!pip install -q \"alpaca-py\" ta python-dotenv gym-anytrading pywavelets yfinance\n",
        "\n",
        "# Only force these if you actually hit websocket/yaml issues\n",
        "!pip install -q --upgrade --force-reinstall \"websockets==15.0.1\" \"PyYAML==6.0.2\"\n",
        "\n",
        "import os; os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxNoUwLsHskD",
        "outputId": "abec1192-1068-4a90-bd4f-3f2a2f977014"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15.0.1\n",
            "6.0.2\n"
          ]
        }
      ],
      "source": [
        "import websockets, yaml\n",
        "print(websockets.__version__)\n",
        "print(yaml.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZmdUTXz5nQg",
        "outputId": "3b8e7cbc-696e-4cb9-f183-f0d635527213"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch: 2.10.0+cpu\n",
            "gymnasium: 1.2.0\n",
            "shimmy: 2.0.0\n",
            "stable-baselines3: 2.7.0\n",
            "alpaca-py: 0.43.2\n",
            "websockets: 15.0.1\n",
            "yfinance: 0.2.66\n",
            "pywavelets: 1.8.0\n"
          ]
        }
      ],
      "source": [
        "import torch, gymnasium, shimmy, stable_baselines3 as sb3\n",
        "import alpaca, websockets, yfinance, pywt\n",
        "\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"gymnasium:\", gymnasium.__version__)\n",
        "print(\"shimmy:\", shimmy.__version__)\n",
        "print(\"stable-baselines3:\", sb3.__version__)\n",
        "print(\"alpaca-py:\", alpaca.__version__)\n",
        "print(\"websockets:\", websockets.__version__)\n",
        "print(\"yfinance:\", yfinance.__version__)\n",
        "print(\"pywavelets:\", pywt.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cM2YQVmcZjEu",
        "outputId": "9b089644-2b98-47e5-e82c-696a14f35f21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "2026-02-27 17:37:37,683 | INFO | No duplicate (1) artifacts found.\n",
            "2026-02-27 17:37:37,689 | INFO | [COLAB UPLOAD] Please upload your .env file (contains Alpaca API keys).\n",
            "2026-02-27 17:37:37,690 | INFO | [COLAB UPLOAD] If you already have /content/drive/MyDrive/AlpacaPaper/.env, you can cancel/skip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2cb1749a-1001-4f8c-a2e0-5ff3e1c3c3b2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2cb1749a-1001-4f8c-a2e0-5ff3e1c3c3b2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Alpaca_keys.env.txt to Alpaca_keys.env.txt\n",
            "2026-02-27 17:37:49,783 | INFO | [COLAB UPLOAD] Saved .env -> /content/drive/MyDrive/AlpacaPaper/.env\n",
            "2026-02-27 17:37:49,784 | INFO | [COLAB UPLOAD] Now upload PPO artifact files (model .zip, vecnorm .pkl, features .json, etc.).\n",
            "2026-02-27 17:37:49,786 | INFO | [COLAB UPLOAD] Target artifacts directory: /content/drive/MyDrive/AlpacaPaper/artifacts\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ac53bf3d-4079-4250-96a5-d043d4c4ea09\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ac53bf3d-4079-4250-96a5-d043d4c4ea09\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ppo_GE_window1_features.json to ppo_GE_window1_features.json\n",
            "Saving ppo_GE_window1_model_info.json to ppo_GE_window1_model_info.json\n",
            "Saving ppo_GE_window1_model.zip to ppo_GE_window1_model.zip\n",
            "Saving ppo_GE_window1_probability_config.json to ppo_GE_window1_probability_config.json\n",
            "Saving ppo_GE_window1_vecnorm.pkl to ppo_GE_window1_vecnorm.pkl\n",
            "Saving ppo_UNH_window3_features.json to ppo_UNH_window3_features.json\n",
            "Saving ppo_UNH_window3_model_info.json to ppo_UNH_window3_model_info.json\n",
            "Saving ppo_UNH_window3_model.zip to ppo_UNH_window3_model.zip\n",
            "Saving ppo_UNH_window3_probability_config.json to ppo_UNH_window3_probability_config.json\n",
            "Saving ppo_UNH_window3_vecnorm.pkl to ppo_UNH_window3_vecnorm.pkl\n",
            "2026-02-27 17:38:05,356 | INFO | [COLAB UPLOAD] Saved 10 artifact file(s) into /content/drive/MyDrive/AlpacaPaper/artifacts\n",
            "2026-02-27 17:38:05,357 | INFO | [COLAB UPLOAD] Files: ppo_GE_window1_features.json, ppo_GE_window1_model_info.json, ppo_GE_window1_model.zip, ppo_GE_window1_probability_config.json, ppo_GE_window1_vecnorm.pkl, ppo_UNH_window3_features.json, ppo_UNH_window3_model_info.json, ppo_UNH_window3_model.zip, ppo_UNH_window3_probability_config.json, ppo_UNH_window3_vecnorm.pkl\n",
            "2026-02-27 17:38:06,021 | INFO | EXIT_AFTER_CLOSE      : 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-27 17:38:06,246 | INFO | FORCE_FIRST_BUY       : False\n",
            "2026-02-27 17:38:06,268 | INFO | FORCE_FLATTEN_ON_EXIT : False\n",
            "2026-02-27 17:38:06,273 | INFO | CONFIG\n",
            "2026-02-27 17:38:06,276 | INFO | Project root          : /content/drive/MyDrive/AlpacaPaper\n",
            "2026-02-27 17:38:06,279 | INFO | ARTIFACTS_DIR         : /content/drive/MyDrive/AlpacaPaper/artifacts\n",
            "2026-02-27 17:38:06,281 | INFO | RESULTS_DIR           : /content/drive/MyDrive/AlpacaPaper/results/2026-02-27\n",
            "2026-02-27 17:38:06,283 | INFO | Tickers               : ['UNH', 'GE']\n",
            "2026-02-27 17:38:06,286 | INFO | API base              : https://paper-api.alpaca.markets\n",
            "2026-02-27 17:38:06,288 | INFO | AUTO_RUN_LIVE         : 1\n",
            "2026-02-27 17:38:06,290 | INFO | INF_DETERMINISTIC     : True\n",
            "2026-02-27 17:38:06,291 | INFO | ALLOW_SHORTS          : True\n",
            "2026-02-27 17:38:06,293 | INFO | FLATTEN_INTO_CLOSE    : False\n",
            "2026-02-27 17:38:06,294 | INFO | DRY_RUN=False | BARS_FEED= | USE_FRACTIONALS=True | COOLDOWN_MIN=10 | STALE_MAX_SEC=4200\n",
            "2026-02-27 17:38:06,296 | INFO | DATA_TIMEFRAME        : 1H (model bars)\n",
            "2026-02-27 17:38:06,298 | INFO | EQUITY_TIMEFRAME      : 5Min (equity reporting)\n",
            "2026-02-27 17:38:06,300 | INFO | MAX_DD_PCT: 0.050 | KILL_SWITCH_COOLDOWN_MIN: 30\n",
            "2026-02-27 17:38:06,302 | INFO | WEIGHT_CAP: 0.400 | SIZING_MODE: linear | ENTER_CONF_MIN: 0.020 | ENTER_WEIGHT_MIN: 0.002 | EXIT_WEIGHT_MAX: 0.001 | REBALANCE_MIN_NOTIONAL: 25.00\n",
            "2026-02-27 17:38:06,304 | INFO | TAKE_PROFIT_PCT: 0.050 | STOP_LOSS_PCT: 0.020 | BEST_WINDOW_ENV: \n",
            "2026-02-27 17:38:06,305 | INFO | GROSS_CAP: 1.000 | NET_CAP: 0.800 | EMERGENCY_FLATTEN_ON_EXPOSURE: False\n",
            "2026-02-27 17:38:06,308 | INFO | Artifacts present (16): _dupes_backup, ppo_CVX_window1_features.json, ppo_CVX_window1_model.zip, ppo_CVX_window1_model_info.json, ppo_CVX_window1_probability_config.json, ppo_CVX_window1_vecnorm.pkl, ppo_GE_window1_features.json, ppo_GE_window1_model.zip, ppo_GE_window1_model_info.json, ppo_GE_window1_probability_config.json, ppo_GE_window1_vecnorm.pkl, ppo_UNH_window3_features.json, ppo_UNH_window3_model.zip, ppo_UNH_window3_model_info.json, ppo_UNH_window3_probability_config.json, ppo_UNH_window3_vecnorm.pkl\n",
            "2026-02-27 17:38:06,309 | INFO | DATA_TIMEFRAME=1H -> LIVE_TIMEFRAME=1Hour\n",
            "2026-02-27 17:38:06,489 | INFO | SHORT CHECK (pre): ALLOW_SHORTS=True | acct.shorting_enabled=True\n",
            "2026-02-27 17:38:06,534 | INFO | [UNH] asset.shortable=True | tradable=True | fractionable=True\n",
            "2026-02-27 17:38:06,568 | INFO | [GE] asset.shortable=True | tradable=True | fractionable=True\n",
            "2026-02-27 17:38:06,570 | INFO | Account status: AccountStatus.ACTIVE | equity=95277.96 | cash=134821.76\n",
            "2026-02-27 17:38:06,577 | INFO | [UNH] model=ppo_UNH_window3_model.zip | window=3 | vecnorm=ppo_UNH_window3_vecnorm.pkl | features=ppo_UNH_window3_features.json\n",
            "2026-02-27 17:38:12,711 | INFO | [UNH] Artifacts loaded and ready.\n",
            "2026-02-27 17:38:12,723 | INFO | [GE] model=ppo_GE_window1_model.zip | window=1 | vecnorm=ppo_GE_window1_vecnorm.pkl | features=ppo_GE_window1_features.json\n",
            "2026-02-27 17:38:12,758 | INFO | [GE] Artifacts loaded and ready.\n",
            "2026-02-27 17:38:12,759 | INFO | Starting live execution for (loaded): ['UNH', 'GE']\n",
            "2026-02-27 17:38:12,762 | INFO | Starting live trading loop\n",
            "2026-02-27 17:38:12,831 | INFO | Session open equity anchor set: 95277.14\n",
            "[HEARTBEAT] 2026-02-27T17:38:12.873795+00:00 cycle=0 equity=95,277.14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Position Summary:\n",
            "  GE: -7.0 shares @ $341.08 | Value: $-2,387.59\n",
            "  UNH: -127.0 shares @ $292.57 | Value: $-37,157.03\n",
            "\n",
            "Total Market Value: $-39,544.62\n",
            "2026-02-27 17:38:13,012 | INFO | [EXPOSURE_CAP] ok: gross=0.415 (cap=1.000) | net=-0.415 (cap=0.800)\n",
            "2026-02-27 17:38:13,063 | WARNING | START_FLAT=1: strict flatten (cancel orders + close positions + wait flat).\n",
            "2026-02-27 17:38:13,065 | WARNING | [START_FLAT] strict flatten begin for symbols=['UNH', 'GE']\n",
            "2026-02-27 17:38:13,126 | INFO | [UNH] close_position submitted.\n",
            "2026-02-27 17:38:13,185 | INFO | [GE] close_position submitted.\n",
            "2026-02-27 17:38:14,726 | WARNING | [START_FLAT] strict flatten complete: FLAT ✅\n",
            "2026-02-27 17:38:14,728 | INFO | Flatten wait complete: True\n",
            "2026-02-27 17:38:14,879 | INFO | [UNH] Re-entry cooldown active (59s left); skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-27 17:38:15,975 | INFO | [TIMER] UNH symbol work: 1.245s\n",
            "2026-02-27 17:38:16,021 | INFO | [GE] Re-entry cooldown active (58s left); skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-27 17:38:16,236 | INFO | [TIMER] GE symbol work: 0.260s\n",
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2026-02-27/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n",
            "2026-02-27 17:38:18,157 | INFO | Perf: cum_return=-0.12% | sharpe=-28.36 | maxDD=-0.16%\n",
            "2026-02-27 17:38:18,158 | INFO | [TIMER] full-cycle active time: 5.325s (cooldown=10 min)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HEARTBEAT] 2026-02-27T17:40:00.144732+00:00 cycle=1 equity=95,267.16\n",
            "\n",
            "Position Summary:\n",
            "  (no open positions)\n",
            "\n",
            "Total Market Value: $0.00\n",
            "2026-02-27 17:40:00,335 | INFO | [EXPOSURE_CAP] ok: gross=0.000 (cap=1.000) | net=0.000 (cap=0.800)\n",
            "2026-02-27 17:40:00,489 | INFO | [TIMER] UNH symbol work: 0.152s\n",
            "2026-02-27 17:40:00,593 | INFO | [TIMER] GE symbol work: 0.102s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-27 17:40:00,733 | INFO | [TIMER] full-cycle active time: 0.629s (cooldown=10 min)\n",
            "[HEARTBEAT] 2026-02-27T17:50:00.142014+00:00 cycle=2 equity=95,267.16\n",
            "\n",
            "Position Summary:\n",
            "  (no open positions)\n",
            "\n",
            "Total Market Value: $0.00\n",
            "2026-02-27 17:50:00,237 | INFO | [EXPOSURE_CAP] ok: gross=0.000 (cap=1.000) | net=0.000 (cap=0.800)\n",
            "2026-02-27 17:50:00,445 | INFO | [TIMER] UNH symbol work: 0.206s\n",
            "2026-02-27 17:50:00,539 | INFO | [TIMER] GE symbol work: 0.092s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-27 17:50:00,937 | INFO | [TIMER] full-cycle active time: 0.832s (cooldown=10 min)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HEARTBEAT] 2026-02-27T18:00:00.140316+00:00 cycle=3 equity=95,267.16\n",
            "\n",
            "Position Summary:\n",
            "  (no open positions)\n",
            "\n",
            "Total Market Value: $0.00\n",
            "2026-02-27 18:00:00,238 | INFO | [EXPOSURE_CAP] ok: gross=0.000 (cap=1.000) | net=0.000 (cap=0.800)\n",
            "2026-02-27 18:00:02,029 | INFO | [TIMER] UNH symbol work: 1.790s\n",
            "2026-02-27 18:00:02,222 | INFO | [TIMER] GE symbol work: 0.192s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2026-02-27/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n",
            "2026-02-27 18:00:03,150 | INFO | Perf: cum_return=-0.13% | sharpe=-26.47 | maxDD=-0.16%\n",
            "2026-02-27 18:00:03,152 | INFO | [TIMER] full-cycle active time: 3.045s (cooldown=10 min)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HEARTBEAT] 2026-02-27T18:10:00.222621+00:00 cycle=4 equity=95,267.16\n",
            "\n",
            "Position Summary:\n",
            "  (no open positions)\n",
            "\n",
            "Total Market Value: $0.00\n",
            "2026-02-27 18:10:00,412 | INFO | [EXPOSURE_CAP] ok: gross=0.000 (cap=1.000) | net=0.000 (cap=0.800)\n",
            "2026-02-27 18:10:00,582 | INFO | [TIMER] UNH symbol work: 0.169s\n",
            "2026-02-27 18:10:00,632 | INFO | [TIMER] GE symbol work: 0.049s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-27 18:10:00,947 | INFO | [TIMER] full-cycle active time: 0.841s (cooldown=10 min)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HEARTBEAT] 2026-02-27T18:20:00.179986+00:00 cycle=5 equity=95,267.16\n",
            "\n",
            "Position Summary:\n",
            "  (no open positions)\n",
            "\n",
            "Total Market Value: $0.00\n",
            "2026-02-27 18:20:00,305 | INFO | [EXPOSURE_CAP] ok: gross=0.000 (cap=1.000) | net=0.000 (cap=0.800)\n",
            "2026-02-27 18:20:00,507 | INFO | [UNH] predict() ok → raw=-0.4382 target_w=-0.1753 conf=0.438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-27 18:20:00,727 | INFO | [UNH] Submitted sell qty=56.0\n",
            "2026-02-27 18:20:00,760 | INFO | [TIMER] UNH symbol work: 0.454s\n",
            "2026-02-27 18:20:00,884 | INFO | [GE] predict() ok → raw=0.1920 target_w=0.0768 conf=0.192\n",
            "2026-02-27 18:20:01,017 | INFO | [GE] Submitted buy notional=$7317.58\n",
            "2026-02-27 18:20:01,058 | INFO | [TIMER] GE symbol work: 0.296s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2026-02-27/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n",
            "2026-02-27 18:20:01,908 | INFO | Perf: cum_return=-0.13% | sharpe=-24.23 | maxDD=-0.16%\n",
            "2026-02-27 18:20:01,910 | INFO | [TIMER] full-cycle active time: 1.769s (cooldown=10 min)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HEARTBEAT] 2026-02-27T18:30:00.155003+00:00 cycle=6 equity=95,260.07\n",
            "\n",
            "Position Summary:\n",
            "  GE: 21.624054373 shares @ $338.37 | Value: $7,316.93\n",
            "  UNH: -56.0 shares @ $293.15 | Value: $-16,416.68\n",
            "\n",
            "Total Market Value: $-9,099.75\n",
            "2026-02-27 18:30:00,308 | INFO | [EXPOSURE_CAP] ok: gross=0.249 (cap=1.000) | net=-0.096 (cap=0.800)\n",
            "2026-02-27 18:30:00,474 | INFO | [TIMER] UNH symbol work: 0.164s\n",
            "2026-02-27 18:30:00,603 | INFO | [TIMER] GE symbol work: 0.127s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-27 18:30:00,915 | INFO | [TIMER] full-cycle active time: 0.809s (cooldown=10 min)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HEARTBEAT] 2026-02-27T18:40:00.149203+00:00 cycle=7 equity=95,287.32\n",
            "\n",
            "Position Summary:\n",
            "  GE: 21.624054373 shares @ $337.89 | Value: $7,306.66\n",
            "  UNH: -56.0 shares @ $292.49 | Value: $-16,379.16\n",
            "\n",
            "Total Market Value: $-9,072.50\n",
            "2026-02-27 18:40:00,272 | INFO | [EXPOSURE_CAP] ok: gross=0.249 (cap=1.000) | net=-0.095 (cap=0.800)\n",
            "2026-02-27 18:40:00,495 | INFO | [TIMER] UNH symbol work: 0.221s\n",
            "2026-02-27 18:40:00,565 | INFO | [TIMER] GE symbol work: 0.069s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2026-02-27/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n",
            "2026-02-27 18:40:01,423 | INFO | Perf: cum_return=-0.10% | sharpe=-17.72 | maxDD=-0.17%\n",
            "2026-02-27 18:40:01,424 | INFO | [TIMER] full-cycle active time: 1.327s (cooldown=10 min)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HEARTBEAT] 2026-02-27T18:50:00.164013+00:00 cycle=8 equity=95,257.07\n",
            "\n",
            "Position Summary:\n",
            "  GE: 21.624054373 shares @ $338.27 | Value: $7,314.77\n",
            "  UNH: -56.0 shares @ $293.17 | Value: $-16,417.52\n",
            "\n",
            "Total Market Value: $-9,102.75\n",
            "2026-02-27 18:50:00,293 | INFO | [EXPOSURE_CAP] ok: gross=0.249 (cap=1.000) | net=-0.096 (cap=0.800)\n",
            "2026-02-27 18:50:00,437 | INFO | [TIMER] UNH symbol work: 0.142s\n",
            "2026-02-27 18:50:00,601 | INFO | [TIMER] GE symbol work: 0.162s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-27 18:50:00,971 | INFO | [TIMER] full-cycle active time: 0.844s (cooldown=10 min)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HEARTBEAT] 2026-02-27T19:00:00.161303+00:00 cycle=9 equity=95,290.94\n",
            "\n",
            "Position Summary:\n",
            "  GE: 21.624054373 shares @ $338.27 | Value: $7,314.88\n",
            "  UNH: -56.0 shares @ $292.60 | Value: $-16,385.60\n",
            "\n",
            "Total Market Value: $-9,070.72\n",
            "2026-02-27 19:00:00,260 | INFO | [EXPOSURE_CAP] ok: gross=0.249 (cap=1.000) | net=-0.095 (cap=0.800)\n",
            "2026-02-27 19:00:00,469 | INFO | [TIMER] UNH symbol work: 0.208s\n",
            "2026-02-27 19:00:00,526 | INFO | [TIMER] GE symbol work: 0.056s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2026-02-27/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n",
            "2026-02-27 19:00:01,197 | INFO | Perf: cum_return=-0.10% | sharpe=-15.89 | maxDD=-0.18%\n",
            "2026-02-27 19:00:01,199 | INFO | [TIMER] full-cycle active time: 1.093s (cooldown=10 min)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HEARTBEAT] 2026-02-27T19:10:00.160597+00:00 cycle=10 equity=95,307.60\n",
            "\n",
            "Position Summary:\n",
            "  GE: 21.624054373 shares @ $338.82 | Value: $7,326.66\n",
            "  UNH: -56.0 shares @ $292.48 | Value: $-16,378.88\n",
            "\n",
            "Total Market Value: $-9,052.22\n",
            "2026-02-27 19:10:00,307 | INFO | [EXPOSURE_CAP] ok: gross=0.249 (cap=1.000) | net=-0.095 (cap=0.800)\n",
            "2026-02-27 19:10:00,457 | INFO | [TIMER] UNH symbol work: 0.148s\n",
            "2026-02-27 19:10:00,508 | INFO | [TIMER] GE symbol work: 0.049s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-27 19:10:00,857 | INFO | [TIMER] full-cycle active time: 0.758s (cooldown=10 min)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HEARTBEAT] 2026-02-27T19:20:00.187311+00:00 cycle=11 equity=95,367.57\n",
            "\n",
            "Position Summary:\n",
            "  GE: 21.624054373 shares @ $339.34 | Value: $7,337.91\n",
            "  UNH: -56.0 shares @ $291.61 | Value: $-16,330.16\n",
            "\n",
            "Total Market Value: $-8,992.25\n",
            "2026-02-27 19:20:00,285 | INFO | [EXPOSURE_CAP] ok: gross=0.248 (cap=1.000) | net=-0.094 (cap=0.800)\n",
            "2026-02-27 19:20:00,455 | INFO | [UNH] predict() ok → raw=-0.4372 target_w=-0.1749 conf=0.437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-27 19:20:00,681 | INFO | [UNH] Submitted sell qty=57.0\n",
            "2026-02-27 19:20:00,710 | INFO | [TIMER] UNH symbol work: 0.423s\n",
            "2026-02-27 19:20:00,809 | INFO | [GE] predict() ok → raw=0.1924 target_w=0.0770 conf=0.192\n",
            "2026-02-27 19:20:00,921 | INFO | [GE] Submitted buy notional=$7340.18\n",
            "2026-02-27 19:20:00,953 | INFO | [TIMER] GE symbol work: 0.241s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2026-02-27/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n",
            "2026-02-27 19:20:01,797 | INFO | Perf: cum_return=-0.02% | sharpe=-3.03 | maxDD=-0.18%\n",
            "2026-02-27 19:20:01,800 | INFO | [TIMER] full-cycle active time: 1.647s (cooldown=10 min)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HEARTBEAT] 2026-02-27T19:30:00.219851+00:00 cycle=12 equity=95,369.92\n",
            "\n",
            "Position Summary:\n",
            "  GE: 43.249709903 shares @ $339.44 | Value: $14,680.47\n",
            "  UNH: -113.0 shares @ $291.60 | Value: $-32,950.25\n",
            "\n",
            "Total Market Value: $-18,269.78\n",
            "2026-02-27 19:30:00,314 | INFO | [EXPOSURE_CAP] ok: gross=0.499 (cap=1.000) | net=-0.192 (cap=0.800)\n",
            "2026-02-27 19:30:00,482 | INFO | [TIMER] UNH symbol work: 0.166s\n",
            "2026-02-27 19:30:00,558 | INFO | [TIMER] GE symbol work: 0.074s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-27 19:30:00,920 | INFO | [TIMER] full-cycle active time: 0.733s (cooldown=10 min)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HEARTBEAT] 2026-02-27T19:40:00.156554+00:00 cycle=13 equity=95,355.69\n",
            "\n",
            "Position Summary:\n",
            "  GE: 43.249709903 shares @ $339.55 | Value: $14,685.44\n",
            "  UNH: -113.0 shares @ $291.76 | Value: $-32,969.44\n",
            "\n",
            "Total Market Value: $-18,284.01\n",
            "2026-02-27 19:40:00,312 | INFO | [EXPOSURE_CAP] ok: gross=0.500 (cap=1.000) | net=-0.192 (cap=0.800)\n",
            "2026-02-27 19:40:00,482 | INFO | [TIMER] UNH symbol work: 0.167s\n",
            "2026-02-27 19:40:00,585 | INFO | [TIMER] GE symbol work: 0.102s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2026-02-27/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n",
            "2026-02-27 19:40:01,476 | INFO | Perf: cum_return=-0.04% | sharpe=-4.35 | maxDD=-0.18%\n",
            "2026-02-27 19:40:01,478 | INFO | [TIMER] full-cycle active time: 1.378s (cooldown=10 min)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HEARTBEAT] 2026-02-27T19:50:00.268209+00:00 cycle=14 equity=95,350.27\n",
            "\n",
            "Position Summary:\n",
            "  GE: 43.249709903 shares @ $339.49 | Value: $14,682.84\n",
            "  UNH: -113.0 shares @ $291.79 | Value: $-32,972.27\n",
            "\n",
            "Total Market Value: $-18,289.43\n",
            "2026-02-27 19:50:00,435 | INFO | [EXPOSURE_CAP] ok: gross=0.500 (cap=1.000) | net=-0.192 (cap=0.800)\n",
            "2026-02-27 19:50:00,576 | INFO | [TIMER] UNH symbol work: 0.139s\n",
            "2026-02-27 19:50:00,622 | INFO | [TIMER] GE symbol work: 0.045s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-27 19:50:01,008 | INFO | [TIMER] full-cycle active time: 0.904s (cooldown=10 min)\n",
            "[HEARTBEAT] 2026-02-27T20:00:00.141235+00:00 cycle=15 equity=95,326.07\n",
            "\n",
            "Position Summary:\n",
            "  GE: 43.249709903 shares @ $339.27 | Value: $14,673.33\n",
            "  UNH: -113.0 shares @ $291.92 | Value: $-32,986.96\n",
            "\n",
            "Total Market Value: $-18,313.63\n",
            "2026-02-27 20:00:00,241 | INFO | [EXPOSURE_CAP] ok: gross=0.500 (cap=1.000) | net=-0.192 (cap=0.800)\n",
            "2026-02-27 20:00:00,363 | INFO | [TIMER] UNH symbol work: 0.121s\n",
            "2026-02-27 20:00:00,422 | INFO | [TIMER] GE symbol work: 0.058s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2026-02-27/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n",
            "2026-02-27 20:00:01,197 | INFO | Perf: cum_return=-0.07% | sharpe=-7.78 | maxDD=-0.18%\n",
            "2026-02-27 20:00:01,200 | INFO | [TIMER] full-cycle active time: 1.100s (cooldown=10 min)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HEARTBEAT] 2026-02-27T20:10:00.134002+00:00 cycle=16 equity=95,338.77\n",
            "\n",
            "Position Summary:\n",
            "  GE: 43.249709903 shares @ $340.63 | Value: $14,732.36\n",
            "  UNH: -113.0 shares @ $292.33 | Value: $-33,033.29\n",
            "\n",
            "Total Market Value: $-18,300.93\n",
            "2026-02-27 20:10:00,292 | INFO | [EXPOSURE_CAP] ok: gross=0.501 (cap=1.000) | net=-0.192 (cap=0.800)\n",
            "2026-02-27 20:10:00,461 | INFO | [TIMER] UNH symbol work: 0.168s\n",
            "2026-02-27 20:10:00,564 | INFO | [TIMER] GE symbol work: 0.102s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-27 20:10:00,966 | INFO | [TIMER] full-cycle active time: 0.867s (cooldown=10 min)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HEARTBEAT] 2026-02-27T20:20:00.146818+00:00 cycle=17 equity=95,232.12\n",
            "\n",
            "Position Summary:\n",
            "  GE: 43.249709903 shares @ $340.22 | Value: $14,714.42\n",
            "  UNH: -113.0 shares @ $293.12 | Value: $-33,122.00\n",
            "\n",
            "Total Market Value: $-18,407.58\n",
            "2026-02-27 20:20:00,274 | INFO | [EXPOSURE_CAP] ok: gross=0.502 (cap=1.000) | net=-0.193 (cap=0.800)\n",
            "2026-02-27 20:20:00,450 | INFO | [UNH] predict() ok → raw=-0.4365 target_w=-0.1746 conf=0.436\n",
            "2026-02-27 20:20:00,613 | INFO | [UNH] Submitted sell qty=56.0\n",
            "2026-02-27 20:20:00,644 | INFO | [TIMER] UNH symbol work: 0.369s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-27 20:20:00,725 | INFO | [GE] predict() ok → raw=0.1926 target_w=0.0770 conf=0.193\n",
            "2026-02-27 20:20:00,829 | INFO | [GE] Submitted buy notional=$7336.52\n",
            "2026-02-27 20:20:00,859 | INFO | [TIMER] GE symbol work: 0.213s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2026-02-27/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n",
            "2026-02-27 20:20:01,578 | INFO | Perf: cum_return=-0.16% | sharpe=-15.81 | maxDD=-0.20%\n",
            "2026-02-27 20:20:01,580 | INFO | [TIMER] full-cycle active time: 1.477s (cooldown=10 min)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HEARTBEAT] 2026-02-27T20:30:00.167084+00:00 cycle=18 equity=95,196.08\n",
            "\n",
            "Position Summary:\n",
            "  GE: 64.809320823 shares @ $339.60 | Value: $22,009.25\n",
            "  UNH: -169.0 shares @ $293.07 | Value: $-49,528.83\n",
            "\n",
            "Total Market Value: $-27,519.58\n",
            "2026-02-27 20:30:00,309 | INFO | [EXPOSURE_CAP] ok: gross=0.751 (cap=1.000) | net=-0.289 (cap=0.800)\n",
            "2026-02-27 20:30:00,667 | INFO | [TIMER] UNH symbol work: 0.356s\n",
            "2026-02-27 20:30:00,788 | INFO | [TIMER] GE symbol work: 0.121s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-27 20:30:01,118 | INFO | [TIMER] full-cycle active time: 1.014s (cooldown=10 min)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HEARTBEAT] 2026-02-27T20:40:00.150176+00:00 cycle=19 equity=95,161.92\n",
            "\n",
            "Position Summary:\n",
            "  GE: 64.809320823 shares @ $340.09 | Value: $22,041.00\n",
            "  UNH: -169.0 shares @ $293.46 | Value: $-49,594.74\n",
            "\n",
            "Total Market Value: $-27,553.74\n",
            "2026-02-27 20:40:00,261 | INFO | [EXPOSURE_CAP] ok: gross=0.753 (cap=1.000) | net=-0.290 (cap=0.800)\n",
            "2026-02-27 20:40:00,789 | INFO | [TIMER] UNH symbol work: 0.527s\n",
            "2026-02-27 20:40:00,888 | INFO | [TIMER] GE symbol work: 0.094s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2026-02-27/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n",
            "2026-02-27 20:40:01,726 | INFO | Perf: cum_return=-0.24% | sharpe=-21.92 | maxDD=-0.28%\n",
            "2026-02-27 20:40:01,727 | INFO | [TIMER] full-cycle active time: 1.624s (cooldown=10 min)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HEARTBEAT] 2026-02-27T20:50:00.187139+00:00 cycle=20 equity=95,174.72\n",
            "\n",
            "Position Summary:\n",
            "  GE: 64.809320823 shares @ $341.80 | Value: $22,151.83\n",
            "  UNH: -169.0 shares @ $294.05 | Value: $-49,694.45\n",
            "\n",
            "Total Market Value: $-27,542.62\n",
            "2026-02-27 20:50:00,310 | INFO | [EXPOSURE_CAP] ok: gross=0.755 (cap=1.000) | net=-0.289 (cap=0.800)\n",
            "2026-02-27 20:50:00,692 | INFO | [TIMER] UNH symbol work: 0.379s\n",
            "2026-02-27 20:50:00,854 | INFO | [TIMER] GE symbol work: 0.159s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-27 20:50:01,225 | INFO | [TIMER] full-cycle active time: 1.097s (cooldown=10 min)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-27 21:00:00,145 | INFO | Market closed. Next open in 235799s.\n",
            "2026-02-27 21:00:00,149 | INFO | IN_COLAB and COLAB_EXIT_WHEN_CLOSED=1 -> exiting cleanly instead of long sleep.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7c299f5b7ee0>\n",
            "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2026-02-27/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n",
            "2026-02-27 21:00:00,640 | INFO | Live loop exited cleanly.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "PPO Alpaca Paper Live Loop (alpaca-py) — End-to-end single-file script (02.27 + safety ports from 02.26)\n",
        "\n",
        "✅ Keeps 02.27 alpaca-py client architecture:\n",
        "   - TradingClient, StockHistoricalDataClient\n",
        "   - init_clients()\n",
        "   - resolve_credentials()\n",
        "\n",
        "✅ Re-added / enforced from 02.26 (as requested):\n",
        "A) STRICT START_FLAT flatten logic (CRITICAL)\n",
        "   - list_open_orders()\n",
        "   - cancel_open_orders_for_symbols()\n",
        "   - wait_until_flat_positions_and_orders()\n",
        "   - flatten_symbols_strict()\n",
        "\n",
        "B) Hard-cap enforcement (prevents runaway weights)\n",
        "   - enforce_position_caps_if_violated() runs BEFORE normal policy rebalance each bar\n",
        "\n",
        "C) Market-closed behavior for Colab\n",
        "   - _sleep_until_open_or_exit() (exit in Colab when closed if COLAB_EXIT_WHEN_CLOSED=1)\n",
        "\n",
        "D) Exposure cap knobs (GROSS_CAP / NET_CAP)\n",
        "   - parsed from env/config\n",
        "   - enforcement behavior:\n",
        "       * compute gross/net exposure % each cycle\n",
        "       * if cap violated: block risk-increasing trades (only allow de-risking)\n",
        "       * optional emergency de-risk (flatten) if EMERGENCY_FLATTEN_ON_EXPOSURE=1\n",
        "\"\"\"\n",
        "\n",
        "import csv\n",
        "import sys\n",
        "import gc\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "import shutil\n",
        "import time\n",
        "import warnings\n",
        "from dataclasses import dataclass, field\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from decimal import Decimal, ROUND_DOWN, ROUND_HALF_UP\n",
        "from functools import lru_cache\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Mapping, Optional, Tuple\n",
        "\n",
        "# Scientific / data stack\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Fix seed for reproducibility\n",
        "import random\n",
        "import torch\n",
        "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Alpaca (alpaca-py)\n",
        "from alpaca.data.timeframe import TimeFrame\n",
        "from alpaca.trading.client import TradingClient\n",
        "from alpaca.data.historical import StockHistoricalDataClient\n",
        "from alpaca.data.requests import StockBarsRequest, StockLatestTradeRequest, StockLatestQuoteRequest\n",
        "from alpaca.trading.requests import GetPortfolioHistoryRequest, MarketOrderRequest, GetOrdersRequest\n",
        "from alpaca.trading.enums import OrderSide, TimeInForce, OrderStatus\n",
        "\n",
        "# RL models\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import VecNormalize\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Colab detection\n",
        "# ------------------------------------------------------------\n",
        "IN_COLAB = False\n",
        "try:\n",
        "    import google.colab  # type: ignore\n",
        "    from google.colab import drive  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Timeframe normalization / mapping\n",
        "# ============================================================\n",
        "def normalize_tf_key(tf: str) -> str:\n",
        "    s = str(tf or \"\").strip().lower()\n",
        "    s = s.replace(\" \", \"\").replace(\"_\", \"\")\n",
        "    s = s.replace(\"minutes\", \"min\").replace(\"minute\", \"min\")\n",
        "    s = s.replace(\"hours\", \"h\").replace(\"hour\", \"h\")\n",
        "    return s\n",
        "\n",
        "try:\n",
        "    _MIN_UNIT = TimeFrame.Unit.Minute\n",
        "except Exception:\n",
        "    from alpaca.data.timeframe import TimeFrameUnit as _TFU\n",
        "    _MIN_UNIT = _TFU.Minute\n",
        "\n",
        "try:\n",
        "    _TF_MAP = {\n",
        "        \"1min\":  TimeFrame.Minute,\n",
        "        \"5min\":  TimeFrame(5, _MIN_UNIT),\n",
        "        \"15min\": TimeFrame(15, _MIN_UNIT),\n",
        "        \"1h\":    TimeFrame.Hour,\n",
        "        \"60min\": TimeFrame.Hour,\n",
        "        \"1d\":    TimeFrame.Day,\n",
        "        \"1hour\": TimeFrame.Hour,\n",
        "        \"1hr\":   TimeFrame.Hour,\n",
        "    }\n",
        "except Exception:\n",
        "    _TF_MAP = {}\n",
        "\n",
        "_PF_TF_STR = {\n",
        "    \"1min\":  \"1Min\",\n",
        "    \"5min\":  \"5Min\",\n",
        "    \"15min\": \"15Min\",\n",
        "    \"1h\":    \"1Hour\",\n",
        "    \"60min\": \"1Hour\",\n",
        "    \"1d\":    \"1Day\",\n",
        "    \"1hour\": \"1Hour\",\n",
        "    \"1hr\":   \"1Hour\",\n",
        "}\n",
        "\n",
        "LIVE_TIMEFRAME = TimeFrame.Hour  # overwritten after config\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Utils / Paths / Globals\n",
        "# ============================================================\n",
        "def round_to_cents(x: float) -> float:\n",
        "    return float(Decimal(str(x)).quantize(Decimal(\"0.01\"), rounding=ROUND_DOWN))\n",
        "\n",
        "def _to_bool(x: str) -> bool:\n",
        "    return str(x).strip().lower() in (\"1\", \"true\", \"yes\", \"y\", \"on\")\n",
        "\n",
        "def to_2dp_str(x) -> str:\n",
        "    return format(Decimal(str(x)).quantize(Decimal(\"0.01\"), rounding=ROUND_HALF_UP), \"f\")\n",
        "\n",
        "def to_6dp_str(x) -> str:\n",
        "    return format(Decimal(str(x)).quantize(Decimal(\"0.000001\"), rounding=ROUND_DOWN), \"f\")\n",
        "\n",
        "if IN_COLAB:\n",
        "    try:\n",
        "        drive.mount(\"/content/drive\", force_remount=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "if IN_COLAB:\n",
        "    PROJECT_ROOT = Path(\"/content/drive/MyDrive/AlpacaPaper\")\n",
        "else:\n",
        "    PROJECT_ROOT = Path.cwd() / \"AlpacaPaper\"\n",
        "PROJECT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Order throttling timestamps (per symbol)\n",
        "_ORDER_EVENT_TS: Dict[str, float] = {}\n",
        "_LAST_ORDER_TS: Dict[str, float] = {}\n",
        "\n",
        "_FORCED_FIRST_BUY_DONE: Dict[str, bool] = {}\n",
        "_NO_POS_CYCLE_COUNT: Dict[str, int] = {}\n",
        "_REENTRY_BLOCK_UNTIL: Dict[str, float] = {}\n",
        "\n",
        "_LAST_TARGET_W: Dict[str, float] = {}\n",
        "_LAST_RAW_A: Dict[str, float] = {}\n",
        "_LAST_CONF: Dict[str, float] = {}\n",
        "\n",
        "_RUN_SUMMARY_HEADER_CACHE: Dict[str, List[str]] = {}\n",
        "_LAST_BAR_TIME_SEEN: Dict[str, pd.Timestamp] = {}\n",
        "_LAST_BAR_TIME_SKIPPED: Dict[str, pd.Timestamp] = {}\n",
        "\n",
        "SESSION_OPEN_EQUITY: Optional[float] = None\n",
        "_last_kill_ts: float = 0.0\n",
        "\n",
        "_SEED_COOLDOWN_SEC = 10\n",
        "\n",
        "# Exposure-cap state (D)\n",
        "_EXPOSURE_CAPS_BLOCK_RISK: bool = False\n",
        "_EXPOSURE_LAST: Dict[str, float] = {\"gross\": float(\"nan\"), \"net\": float(\"nan\")}\n",
        "\n",
        "def begin_order_event(symbol: str, min_gap_sec: int) -> bool:\n",
        "    now = time.time()\n",
        "    last = _ORDER_EVENT_TS.get(symbol, 0.0)\n",
        "    if (now - last) < float(min_gap_sec):\n",
        "        return False\n",
        "    _ORDER_EVENT_TS[symbol] = now\n",
        "    return True\n",
        "\n",
        "def stamp_order_event(symbol: str) -> None:\n",
        "    ts = time.time()\n",
        "    _ORDER_EVENT_TS[symbol] = ts\n",
        "    _LAST_ORDER_TS[symbol] = ts\n",
        "\n",
        "warnings.filterwarnings(\"default\")\n",
        "\n",
        "# Load env (PROJECT_ROOT/.env preferred)\n",
        "env_candidates = [PROJECT_ROOT / \".env\", Path(\".env\")]\n",
        "for env_path in env_candidates:\n",
        "    if env_path.exists():\n",
        "        load_dotenv(dotenv_path=env_path, override=True)\n",
        "        break\n",
        "else:\n",
        "    load_dotenv(override=True)\n",
        "\n",
        "# Defaults\n",
        "os.environ.setdefault(\"PH_TIMEOUT_SEC\", \"8\")\n",
        "os.environ.setdefault(\"EQUITY_TIMEFRAME\", \"5Min\")\n",
        "os.environ.setdefault(\"DEBUG_FORCE_SEED_IF_IDLE\", \"0\")\n",
        "os.environ.setdefault(\"DEBUG_SEED_IDLE_CYCLES\", \"10\")\n",
        "os.environ.setdefault(\"START_FLAT\", \"1\")\n",
        "os.environ.setdefault(\"COLAB_EXIT_WHEN_CLOSED\", \"1\")\n",
        "\n",
        "# Basic logger early (replaced after RESULTS_DIR exists)\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
        "logging.getLogger().setLevel(getattr(logging, os.getenv(\"LOG_LEVEL\", \"INFO\").upper(), logging.INFO))\n",
        "root = logging.getLogger()\n",
        "root.handlers.clear()\n",
        "handler = logging.StreamHandler(sys.stdout)\n",
        "handler.setFormatter(logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\"))\n",
        "handler.setLevel(getattr(logging, os.getenv(\"LOG_LEVEL\", \"INFO\").upper(), logging.INFO))\n",
        "root.addHandler(handler)\n",
        "root.setLevel(handler.level)\n",
        "try:\n",
        "    sys.stdout.reconfigure(line_buffering=True)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Credentials (keeps 02.27 pattern)\n",
        "# ============================================================\n",
        "def resolve_credentials() -> Tuple[str, str, str]:\n",
        "    \"\"\"\n",
        "    Single source of truth for API keys + base URL.\n",
        "    Looks in:\n",
        "      - APCA_API_KEY_ID / APCA_API_SECRET_KEY / APCA_API_BASE_URL\n",
        "      - ALPACA_API_KEY_ID / ALPACA_API_SECRET_KEY\n",
        "      - PROJECT_ROOT/.env\n",
        "    \"\"\"\n",
        "    base_url = (os.getenv(\"APCA_API_BASE_URL\") or \"https://paper-api.alpaca.markets\").strip()\n",
        "\n",
        "    key = (os.getenv(\"APCA_API_KEY_ID\") or os.getenv(\"ALPACA_API_KEY_ID\") or \"\").strip()\n",
        "    sec = (os.getenv(\"APCA_API_SECRET_KEY\") or os.getenv(\"ALPACA_API_SECRET_KEY\") or \"\").strip()\n",
        "\n",
        "    if not key or not sec:\n",
        "        raise RuntimeError(\n",
        "            \"Missing Alpaca credentials. Set APCA_API_KEY_ID and APCA_API_SECRET_KEY (or ALPACA_* equivalents) \"\n",
        "            \"in your environment or in PROJECT_ROOT/.env\"\n",
        "        )\n",
        "\n",
        "    return key, sec, base_url\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Config dataclass\n",
        "# ============================================================\n",
        "def _to_list_csv(x: str) -> list:\n",
        "    return [s.strip().upper() for s in str(x).split(\",\") if s.strip()]\n",
        "\n",
        "@dataclass\n",
        "class Knobs:\n",
        "    APCA_API_BASE_URL: str = \"https://paper-api.alpaca.markets\"\n",
        "    DRY_RUN: bool = False\n",
        "    AUTO_RUN_LIVE: bool = True\n",
        "    INF_DETERMINISTIC: bool = True\n",
        "\n",
        "    FLATTEN_INTO_CLOSE: bool = False\n",
        "    FORCE_FIRST_BUY: bool = False\n",
        "    FORCE_FLATTEN_ON_EXIT: bool = False\n",
        "\n",
        "    EQUITY_LOG_THROTTLE_SEC: int = 900\n",
        "    SKIP_EQUITY_WHEN_DRY_RUN: bool = True\n",
        "\n",
        "    TICKERS: List[str] = field(default_factory=list)\n",
        "    ARTIFACTS_DIR: str = \"\"\n",
        "    RESULTS_ROOT: str = \"\"\n",
        "\n",
        "    BARS_FEED: str = \"iex\"\n",
        "    COOLDOWN_MIN: int = 10\n",
        "    STALE_MAX_SEC: int = 4200\n",
        "\n",
        "    SIZING_MODE: str = \"threshold\"  # \"linear\" | \"threshold\"\n",
        "    WEIGHT_CAP: float = 0.35\n",
        "    CONF_FLOOR: float = 0.15\n",
        "    ENTER_CONF_MIN: float = 0.12\n",
        "    ENTER_WEIGHT_MIN: float = 0.02\n",
        "    EXIT_WEIGHT_MAX: float = 0.008\n",
        "    REBALANCE_MIN_NOTIONAL: float = 10.00\n",
        "    USE_FRACTIONALS: bool = True\n",
        "    SEED_FIRST_SHARE: bool = True\n",
        "    ALLOW_SHORTS: bool = False\n",
        "\n",
        "    DELTA_WEIGHT_MIN: float = 0.002\n",
        "    RAW_POS_MIN: float = 0.00\n",
        "    RAW_NEG_MAX: float = 0.00\n",
        "\n",
        "    TAKE_PROFIT_PCT: float = 0.05\n",
        "    STOP_LOSS_PCT: float = 0.03\n",
        "\n",
        "    TRAIN_TIMEFRAME: str = \"1H\"\n",
        "    DATA_TIMEFRAME: str = \"1H\"\n",
        "    EQUITY_TIMEFRAME: str = \"5Min\"\n",
        "\n",
        "    MAX_DAILY_DRAWDOWN_PCT: float = 0.05\n",
        "    KILL_SWITCH_COOLDOWN_MIN: int = 30\n",
        "\n",
        "    # (D) Exposure caps\n",
        "    GROSS_CAP: float = 1.00\n",
        "    NET_CAP: float = 0.80\n",
        "    EMERGENCY_FLATTEN_ON_EXPOSURE: bool = False\n",
        "\n",
        "    EXIT_AFTER_CLOSE: bool = False\n",
        "\n",
        "    APCA_API_KEY_ID: str = \"\"\n",
        "    APCA_API_SECRET_KEY: str = \"\"\n",
        "    STALE_BEST_WINDOW: str = \"\"\n",
        "\n",
        "    @classmethod\n",
        "    def from_env(\n",
        "        cls,\n",
        "        defaults: \"Knobs\",\n",
        "        project_root: Path,\n",
        "        env: Mapping[str, str],\n",
        "        overrides: Mapping[str, object] = None,\n",
        "    ):\n",
        "        kv = {**defaults.__dict__}\n",
        "        kv.update({\n",
        "            \"APCA_API_BASE_URL\": env.get(\"APCA_API_BASE_URL\", kv[\"APCA_API_BASE_URL\"]),\n",
        "            \"AUTO_RUN_LIVE\":     _to_bool(env.get(\"AUTO_RUN_LIVE\", str(kv[\"AUTO_RUN_LIVE\"]))),\n",
        "            \"DRY_RUN\":           _to_bool(env.get(\"DRY_RUN\", str(kv[\"DRY_RUN\"]))),\n",
        "            \"INF_DETERMINISTIC\": _to_bool(env.get(\"INF_DETERMINISTIC\", str(kv[\"INF_DETERMINISTIC\"]))),\n",
        "\n",
        "            \"FLATTEN_INTO_CLOSE\": _to_bool(env.get(\"FLATTEN_INTO_CLOSE\", str(kv.get(\"FLATTEN_INTO_CLOSE\", False)))),\n",
        "            \"FORCE_FIRST_BUY\": _to_bool(env.get(\"FORCE_FIRST_BUY\", str(kv.get(\"FORCE_FIRST_BUY\", False)))),\n",
        "            \"FORCE_FLATTEN_ON_EXIT\": _to_bool(env.get(\"FORCE_FLATTEN_ON_EXIT\", str(kv.get(\"FORCE_FLATTEN_ON_EXIT\", False)))),\n",
        "\n",
        "            \"EQUITY_LOG_THROTTLE_SEC\": int(env.get(\"EQUITY_LOG_THROTTLE_SEC\", str(kv[\"EQUITY_LOG_THROTTLE_SEC\"]))),\n",
        "            \"SKIP_EQUITY_WHEN_DRY_RUN\": _to_bool(env.get(\"SKIP_EQUITY_WHEN_DRY_RUN\", str(kv[\"SKIP_EQUITY_WHEN_DRY_RUN\"]))),\n",
        "\n",
        "            \"USE_FRACTIONALS\": _to_bool(env.get(\"USE_FRACTIONALS\", str(kv[\"USE_FRACTIONALS\"]))),\n",
        "            \"SEED_FIRST_SHARE\": _to_bool(env.get(\"SEED_FIRST_SHARE\", str(kv[\"SEED_FIRST_SHARE\"]))),\n",
        "            \"ALLOW_SHORTS\": _to_bool(env.get(\"ALLOW_SHORTS\", str(kv[\"ALLOW_SHORTS\"]))),\n",
        "\n",
        "            \"TICKERS\": _to_list_csv(env.get(\"TICKERS\", \",\".join(kv[\"TICKERS\"] or [\"UNH\", \"GE\"]))),\n",
        "            \"ARTIFACTS_DIR\": env.get(\"ARTIFACTS_DIR\", kv[\"ARTIFACTS_DIR\"] or str(project_root / \"artifacts\")),\n",
        "            \"RESULTS_ROOT\": env.get(\"RESULTS_ROOT\", kv[\"RESULTS_ROOT\"] or str(project_root / \"results\")),\n",
        "\n",
        "            \"BARS_FEED\": env.get(\"BARS_FEED\", kv[\"BARS_FEED\"]),\n",
        "            \"COOLDOWN_MIN\": int(env.get(\"COOLDOWN_MIN\", str(kv[\"COOLDOWN_MIN\"])) or kv[\"COOLDOWN_MIN\"]),\n",
        "            \"STALE_MAX_SEC\": int(env.get(\"STALE_MAX_SEC\", str(kv[\"STALE_MAX_SEC\"])) or kv[\"STALE_MAX_SEC\"]),\n",
        "\n",
        "            \"SIZING_MODE\": env.get(\"SIZING_MODE\", kv[\"SIZING_MODE\"]),\n",
        "            \"WEIGHT_CAP\": float(env.get(\"WEIGHT_CAP\", str(kv[\"WEIGHT_CAP\"]))),\n",
        "            \"CONF_FLOOR\": float(env.get(\"CONF_FLOOR\", str(kv[\"CONF_FLOOR\"]))),\n",
        "            \"ENTER_CONF_MIN\": float(env.get(\"ENTER_CONF_MIN\", str(kv[\"ENTER_CONF_MIN\"]))),\n",
        "            \"ENTER_WEIGHT_MIN\": float(env.get(\"ENTER_WEIGHT_MIN\", str(kv[\"ENTER_WEIGHT_MIN\"]))),\n",
        "            \"EXIT_WEIGHT_MAX\": float(env.get(\"EXIT_WEIGHT_MAX\", str(kv[\"EXIT_WEIGHT_MAX\"]))),\n",
        "            \"REBALANCE_MIN_NOTIONAL\": float(env.get(\"REBALANCE_MIN_NOTIONAL\", str(kv[\"REBALANCE_MIN_NOTIONAL\"]))),\n",
        "\n",
        "            \"TAKE_PROFIT_PCT\": float(env.get(\"TAKE_PROFIT_PCT\", str(kv[\"TAKE_PROFIT_PCT\"]))),\n",
        "            \"STOP_LOSS_PCT\": float(env.get(\"STOP_LOSS_PCT\", str(kv[\"STOP_LOSS_PCT\"]))),\n",
        "\n",
        "            \"DELTA_WEIGHT_MIN\": float(env.get(\"DELTA_WEIGHT_MIN\", str(kv.get(\"DELTA_WEIGHT_MIN\", 0.002)))),\n",
        "            \"RAW_POS_MIN\": float(env.get(\"RAW_POS_MIN\", str(kv.get(\"RAW_POS_MIN\", 0.0)))),\n",
        "            \"RAW_NEG_MAX\": float(env.get(\"RAW_NEG_MAX\", str(kv.get(\"RAW_NEG_MAX\", 0.0)))),\n",
        "\n",
        "            # (D)\n",
        "            \"GROSS_CAP\": float(env.get(\"GROSS_CAP\", str(kv.get(\"GROSS_CAP\", 1.00)))),\n",
        "            \"NET_CAP\":   float(env.get(\"NET_CAP\",   str(kv.get(\"NET_CAP\",   0.80)))),\n",
        "            \"EMERGENCY_FLATTEN_ON_EXPOSURE\": _to_bool(env.get(\"EMERGENCY_FLATTEN_ON_EXPOSURE\", str(kv.get(\"EMERGENCY_FLATTEN_ON_EXPOSURE\", False)))),\n",
        "\n",
        "            \"EXIT_AFTER_CLOSE\": _to_bool(env.get(\"EXIT_AFTER_CLOSE\", str(kv.get(\"EXIT_AFTER_CLOSE\", False)))),\n",
        "\n",
        "            \"STALE_BEST_WINDOW\": env.get(\"STALE_BEST_WINDOW\", kv.get(\"STALE_BEST_WINDOW\", \"\")),\n",
        "            \"DATA_TIMEFRAME\": env.get(\"DATA_TIMEFRAME\", kv.get(\"DATA_TIMEFRAME\", \"1H\")),\n",
        "            \"TRAIN_TIMEFRAME\": env.get(\"TRAIN_TIMEFRAME\", kv.get(\"TRAIN_TIMEFRAME\", \"1H\")),\n",
        "            \"EQUITY_TIMEFRAME\": env.get(\"EQUITY_TIMEFRAME\", kv.get(\"EQUITY_TIMEFRAME\", \"5Min\")),\n",
        "        })\n",
        "\n",
        "        kv[\"APCA_API_KEY_ID\"] = env.get(\"APCA_API_KEY_ID\") or env.get(\"ALPACA_API_KEY_ID\", \"\") or \"\"\n",
        "        kv[\"APCA_API_SECRET_KEY\"] = env.get(\"APCA_API_SECRET_KEY\") or env.get(\"ALPACA_API_SECRET_KEY\", \"\") or \"\"\n",
        "\n",
        "        if overrides:\n",
        "            for k, v in overrides.items():\n",
        "                key = str(k)\n",
        "                if key.upper() == \"TICKERS\" and isinstance(v, str):\n",
        "                    v = _to_list_csv(v)\n",
        "                kv[key] = v\n",
        "\n",
        "        return cls(**kv)\n",
        "\n",
        "    def apply_to_globals(self):\n",
        "        g = globals()\n",
        "        g[\"BASE_URL\"] = self.APCA_API_BASE_URL\n",
        "        g[\"DRY_RUN\"] = bool(self.DRY_RUN)\n",
        "        g[\"INF_DETERMINISTIC\"] = bool(self.INF_DETERMINISTIC)\n",
        "        g[\"TICKERS\"] = list(self.TICKERS or [\"UNH\", \"GE\"])\n",
        "\n",
        "        g[\"ARTIFACTS_DIR\"] = Path(self.ARTIFACTS_DIR)\n",
        "        g[\"RESULTS_ROOT\"] = Path(self.RESULTS_ROOT)\n",
        "        g[\"RESULTS_DIR\"] = g[\"RESULTS_ROOT\"] / datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
        "        g[\"LATEST_DIR\"] = g[\"RESULTS_ROOT\"] / \"latest\"\n",
        "\n",
        "        for p in (g[\"ARTIFACTS_DIR\"], g[\"RESULTS_DIR\"], g[\"LATEST_DIR\"]):\n",
        "            p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        g[\"BARS_FEED\"] = str(self.BARS_FEED).strip()\n",
        "        g[\"COOLDOWN_MIN\"] = int(self.COOLDOWN_MIN)\n",
        "        g[\"STALE_MAX_SEC\"] = int(self.STALE_MAX_SEC)\n",
        "        g[\"SIZING_MODE\"] = self.SIZING_MODE\n",
        "        g[\"WEIGHT_CAP\"] = float(self.WEIGHT_CAP)\n",
        "        g[\"ENTER_CONF_MIN\"] = float(self.ENTER_CONF_MIN)\n",
        "        g[\"ENTER_WEIGHT_MIN\"] = float(self.ENTER_WEIGHT_MIN)\n",
        "        g[\"EXIT_WEIGHT_MAX\"] = float(self.EXIT_WEIGHT_MAX)\n",
        "        g[\"REBALANCE_MIN_NOTIONAL\"] = float(self.REBALANCE_MIN_NOTIONAL)\n",
        "        g[\"USE_FRACTIONALS\"] = bool(self.USE_FRACTIONALS)\n",
        "        g[\"SEED_FIRST_SHARE\"] = bool(self.SEED_FIRST_SHARE)\n",
        "        g[\"ALLOW_SHORTS\"] = bool(self.ALLOW_SHORTS)\n",
        "        g[\"CONF_FLOOR\"] = float(self.CONF_FLOOR)\n",
        "        g[\"TAKE_PROFIT_PCT\"] = float(self.TAKE_PROFIT_PCT)\n",
        "        g[\"STOP_LOSS_PCT\"] = float(self.STOP_LOSS_PCT)\n",
        "        g[\"BEST_WINDOW_ENV\"] = (self.STALE_BEST_WINDOW or None)\n",
        "\n",
        "        # creds (resolved later too)\n",
        "        g[\"API_KEY\"] = self.APCA_API_KEY_ID or \"\"\n",
        "        g[\"API_SECRET\"] = self.APCA_API_SECRET_KEY or \"\"\n",
        "\n",
        "        g[\"DELTA_WEIGHT_MIN\"] = float(self.DELTA_WEIGHT_MIN)\n",
        "        g[\"RAW_POS_MIN\"] = float(self.RAW_POS_MIN)\n",
        "        g[\"RAW_NEG_MAX\"] = float(self.RAW_NEG_MAX)\n",
        "\n",
        "        g[\"TRADE_LOG_CSV\"] = g[\"RESULTS_DIR\"] / \"trade_log_master.csv\"\n",
        "        g[\"EQUITY_LOG_CSV\"] = g[\"RESULTS_DIR\"] / \"equity_log.csv\"\n",
        "        g[\"PLOT_PATH\"] = g[\"RESULTS_DIR\"] / \"equity_curve.png\"\n",
        "        g[\"PLOT_PATH_LATEST\"] = g[\"LATEST_DIR\"] / \"equity_curve.png\"\n",
        "        g[\"EQUITY_LOG_LATEST\"] = g[\"LATEST_DIR\"] / \"equity_log.csv\"\n",
        "        g[\"TRADE_LOG_LATEST\"] = g[\"LATEST_DIR\"] / \"trade_log_master.csv\"\n",
        "\n",
        "        g[\"EQUITY_LOG_THROTTLE_SEC\"] = int(self.EQUITY_LOG_THROTTLE_SEC)\n",
        "        g[\"SKIP_EQUITY_WHEN_DRY_RUN\"] = bool(self.SKIP_EQUITY_WHEN_DRY_RUN)\n",
        "        g[\"_LAST_EQUITY_LOG_TS\"] = 0\n",
        "        g[\"_TRADE_EVENT_FLAG\"] = False\n",
        "\n",
        "        g[\"MAX_DAILY_DRAWDOWN_PCT\"] = float(self.MAX_DAILY_DRAWDOWN_PCT)\n",
        "        g[\"KILL_SWITCH_COOLDOWN_MIN\"] = int(self.KILL_SWITCH_COOLDOWN_MIN)\n",
        "        g[\"EXIT_AFTER_CLOSE\"] = bool(self.EXIT_AFTER_CLOSE)\n",
        "\n",
        "        g[\"FLATTEN_INTO_CLOSE\"] = bool(self.FLATTEN_INTO_CLOSE)\n",
        "        g[\"FORCE_FIRST_BUY\"] = bool(self.FORCE_FIRST_BUY)\n",
        "        g[\"FORCE_FLATTEN_ON_EXIT\"] = bool(self.FORCE_FLATTEN_ON_EXIT)\n",
        "\n",
        "        g[\"DATA_TIMEFRAME\"] = str(self.DATA_TIMEFRAME)\n",
        "        g[\"TRAIN_TIMEFRAME\"] = str(self.TRAIN_TIMEFRAME)\n",
        "        g[\"EQUITY_TIMEFRAME\"] = str(self.EQUITY_TIMEFRAME)\n",
        "\n",
        "        # (D)\n",
        "        g[\"GROSS_CAP\"] = float(self.GROSS_CAP)\n",
        "        g[\"NET_CAP\"] = float(self.NET_CAP)\n",
        "        g[\"EMERGENCY_FLATTEN_ON_EXPOSURE\"] = bool(self.EMERGENCY_FLATTEN_ON_EXPOSURE)\n",
        "\n",
        "        os.environ[\"EXIT_AFTER_CLOSE\"] = \"1\" if self.EXIT_AFTER_CLOSE else \"0\"\n",
        "        os.environ[\"APCA_API_BASE_URL\"] = self.APCA_API_BASE_URL\n",
        "        os.environ[\"DRY_RUN\"] = \"1\" if self.DRY_RUN else \"0\"\n",
        "        os.environ[\"AUTO_RUN_LIVE\"] = \"1\" if self.AUTO_RUN_LIVE else \"0\"\n",
        "        os.environ[\"BARS_FEED\"] = self.BARS_FEED\n",
        "\n",
        "def configure_knobs(overrides: Mapping[str, object] = None) -> Knobs:\n",
        "    defaults = Knobs(\n",
        "        TICKERS=_to_list_csv(os.getenv(\"TICKERS\", \"UNH,GE\")),\n",
        "        ARTIFACTS_DIR=os.getenv(\"ARTIFACTS_DIR\", str(PROJECT_ROOT / \"artifacts\")),\n",
        "        RESULTS_ROOT=os.getenv(\"RESULTS_ROOT\", str(PROJECT_ROOT / \"results\")),\n",
        "        DATA_TIMEFRAME=os.getenv(\"DATA_TIMEFRAME\", \"1H\"),\n",
        "        TRAIN_TIMEFRAME=os.getenv(\"TRAIN_TIMEFRAME\", \"1H\"),\n",
        "        EQUITY_TIMEFRAME=os.getenv(\"EQUITY_TIMEFRAME\", \"5Min\"),\n",
        "    )\n",
        "    cfg = Knobs.from_env(defaults, PROJECT_ROOT, os.environ, overrides=overrides)\n",
        "    cfg.apply_to_globals()\n",
        "    return cfg\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Time helpers\n",
        "# ============================================================\n",
        "def ensure_utc(ts_like) -> pd.Timestamp:\n",
        "    ts = pd.Timestamp(ts_like)\n",
        "    if ts.tzinfo is None:\n",
        "        return ts.tz_localize(\"UTC\")\n",
        "    return ts.tz_convert(\"UTC\")\n",
        "\n",
        "def now_utc() -> datetime:\n",
        "    return datetime.now(timezone.utc)\n",
        "\n",
        "def utcnow_iso() -> str:\n",
        "    return datetime.now(timezone.utc).isoformat()\n",
        "\n",
        "def utc_ts(dt_like) -> int:\n",
        "    ts = ensure_utc(dt_like)\n",
        "    return int(ts.value // 10**9)\n",
        "\n",
        "def _sleep_to_next_minute_block(n: int):\n",
        "    n = max(1, int(n))\n",
        "    now = now_utc()\n",
        "    base = now.replace(second=0, microsecond=0)\n",
        "    remainder = base.minute % n\n",
        "    add = (n - remainder) % n\n",
        "    if add == 0:\n",
        "        add = n\n",
        "    next_slot = base + timedelta(minutes=add)\n",
        "    time.sleep(max(0.0, (next_slot - now).total_seconds()))\n",
        "\n",
        "def is_hour_close(ts: pd.Timestamp) -> bool:\n",
        "    ts = ensure_utc(ts)\n",
        "    return (ts.minute == 0) and (ts.second == 0)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Trade logging (master)\n",
        "# ============================================================\n",
        "TRADE_FIELDS = [\"datetime_utc\", \"ticker\", \"signal\", \"action\", \"price\", \"equity\", \"qty\", \"comment\"]\n",
        "\n",
        "def ensure_trade_log_header():\n",
        "    if (not TRADE_LOG_CSV.exists()) or (TRADE_LOG_CSV.stat().st_size == 0):\n",
        "        with TRADE_LOG_CSV.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            csv.DictWriter(f, fieldnames=TRADE_FIELDS).writeheader()\n",
        "\n",
        "def log_trade(\n",
        "    ticker: str,\n",
        "    signal: float,\n",
        "    action: str,\n",
        "    price: float,\n",
        "    equity: float,\n",
        "    qty: float = None,\n",
        "    comment: str = \"\",\n",
        "):\n",
        "    ensure_trade_log_header()\n",
        "    row = {\n",
        "        \"datetime_utc\": utcnow_iso(),\n",
        "        \"ticker\": ticker,\n",
        "        \"signal\": int(signal) if signal is not None else \"\",\n",
        "        \"action\": action,\n",
        "        \"price\": (float(price) if price is not None and np.isfinite(price) else \"\"),\n",
        "        \"equity\": (float(equity) if equity is not None and np.isfinite(equity) else \"\"),\n",
        "        \"qty\": (float(qty) if qty is not None and np.isfinite(qty) else \"\"),\n",
        "        \"comment\": (str(comment) if comment else \"\"),\n",
        "    }\n",
        "    with TRADE_LOG_CSV.open(\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        csv.DictWriter(f, fieldnames=TRADE_FIELDS).writerow(row)\n",
        "    try:\n",
        "        shutil.copy2(TRADE_LOG_CSV, TRADE_LOG_LATEST)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Alpaca clients (single source of truth) — 02.27 pattern\n",
        "# ============================================================\n",
        "def init_clients() -> Tuple[TradingClient, StockHistoricalDataClient]:\n",
        "    api_key, api_secret, base_url = resolve_credentials()\n",
        "    # Keep paper=True, but also enforce BASE_URL later\n",
        "    trading_api = TradingClient(api_key, api_secret, paper=True, url_override=base_url)\n",
        "    data_api = StockHistoricalDataClient(api_key, api_secret)\n",
        "    return trading_api, data_api\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Timeout-safe wrapper\n",
        "# ============================================================\n",
        "from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError\n",
        "_TIMEOUT_EXEC = ThreadPoolExecutor(max_workers=8)\n",
        "\n",
        "def _call_with_timeout(func, timeout_sec: int, *args, **kwargs):\n",
        "    fut = _TIMEOUT_EXEC.submit(func, *args, **kwargs)\n",
        "    try:\n",
        "        return fut.result(timeout=timeout_sec)\n",
        "    except FuturesTimeoutError:\n",
        "        raise TimeoutError(f\"Timed out after {timeout_sec}s\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Portfolio history / equity snapshots\n",
        "# ============================================================\n",
        "def _resolve_pf_timeframe_str(timeframe, default: str = \"1Hour\") -> str:\n",
        "    tf_raw = timeframe or os.getenv(\"EQUITY_TIMEFRAME\", os.getenv(\"DATA_TIMEFRAME\", default))\n",
        "    key = normalize_tf_key(tf_raw)\n",
        "    return _PF_TF_STR.get(key, default)\n",
        "\n",
        "def _resolve_pf_timeframe_obj(timeframe, default_obj=None):\n",
        "    tf_raw = timeframe or os.getenv(\"EQUITY_TIMEFRAME\", os.getenv(\"DATA_TIMEFRAME\", \"1H\"))\n",
        "    key = normalize_tf_key(tf_raw)\n",
        "    return _TF_MAP.get(key, default_obj or TimeFrame.Hour)\n",
        "\n",
        "def get_portfolio_history_safe(\n",
        "    trading_api: TradingClient,\n",
        "    period: str = \"1M\",\n",
        "    timeframe=None,\n",
        "    extended_hours: bool = False,\n",
        "    timeout_sec: int = 8,\n",
        "    retries: int = 1,\n",
        "):\n",
        "    last_exc = None\n",
        "    try:\n",
        "        tf_obj = _resolve_pf_timeframe_obj(timeframe, default_obj=TimeFrame.Hour)\n",
        "        req = GetPortfolioHistoryRequest(period=str(period), timeframe=tf_obj, extended_hours=bool(extended_hours))\n",
        "    except Exception as e:\n",
        "        last_exc = e\n",
        "        tf_str = _resolve_pf_timeframe_str(timeframe, default=\"1Hour\")\n",
        "        req = GetPortfolioHistoryRequest(period=str(period), timeframe=tf_str, extended_hours=bool(extended_hours))\n",
        "\n",
        "    for _ in range(max(1, retries + 1)):\n",
        "        try:\n",
        "            return _call_with_timeout(trading_api.get_portfolio_history, timeout_sec, req)\n",
        "        except Exception as e:\n",
        "            last_exc = e\n",
        "            time.sleep(0.5)\n",
        "\n",
        "    logging.warning(f\"get_portfolio_history_safe failed: {last_exc}\")\n",
        "    return None\n",
        "\n",
        "def fetch_portfolio_history(period=\"1D\", timeframe=None, trading_api_in=None):\n",
        "    trading_api = trading_api_in if trading_api_in is not None else globals().get(\"trading_api\", None)\n",
        "    if trading_api is None:\n",
        "        return pd.DataFrame(columns=[\"timestamp_utc\", \"equity\"])\n",
        "\n",
        "    tf_raw = timeframe or os.getenv(\"EQUITY_TIMEFRAME\", os.getenv(\"DATA_TIMEFRAME\", \"1H\"))\n",
        "    timeout_sec = int(os.getenv(\"PH_TIMEOUT_SEC\", \"8\"))\n",
        "    hist = get_portfolio_history_safe(\n",
        "        trading_api,\n",
        "        period=str(period),\n",
        "        timeframe=tf_raw,\n",
        "        extended_hours=False,\n",
        "        timeout_sec=timeout_sec,\n",
        "        retries=1,\n",
        "    )\n",
        "\n",
        "    if (not hist) or (not getattr(hist, \"timestamp\", None)) or (not getattr(hist, \"equity\", None)):\n",
        "        if EQUITY_LOG_CSV.exists():\n",
        "            try:\n",
        "                df = pd.read_csv(EQUITY_LOG_CSV, parse_dates=[\"datetime_utc\"])\n",
        "                return df.rename(columns={\"datetime_utc\": \"timestamp_utc\"})[[\"timestamp_utc\", \"equity\"]]\n",
        "            except Exception:\n",
        "                pass\n",
        "        return pd.DataFrame(columns=[\"timestamp_utc\", \"equity\"])\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        \"timestamp_utc\": pd.to_datetime(hist.timestamp, unit=\"s\", utc=True),\n",
        "        \"equity\": pd.to_numeric(pd.Series(hist.equity), errors=\"coerce\"),\n",
        "    }).dropna()\n",
        "\n",
        "def log_equity_snapshot(trading_api_in=None):\n",
        "    trading_api = trading_api_in if trading_api_in is not None else globals().get(\"trading_api\", None)\n",
        "    if trading_api is None:\n",
        "        return\n",
        "\n",
        "    snap = fetch_portfolio_history(\n",
        "        period=\"1D\",\n",
        "        timeframe=os.getenv(\"EQUITY_TIMEFRAME\", \"5Min\"),\n",
        "        trading_api_in=trading_api\n",
        "    )\n",
        "    if snap.empty:\n",
        "        return\n",
        "\n",
        "    latest = snap.iloc[-1:].copy().rename(columns={\"timestamp_utc\": \"datetime_utc\"})\n",
        "\n",
        "    if EQUITY_LOG_CSV.exists() and EQUITY_LOG_CSV.stat().st_size > 0:\n",
        "        try:\n",
        "            df_old = pd.read_csv(EQUITY_LOG_CSV, parse_dates=[\"datetime_utc\"])\n",
        "        except Exception:\n",
        "            df_old = pd.DataFrame(columns=[\"datetime_utc\", \"equity\"])\n",
        "\n",
        "        if (not df_old.empty) and (pd.to_datetime(df_old[\"datetime_utc\"].iloc[-1]) == latest[\"datetime_utc\"].iloc[0]):\n",
        "            return\n",
        "\n",
        "        out = (\n",
        "            pd.concat([df_old, latest], ignore_index=True)\n",
        "              .drop_duplicates(subset=[\"datetime_utc\"], keep=\"last\")\n",
        "              .sort_values(\"datetime_utc\")\n",
        "        )\n",
        "        out.to_csv(EQUITY_LOG_CSV, index=False)\n",
        "    else:\n",
        "        latest.to_csv(EQUITY_LOG_CSV, index=False)\n",
        "\n",
        "    try:\n",
        "        shutil.copy2(EQUITY_LOG_CSV, EQUITY_LOG_LATEST)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def maybe_log_equity_snapshot(trading_api_in=None, reason=None):\n",
        "    global _LAST_EQUITY_LOG_TS, _TRADE_EVENT_FLAG\n",
        "    trading_api = trading_api_in if trading_api_in is not None else globals().get(\"trading_api\", None)\n",
        "    if trading_api is None:\n",
        "        return\n",
        "\n",
        "    if reason is None:\n",
        "        reason = (\"trade\" if bool(_TRADE_EVENT_FLAG) else \"cycle\")\n",
        "\n",
        "    if bool(globals().get(\"DRY_RUN\", False)) and bool(globals().get(\"SKIP_EQUITY_WHEN_DRY_RUN\", True)):\n",
        "        return\n",
        "\n",
        "    now_ts = time.time()\n",
        "    force = reason in {\"trade\", \"finalize\", \"close\"}\n",
        "    last_ts = float(_LAST_EQUITY_LOG_TS or 0.0)\n",
        "    throttle = int(globals().get(\"EQUITY_LOG_THROTTLE_SEC\", 900))\n",
        "\n",
        "    if force or (now_ts - last_ts) >= throttle:\n",
        "        try:\n",
        "            log_equity_snapshot(trading_api_in=trading_api)\n",
        "            _LAST_EQUITY_LOG_TS = now_ts\n",
        "        except Exception as e:\n",
        "            logging.debug(f\"maybe_log_equity_snapshot failed: {e}\")\n",
        "\n",
        "    if reason == \"trade\":\n",
        "        _TRADE_EVENT_FLAG = False\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Run summary logging (append-only)\n",
        "# ============================================================\n",
        "def _safe_float(x):\n",
        "    try:\n",
        "        v = float(x)\n",
        "        return v if np.isfinite(v) else float(\"nan\")\n",
        "    except Exception:\n",
        "        return float(\"nan\")\n",
        "\n",
        "def _ensure_csv_header(path: Path, fieldnames: List[str]) -> None:\n",
        "    if (not path.exists()) or (path.stat().st_size == 0):\n",
        "        with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            csv.DictWriter(f, fieldnames=fieldnames).writeheader()\n",
        "\n",
        "def _run_summary_fieldnames(tickers_u: List[str]) -> List[str]:\n",
        "    syms = [str(s).upper() for s in tickers_u]\n",
        "    fields = [\n",
        "        \"datetime_utc\",\n",
        "        \"equity\",\n",
        "        \"cash\",\n",
        "        \"total_market_value\",\n",
        "        \"gross_exposure_pct\",\n",
        "        \"net_exposure_pct\",\n",
        "        \"error\",\n",
        "    ]\n",
        "    for s in syms:\n",
        "        fields += [\n",
        "            f\"target_weight_{s}\",\n",
        "            f\"actual_weight_{s}\",\n",
        "            f\"position_qty_{s}\",\n",
        "            f\"position_mv_{s}\",\n",
        "            f\"raw_action_{s}\",\n",
        "            f\"confidence_{s}\",\n",
        "        ]\n",
        "    return fields\n",
        "\n",
        "def append_run_summary(\n",
        "    trading_api: TradingClient,\n",
        "    tickers: List[str],\n",
        "    results_dir: Path,\n",
        "    latest_dir: Optional[Path] = None,\n",
        "    error: str = \"\",\n",
        ") -> None:\n",
        "    try:\n",
        "        ts = utcnow_iso()\n",
        "        row: Dict[str, Any] = {\"datetime_utc\": ts, \"error\": (str(error)[:300] if error else \"\")}\n",
        "\n",
        "        acct = None\n",
        "        try:\n",
        "            acct = trading_api.get_account()\n",
        "            row[\"equity\"] = _safe_float(getattr(acct, \"equity\", float(\"nan\")))\n",
        "            row[\"cash\"] = _safe_float(getattr(acct, \"cash\", float(\"nan\")))\n",
        "        except Exception as e:\n",
        "            row[\"equity\"] = float(\"nan\")\n",
        "            row[\"cash\"] = float(\"nan\")\n",
        "            row[\"error\"] = (row[\"error\"] + f\" | get_account:{e}\")[:300] if row[\"error\"] else f\"get_account:{e}\"[:300]\n",
        "\n",
        "        equity = _safe_float(row.get(\"equity\", float(\"nan\")))\n",
        "\n",
        "        positions = []\n",
        "        try:\n",
        "            positions = trading_api.get_all_positions()\n",
        "        except Exception as e:\n",
        "            row[\"error\"] = (row[\"error\"] + f\" | list_positions:{e}\")[:300] if row[\"error\"] else f\"list_positions:{e}\"[:300]\n",
        "            positions = []\n",
        "\n",
        "        all_pos_mv = []\n",
        "        for p in positions or []:\n",
        "            try:\n",
        "                mv = _safe_float(getattr(p, \"market_value\", float(\"nan\")))  # signed\n",
        "                if np.isfinite(mv):\n",
        "                    all_pos_mv.append(mv)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        total_mv_all = float(np.nansum(all_pos_mv)) if all_pos_mv else 0.0\n",
        "        gross_mv_all = float(np.nansum([abs(x) for x in all_pos_mv])) if all_pos_mv else 0.0\n",
        "        row[\"total_market_value\"] = _safe_float(total_mv_all)\n",
        "        if np.isfinite(equity) and equity > 0:\n",
        "            row[\"gross_exposure_pct\"] = gross_mv_all / equity\n",
        "            row[\"net_exposure_pct\"] = total_mv_all / equity\n",
        "        else:\n",
        "            row[\"gross_exposure_pct\"] = float(\"nan\")\n",
        "            row[\"net_exposure_pct\"] = float(\"nan\")\n",
        "\n",
        "        tickers_u = [str(s).upper() for s in tickers]\n",
        "        pos_qty: Dict[str, float] = {s: 0.0 for s in tickers_u}\n",
        "        pos_mv: Dict[str, float] = {s: 0.0 for s in tickers_u}\n",
        "\n",
        "        for p in positions or []:\n",
        "            try:\n",
        "                sym = str(getattr(p, \"symbol\", \"\")).upper()\n",
        "                if sym not in pos_qty:\n",
        "                    continue\n",
        "                q = _safe_float(getattr(p, \"qty\", 0.0))\n",
        "                mv = _safe_float(getattr(p, \"market_value\", 0.0))\n",
        "                pos_qty[sym] = q\n",
        "                pos_mv[sym] = mv\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        for s in tickers_u:\n",
        "            tw = _safe_float(_LAST_TARGET_W.get(s, float(\"nan\")))\n",
        "            rw = _safe_float(_LAST_RAW_A.get(s, float(\"nan\")))\n",
        "            cf = _safe_float(_LAST_CONF.get(s, float(\"nan\")))\n",
        "\n",
        "            mv = _safe_float(pos_mv.get(s, 0.0))\n",
        "            aq = _safe_float(pos_qty.get(s, 0.0))\n",
        "            aw = (mv / equity) if (np.isfinite(equity) and equity > 0) else float(\"nan\")\n",
        "\n",
        "            row[f\"target_weight_{s}\"] = tw\n",
        "            row[f\"actual_weight_{s}\"] = aw\n",
        "            row[f\"position_qty_{s}\"] = aq\n",
        "            row[f\"position_mv_{s}\"] = mv\n",
        "            row[f\"raw_action_{s}\"] = rw\n",
        "            row[f\"confidence_{s}\"] = cf\n",
        "\n",
        "        out_path = results_dir / \"run_summary.csv\"\n",
        "        fieldnames = _run_summary_fieldnames(tickers_u)\n",
        "        _ensure_csv_header(out_path, fieldnames)\n",
        "        out_key = str(out_path.resolve())\n",
        "\n",
        "        old_header = _RUN_SUMMARY_HEADER_CACHE.get(out_key)\n",
        "        if old_header is None:\n",
        "            try:\n",
        "                with out_path.open(\"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "                    old_header = next(csv.reader(f))\n",
        "            except Exception:\n",
        "                old_header = []\n",
        "            _RUN_SUMMARY_HEADER_CACHE[out_key] = list(old_header)\n",
        "\n",
        "        if old_header and old_header != fieldnames:\n",
        "            schema_stamp = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
        "            rotated_main = results_dir / f\"run_summary__oldschema__{schema_stamp}.csv\"\n",
        "            try:\n",
        "                out_path.replace(rotated_main)\n",
        "            except Exception:\n",
        "                try:\n",
        "                    shutil.copy2(out_path, rotated_main)\n",
        "                    out_path.unlink(missing_ok=True)\n",
        "                except Exception:\n",
        "                    pass\n",
        "            _ensure_csv_header(out_path, fieldnames)\n",
        "            _RUN_SUMMARY_HEADER_CACHE[out_key] = list(fieldnames)\n",
        "\n",
        "        with out_path.open(\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            w = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "            w.writerow({k: row.get(k, \"\") for k in fieldnames})\n",
        "\n",
        "        _RUN_SUMMARY_HEADER_CACHE[out_key] = list(fieldnames)\n",
        "\n",
        "        if latest_dir is not None:\n",
        "            try:\n",
        "                latest_dir.mkdir(parents=True, exist_ok=True)\n",
        "                shutil.copy2(out_path, latest_dir / \"run_summary.csv\")\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    except Exception as fatal:\n",
        "        try:\n",
        "            out_path = results_dir / \"run_summary.csv\"\n",
        "            fieldnames = _run_summary_fieldnames([str(s).upper() for s in tickers])\n",
        "            _ensure_csv_header(out_path, fieldnames)\n",
        "            fallback = {k: \"\" for k in fieldnames}\n",
        "            fallback[\"datetime_utc\"] = utcnow_iso()\n",
        "            fallback[\"error\"] = f\"append_run_summary_fatal:{str(fatal)[:260]}\"\n",
        "            with out_path.open(\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "                csv.DictWriter(f, fieldnames=fieldnames).writerow(fallback)\n",
        "            if latest_dir is not None:\n",
        "                try:\n",
        "                    latest_dir.mkdir(parents=True, exist_ok=True)\n",
        "                    shutil.copy2(out_path, latest_dir / \"run_summary.csv\")\n",
        "                except Exception:\n",
        "                    pass\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Plotting + metrics\n",
        "# ============================================================\n",
        "def plot_equity_curve(from_equity_csv: bool = True):\n",
        "    with plt.ioff():\n",
        "        if from_equity_csv and EQUITY_LOG_CSV.exists():\n",
        "            df = pd.read_csv(EQUITY_LOG_CSV, parse_dates=[\"datetime_utc\"]).sort_values(\"datetime_utc\")\n",
        "        else:\n",
        "            df = fetch_portfolio_history(period=\"3M\", timeframe=os.getenv(\"EQUITY_TIMEFRAME\", \"5Min\")) \\\n",
        "                .rename(columns={\"timestamp_utc\": \"datetime_utc\"})\n",
        "\n",
        "        if df.empty:\n",
        "            print(\"No equity data to plot yet.\")\n",
        "            return\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 4))\n",
        "        ax.plot(df[\"datetime_utc\"], df[\"equity\"])\n",
        "        ax.set_title(\"Portfolio Value Over Time (Paper)\")\n",
        "        ax.set_xlabel(\"Time (UTC)\")\n",
        "        ax.set_ylabel(\"Equity ($)\")\n",
        "        fig.tight_layout()\n",
        "        fig.savefig(PLOT_PATH, bbox_inches=\"tight\")\n",
        "        fig.savefig(PLOT_PATH_LATEST, bbox_inches=\"tight\")\n",
        "        plt.close(fig)\n",
        "        print(f\"Saved equity curve → {PLOT_PATH}\")\n",
        "        print(f\"Updated latest copy → {PLOT_PATH_LATEST}\")\n",
        "\n",
        "def compute_performance_metrics(df_equity: pd.DataFrame):\n",
        "    if df_equity.empty or df_equity[\"equity\"].isna().all():\n",
        "        return {\"cum_return\": np.nan, \"sharpe\": np.nan, \"max_drawdown\": np.nan}\n",
        "\n",
        "    df = df_equity.sort_values(\"datetime_utc\")\n",
        "    e = df[\"equity\"].astype(float)\n",
        "    r = e.pct_change().dropna()\n",
        "    if r.empty:\n",
        "        return {\"cum_return\": 0.0, \"sharpe\": np.nan, \"max_drawdown\": np.nan}\n",
        "\n",
        "    dt_sec = df[\"datetime_utc\"].diff().dt.total_seconds().dropna().median()\n",
        "    if not (isinstance(dt_sec, (int, float)) and dt_sec > 0):\n",
        "        periods_per_year = 252 * 78\n",
        "    else:\n",
        "        periods_per_day = (6.5 * 3600) / dt_sec\n",
        "        periods_per_year = 252 * periods_per_day\n",
        "\n",
        "    sharpe = (r.mean() / (r.std() + 1e-12)) * math.sqrt(periods_per_year)\n",
        "    cum = (1 + r).cumprod()\n",
        "    peak = cum.cummax()\n",
        "    dd = (cum / peak - 1.0).min()\n",
        "    cum_return = e.iloc[-1] / e.iloc[0] - 1.0\n",
        "    return {\"cum_return\": float(cum_return), \"sharpe\": float(sharpe), \"max_drawdown\": float(dd)}\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Per-ticker CSV logging\n",
        "# ============================================================\n",
        "def _append_csv_row(path: Path, row: dict):\n",
        "    fieldnames = list(row.keys())\n",
        "\n",
        "    if not path.exists():\n",
        "        with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            w = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "            w.writeheader()\n",
        "            w.writerow(row)\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        with path.open(\"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            old_header = next(csv.reader(f))\n",
        "    except Exception:\n",
        "        old_header = []\n",
        "\n",
        "    if old_header != fieldnames:\n",
        "        tmp = path.with_suffix(\".tmp\")\n",
        "        with tmp.open(\"w\", newline=\"\", encoding=\"utf-8\") as wf, path.open(\"r\", newline=\"\", encoding=\"utf-8\") as rf:\n",
        "            r = csv.DictReader(rf) if old_header else None\n",
        "            w = csv.DictWriter(wf, fieldnames=fieldnames)\n",
        "            w.writeheader()\n",
        "            if r:\n",
        "                for old_row in r:\n",
        "                    merged = {k: old_row.get(k, \"\") for k in fieldnames}\n",
        "                    w.writerow(merged)\n",
        "        tmp.replace(path)\n",
        "\n",
        "    with path.open(\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        csv.DictWriter(f, fieldnames=fieldnames).writerow(row)\n",
        "\n",
        "def log_trade_symbol(\n",
        "    symbol: str,\n",
        "    bar_time,\n",
        "    raw_action: float,\n",
        "    weight: float,\n",
        "    confidence: float,\n",
        "    price: float,\n",
        "    equity: float,\n",
        "    dry_run: bool,\n",
        "    note: str = \"\",\n",
        "    order_submitted: int = 0,\n",
        "    order_id: str = \"\",\n",
        "    order_status: str = \"\",\n",
        "    filled_qty: str = \"\",\n",
        "):\n",
        "    try:\n",
        "        bt = pd.Timestamp(bar_time)\n",
        "        if bt.tzinfo is None:\n",
        "            bt = bt.tz_localize(\"UTC\")\n",
        "        else:\n",
        "            bt = bt.tz_convert(\"UTC\")\n",
        "        bt_iso = bt.isoformat()\n",
        "        age_sec = int((now_utc() - bt.to_pydatetime()).total_seconds())\n",
        "    except Exception:\n",
        "        bt_iso = \"\"\n",
        "        age_sec = \"\"\n",
        "\n",
        "    resolved_feed = (os.getenv(\"BARS_FEED\", \"\").strip() or \"default\")\n",
        "\n",
        "    if abs(float(weight)) <= float(globals().get(\"EXIT_WEIGHT_MAX\", 0.0)):\n",
        "        sig = \"FLAT\"\n",
        "    elif float(weight) > 0:\n",
        "        sig = \"LONG\"\n",
        "    else:\n",
        "        sig = \"SHORT\"\n",
        "\n",
        "    row = {\n",
        "        \"log_time\": now_utc().isoformat(),\n",
        "        \"symbol\": symbol,\n",
        "        \"bar_time\": bt_iso,\n",
        "        \"bar_age_sec\": age_sec,\n",
        "        \"feed\": resolved_feed,\n",
        "        \"signal\": sig,\n",
        "        \"raw_action\": float(raw_action) if raw_action is not None and np.isfinite(raw_action) else \"\",\n",
        "        \"weight\": float(weight) if weight is not None and np.isfinite(weight) else \"\",\n",
        "        \"confidence\": float(confidence) if confidence is not None and np.isfinite(confidence) else \"\",\n",
        "        \"price\": float(price) if price is not None and np.isfinite(price) else \"\",\n",
        "        \"equity\": float(equity) if equity is not None and np.isfinite(equity) else \"\",\n",
        "        \"dry_run\": int(bool(dry_run)),\n",
        "        \"note\": note,\n",
        "        \"order_submitted\": int(order_submitted),\n",
        "        \"order_id\": str(order_id or \"\"),\n",
        "        \"order_status\": str(order_status or \"\"),\n",
        "        \"filled_qty\": str(filled_qty or \"\"),\n",
        "    }\n",
        "\n",
        "    _append_csv_row(RESULTS_DIR / f\"trade_log_{symbol}.csv\", row)\n",
        "\n",
        "    try:\n",
        "        action = str(note)[:64]\n",
        "        comment = str(note)[:200]\n",
        "        master_sig = -1 if sig == \"SHORT\" else (1 if sig == \"LONG\" else 0)\n",
        "        log_trade(\n",
        "            ticker=symbol,\n",
        "            signal=master_sig,\n",
        "            action=action,\n",
        "            price=float(price) if (price is not None and np.isfinite(price)) else None,\n",
        "            equity=float(equity) if (equity is not None and np.isfinite(equity)) else None,\n",
        "            qty=None,\n",
        "            comment=comment,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logging.debug(\"master trade log write failed: %s\", e)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Artifacts: picker & loaders\n",
        "# ============================================================\n",
        "def _extract_window_idx(path: Path) -> Optional[int]:\n",
        "    m = re.search(r\"_window(\\d+)\", path.stem)\n",
        "    return int(m.group(1)) if m else None\n",
        "\n",
        "def _prefer_same_window(cands, w: Optional[int]):\n",
        "    cands = list(cands)\n",
        "    if not cands:\n",
        "        return []\n",
        "    if w is None:\n",
        "        return sorted(cands)\n",
        "    same = [p for p in cands if _extract_window_idx(p) == w]\n",
        "    return sorted(same or cands)\n",
        "\n",
        "def pick_artifacts_for_ticker(ticker: str, artifacts_dir: str, best_window: Optional[str] = None) -> Dict[str, Optional[Path]]:\n",
        "    p = Path(artifacts_dir)\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f\"Artifacts directory not found: {p.resolve()}\")\n",
        "\n",
        "    models = sorted(p.glob(f\"ppo_{ticker}_window*_model*.zip\"))\n",
        "    if not models:\n",
        "        models = (sorted(p.glob(f\"ppo_{ticker}_model*.zip\")) or sorted(p.glob(f\"*{ticker}*model*.zip\")))\n",
        "    if not models:\n",
        "        raise FileNotFoundError(f\"No PPO model zip found for {ticker} in {p}\")\n",
        "\n",
        "    def _model_sort_key(path: Path):\n",
        "        w = _extract_window_idx(path)\n",
        "        return (w if w is not None else -1, \" (1)\" in path.stem)\n",
        "\n",
        "    models = sorted(models, key=_model_sort_key)\n",
        "\n",
        "    chosen: Optional[Path] = None\n",
        "    if best_window:\n",
        "        chosen = next((m for m in models if f\"_window{best_window}_\" in m.stem), None)\n",
        "        if chosen is None:\n",
        "            logging.warning(\"[%s] BEST_WINDOW=%s not found; falling back.\", ticker, best_window)\n",
        "\n",
        "    if chosen is None:\n",
        "        with_idx = [(m, _extract_window_idx(m)) for m in models]\n",
        "        with_idx = [(m, w) for (m, w) in with_idx if w is not None]\n",
        "        chosen = max(with_idx, key=lambda t: t[1])[0] if with_idx else models[-1]\n",
        "\n",
        "    chosen_w = _extract_window_idx(chosen)\n",
        "\n",
        "    vec_candidates = list(p.glob(f\"ppo_{ticker}_window*_vecnorm*.pkl\"))\n",
        "    feat_candidates = list(p.glob(f\"ppo_{ticker}_window*_features*.json\"))\n",
        "\n",
        "    vecnorm = (_prefer_same_window(vec_candidates, chosen_w)[0] if vec_candidates else None)\n",
        "    feats = (_prefer_same_window(feat_candidates, chosen_w)[0] if feat_candidates else None)\n",
        "\n",
        "    logging.info(\"[%s] model=%s | window=%s | vecnorm=%s | features=%s\",\n",
        "                 ticker, chosen.name, chosen_w,\n",
        "                 vecnorm.name if vecnorm else \"None\",\n",
        "                 feats.name if feats else \"None\")\n",
        "    return {\"model\": chosen, \"vecnorm\": vecnorm, \"features\": feats}\n",
        "\n",
        "def load_vecnormalize(path: Optional[Path]):\n",
        "    if path is None:\n",
        "        return None\n",
        "    try:\n",
        "        with open(path, \"rb\") as f:\n",
        "            return pickle.load(f)\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        return VecNormalize.load(str(path), venv=None)\n",
        "    except Exception as e:\n",
        "        logging.warning(\"VecNormalize load failed (%s). Proceeding without normalization.\", e)\n",
        "        return None\n",
        "\n",
        "def load_features(path: Optional[Path]):\n",
        "    if path is None:\n",
        "        return None\n",
        "    with open(path, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def _const_schedule(val: float):\n",
        "    return lambda _progress_remaining: float(val)\n",
        "\n",
        "def load_ppo_model(model_path: Path):\n",
        "    custom_objects = {\n",
        "        \"lr_schedule\": _const_schedule(5e-5),\n",
        "        \"clip_range\":  _const_schedule(0.2),\n",
        "        \"clip_range_vf\": _const_schedule(0.2),\n",
        "    }\n",
        "    return PPO.load(str(model_path), custom_objects=custom_objects)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Market / account / orders helpers\n",
        "# ============================================================\n",
        "def ensure_market_open(trading_api: TradingClient) -> bool:\n",
        "    try:\n",
        "        return bool(trading_api.get_clock().is_open)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def get_account_equity(trading_api: TradingClient) -> float:\n",
        "    return float(trading_api.get_account().equity)\n",
        "\n",
        "def get_position(trading_api: TradingClient, symbol: str):\n",
        "    try:\n",
        "        return trading_api.get_position(symbol)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def asset_flags(trading_api: TradingClient, symbol: str) -> Tuple[bool, bool, bool]:\n",
        "    try:\n",
        "        a = trading_api.get_asset(symbol)\n",
        "        return (\n",
        "            bool(getattr(a, \"tradable\", True)),\n",
        "            bool(getattr(a, \"fractionable\", False)),\n",
        "            bool(getattr(a, \"shortable\", False)),\n",
        "        )\n",
        "    except Exception:\n",
        "        return True, False, False\n",
        "\n",
        "@lru_cache(maxsize=256)\n",
        "def _asset_flags(symbol: str) -> Tuple[bool, bool, bool]:\n",
        "    try:\n",
        "        trading_api = globals().get(\"trading_api\", None)\n",
        "        if trading_api is None:\n",
        "            return True, False, False\n",
        "        a = trading_api.get_asset(symbol)\n",
        "        return (\n",
        "            bool(getattr(a, \"tradable\", True)),\n",
        "            bool(getattr(a, \"fractionable\", False)),\n",
        "            bool(getattr(a, \"shortable\", False)),\n",
        "        )\n",
        "    except Exception:\n",
        "        return True, False, False\n",
        "\n",
        "def can_short_symbol(trading_api: TradingClient, symbol: str) -> bool:\n",
        "    if not bool(globals().get(\"ALLOW_SHORTS\", False)):\n",
        "        return False\n",
        "    try:\n",
        "        acct = trading_api.get_account()\n",
        "        shorting_ok = bool(getattr(acct, \"shorting_enabled\", False))\n",
        "    except Exception:\n",
        "        shorting_ok = False\n",
        "    try:\n",
        "        a = trading_api.get_asset(symbol)\n",
        "        asset_ok = bool(getattr(a, \"shortable\", False))\n",
        "    except Exception:\n",
        "        asset_ok = False\n",
        "    return shorting_ok and asset_ok\n",
        "\n",
        "def _side_enum(side: str) -> OrderSide:\n",
        "    s = (side or \"\").strip().lower()\n",
        "    if s in (\"buy\", \"long\"):\n",
        "        return OrderSide.BUY\n",
        "    if s in (\"sell\", \"short\"):\n",
        "        return OrderSide.SELL\n",
        "    raise ValueError(f\"Invalid side: {side!r} (expected 'buy' or 'sell')\")\n",
        "\n",
        "def _truncate_toward_zero(x: float) -> int:\n",
        "    # safer than floor for negatives (prevents -0.1 -> -1)\n",
        "    return int(math.trunc(float(x)))\n",
        "\n",
        "def market_order(trading_api: TradingClient, symbol: str, side: str, qty=None, notional: float = None):\n",
        "    if qty is not None and notional is not None:\n",
        "        logging.warning(f\"[{symbol}] Both qty and notional provided; preferring notional.\")\n",
        "        qty = None\n",
        "    if qty is None and notional is None:\n",
        "        logging.warning(f\"[{symbol}] No order size provided; skipping.\")\n",
        "        return None\n",
        "\n",
        "    if bool(globals().get(\"DRY_RUN\", False)):\n",
        "        notional_str = to_2dp_str(notional) if notional is not None else None\n",
        "        logging.info(f\"[DRY_RUN] Would submit {side} {(('notional=$'+str(notional_str)) if notional_str else ('qty='+str(qty)))} {symbol}\")\n",
        "        globals()[\"_TRADE_EVENT_FLAG\"] = True\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        side_enum = _side_enum(side)\n",
        "\n",
        "        qty_arg = None\n",
        "        if qty is not None:\n",
        "            q = float(qty)\n",
        "            if not bool(globals().get(\"USE_FRACTIONALS\", True)):\n",
        "                q = _truncate_toward_zero(q)\n",
        "                if q == 0:\n",
        "                    logging.info(f\"[{symbol}] qty truncates to 0 shares; skipping.\")\n",
        "                    return None\n",
        "            qty_arg = float(to_6dp_str(q)) if bool(globals().get(\"USE_FRACTIONALS\", True)) else int(q)\n",
        "\n",
        "        notional_arg = None\n",
        "        if notional is not None:\n",
        "            notional_arg = float(to_2dp_str(float(notional)))\n",
        "\n",
        "        req = MarketOrderRequest(\n",
        "            symbol=str(symbol).upper(),\n",
        "            side=side_enum,\n",
        "            time_in_force=TimeInForce.DAY,\n",
        "            qty=qty_arg,\n",
        "            notional=notional_arg,\n",
        "        )\n",
        "        o = trading_api.submit_order(req)\n",
        "        size_str = f\"notional=${notional_arg}\" if notional_arg is not None else f\"qty={qty_arg}\"\n",
        "        logging.info(f\"[{symbol}] Submitted {side} {size_str}\")\n",
        "        globals()[\"_TRADE_EVENT_FLAG\"] = True\n",
        "        return o\n",
        "    except Exception as e:\n",
        "        logging.error(f\"[{symbol}] submit_order failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def market_order_to_qty(trading_api: TradingClient, symbol: str, side: str, qty):\n",
        "    if qty is None:\n",
        "        logging.warning(f\"[{symbol}] qty is None; skipping.\")\n",
        "        return None\n",
        "    try:\n",
        "        q = float(qty)\n",
        "    except Exception:\n",
        "        logging.warning(f\"[{symbol}] qty not numeric ({qty}); skipping.\")\n",
        "        return None\n",
        "\n",
        "    if not np.isfinite(q) or q <= 0:\n",
        "        logging.info(f\"[{symbol}] Non-positive qty ({qty}); skipping.\")\n",
        "        return None\n",
        "\n",
        "    use_fractionals = bool(globals().get(\"USE_FRACTIONALS\", True))\n",
        "    if not use_fractionals:\n",
        "        q_int = _truncate_toward_zero(q)\n",
        "        if q_int <= 0:\n",
        "            logging.info(f\"[{symbol}] qty truncates to 0 shares; skipping.\")\n",
        "            return None\n",
        "        q = q_int\n",
        "\n",
        "    if bool(globals().get(\"DRY_RUN\", False)):\n",
        "        logging.info(f\"[DRY_RUN] Would submit {side} qty={q} {symbol}\")\n",
        "        globals()[\"_TRADE_EVENT_FLAG\"] = True\n",
        "        return None\n",
        "\n",
        "    if side.strip().lower() == \"sell\" and (not can_short_symbol(trading_api, symbol)):\n",
        "        have_qty = get_position_qty(trading_api, symbol)\n",
        "        if have_qty <= 0:\n",
        "            logging.info(f\"[{symbol}] Sell skipped (no shares and shorting not allowed).\")\n",
        "            return None\n",
        "        q = min(float(q), float(have_qty))\n",
        "        if q <= 0:\n",
        "            logging.info(f\"[{symbol}] Sell qty clamped to 0; skipping.\")\n",
        "            return None\n",
        "\n",
        "    try:\n",
        "        side_enum = _side_enum(side)\n",
        "        qty_arg = float(to_6dp_str(float(q))) if use_fractionals else int(q)\n",
        "        req = MarketOrderRequest(symbol=str(symbol).upper(), side=side_enum, time_in_force=TimeInForce.DAY, qty=qty_arg)\n",
        "        o = trading_api.submit_order(req)\n",
        "        logging.info(f\"[{symbol}] Submitted {side} qty={qty_arg}\")\n",
        "        globals()[\"_TRADE_EVENT_FLAG\"] = True\n",
        "        return o\n",
        "    except Exception as e:\n",
        "        logging.error(f\"[{symbol}] submit_order(qty) failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def submit_fractional_rebalance(trading_api: TradingClient, symbol: str, delta_notional: float, price: float):\n",
        "    dn = round_to_cents(abs(delta_notional))\n",
        "    if dn < float(globals().get(\"REBALANCE_MIN_NOTIONAL\", 0.0)):\n",
        "        return None\n",
        "    if delta_notional > 0:\n",
        "        return market_order(trading_api, symbol, side=\"buy\", notional=dn)\n",
        "    qty = dn / max(float(price), 1e-9)\n",
        "    if not can_short_symbol(trading_api, symbol):\n",
        "        have_qty = get_position_qty(trading_api, symbol)\n",
        "        if have_qty <= 0:\n",
        "            logging.info(f\"[{symbol}] Fractional sell skipped (no shares, no shorting).\")\n",
        "            return None\n",
        "        qty = min(float(qty), float(have_qty))\n",
        "        if qty <= 0:\n",
        "            return None\n",
        "    return market_order_to_qty(trading_api, symbol, side=\"sell\", qty=qty)\n",
        "\n",
        "NO_ORDER = {\"order_submitted\": 0, \"order_id\": \"\", \"order_status\": \"\", \"filled_qty\": \"\"}\n",
        "\n",
        "def _order_info(order_obj) -> dict:\n",
        "    if order_obj is None:\n",
        "        return dict(NO_ORDER)\n",
        "    return {\n",
        "        \"order_submitted\": 1,\n",
        "        \"order_id\": str(getattr(order_obj, \"id\", \"\") or \"\"),\n",
        "        \"order_status\": str(getattr(order_obj, \"status\", \"\") or \"\"),\n",
        "        \"filled_qty\": str(getattr(order_obj, \"filled_qty\", \"\") or \"\"),\n",
        "    }\n",
        "\n",
        "def get_position_qty(trading_api: TradingClient, symbol: str):\n",
        "    use_fractionals = bool(globals().get(\"USE_FRACTIONALS\", True))\n",
        "    try:\n",
        "        pos = trading_api.get_position(symbol)\n",
        "    except Exception:\n",
        "        pos = None\n",
        "    if not pos:\n",
        "        return 0.0 if use_fractionals else 0\n",
        "    try:\n",
        "        q = float(pos.qty)\n",
        "        if use_fractionals:\n",
        "            return q\n",
        "        return _truncate_toward_zero(q)\n",
        "    except Exception:\n",
        "        return 0.0 if use_fractionals else 0\n",
        "\n",
        "def get_last_price(trading_api: TradingClient, data_api: StockHistoricalDataClient, symbol: str) -> float:\n",
        "    sym = str(symbol).upper()\n",
        "\n",
        "    def _latest_map_get(resp, symbol: str, attr_name: str):\n",
        "        sym2 = str(symbol).upper()\n",
        "        try:\n",
        "            if isinstance(resp, dict):\n",
        "                return resp.get(sym2) or resp.get(symbol)\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            m = getattr(resp, attr_name, None)\n",
        "            if isinstance(m, dict):\n",
        "                return m.get(sym2) or m.get(symbol)\n",
        "        except Exception:\n",
        "            pass\n",
        "        for alt in (\"data\", \"raw\", \"result\"):\n",
        "            try:\n",
        "                m = getattr(resp, alt, None)\n",
        "                if isinstance(m, dict):\n",
        "                    obj = m.get(sym2) or m.get(symbol)\n",
        "                    if obj is not None:\n",
        "                        return obj\n",
        "            except Exception:\n",
        "                continue\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        resp = data_api.get_stock_latest_trade(StockLatestTradeRequest(symbol_or_symbols=sym))\n",
        "        tr = _latest_map_get(resp, sym, \"trades\")\n",
        "        if tr is not None:\n",
        "            price = getattr(tr, \"price\", None)\n",
        "            if price is None:\n",
        "                price = getattr(tr, \"p\", None)\n",
        "            if price is not None and np.isfinite(float(price)):\n",
        "                return float(price)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        resp = data_api.get_stock_latest_quote(StockLatestQuoteRequest(symbol_or_symbols=sym))\n",
        "        qt = _latest_map_get(resp, sym, \"quotes\")\n",
        "        if qt is not None:\n",
        "            ap = getattr(qt, \"ask_price\", getattr(qt, \"ap\", None))\n",
        "            bp = getattr(qt, \"bid_price\", getattr(qt, \"bp\", None))\n",
        "            if ap is not None and bp is not None and np.isfinite(float(ap)) and np.isfinite(float(bp)):\n",
        "                return float((float(ap) + float(bp)) / 2.0)\n",
        "            if ap is not None and np.isfinite(float(ap)):\n",
        "                return float(ap)\n",
        "            if bp is not None and np.isfinite(float(bp)):\n",
        "                return float(bp)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        feed = os.getenv(\"BARS_FEED\", \"\").strip() or None\n",
        "        req = StockBarsRequest(symbol_or_symbols=sym, timeframe=globals().get(\"LIVE_TIMEFRAME\", TimeFrame.Hour), limit=1, feed=feed)\n",
        "        resp = data_api.get_stock_bars(req)\n",
        "        if hasattr(resp, \"df\") and resp.df is not None and not resp.df.empty:\n",
        "            df = resp.df.copy()\n",
        "            if isinstance(df.index, pd.MultiIndex):\n",
        "                try:\n",
        "                    df = df.xs(sym, level=0)\n",
        "                except Exception:\n",
        "                    df = df.reset_index(level=0, drop=True)\n",
        "            if \"close\" in df.columns:\n",
        "                v = df[\"close\"].iloc[-1]\n",
        "                if v is not None and np.isfinite(float(v)):\n",
        "                    return float(v)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        pos = trading_api.get_position(sym)\n",
        "        v = getattr(pos, \"avg_entry_price\", None)\n",
        "        return float(v) if v is not None else float(\"nan\")\n",
        "    except Exception:\n",
        "        return float(\"nan\")\n",
        "\n",
        "def print_position_summary(trading_api: TradingClient) -> float:\n",
        "    try:\n",
        "        positions = trading_api.get_all_positions()\n",
        "        total_market_value = 0.0\n",
        "        print(\"\\nPosition Summary:\")\n",
        "        if not positions:\n",
        "            print(\"  (no open positions)\")\n",
        "            print(\"\\nTotal Market Value: $0.00\")\n",
        "            return 0.0\n",
        "        for p in positions:\n",
        "            sym = str(getattr(p, \"symbol\", \"\")).upper()\n",
        "            qty = float(getattr(p, \"qty\", 0.0) or 0.0)\n",
        "            px  = float(getattr(p, \"current_price\", float(\"nan\")))\n",
        "            mv  = float(getattr(p, \"market_value\", 0.0) or 0.0)  # signed\n",
        "            total_market_value += mv\n",
        "            px_str = f\"${px:.2f}\" if np.isfinite(px) else \"n/a\"\n",
        "            print(f\"  {sym}: {qty} shares @ {px_str} | Value: ${mv:,.2f}\")\n",
        "        print(f\"\\nTotal Market Value: ${total_market_value:,.2f}\")\n",
        "        return total_market_value\n",
        "    except Exception as e:\n",
        "        print(f\"Could not summarize positions: {e}\")\n",
        "        return float(\"nan\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# (A) STRICT START_FLAT: cancel orders + wait for flat (positions + orders)\n",
        "# ============================================================\n",
        "def list_open_orders(trading_api: TradingClient, symbols: Optional[List[str]] = None):\n",
        "    try:\n",
        "        req = GetOrdersRequest(status=OrderStatus.OPEN, limit=500)\n",
        "        orders = trading_api.get_orders(req) or []\n",
        "    except Exception:\n",
        "        orders = []\n",
        "    if symbols:\n",
        "        want = {s.upper() for s in symbols}\n",
        "        out = []\n",
        "        for o in orders:\n",
        "            try:\n",
        "                if str(getattr(o, \"symbol\", \"\")).upper() in want:\n",
        "                    out.append(o)\n",
        "            except Exception:\n",
        "                continue\n",
        "        return out\n",
        "    return orders\n",
        "\n",
        "def cancel_open_orders_for_symbols(trading_api: TradingClient, symbols: List[str]) -> int:\n",
        "    symbols_u = {s.upper() for s in symbols}\n",
        "    orders = list_open_orders(trading_api, symbols=list(symbols_u))\n",
        "    canceled = 0\n",
        "    for o in orders:\n",
        "        oid = getattr(o, \"id\", None)\n",
        "        sym = str(getattr(o, \"symbol\", \"\")).upper()\n",
        "        if sym not in symbols_u or not oid:\n",
        "            continue\n",
        "        try:\n",
        "            trading_api.cancel_order_by_id(oid)\n",
        "            canceled += 1\n",
        "            logging.info(\"[%s] Canceled open order id=%s\", sym, oid)\n",
        "        except Exception as e:\n",
        "            logging.warning(\"[%s] cancel_order_by_id failed (%s): %s\", sym, oid, e)\n",
        "    return canceled\n",
        "\n",
        "def wait_until_flat_positions_and_orders(\n",
        "    trading_api: TradingClient,\n",
        "    symbols: List[str],\n",
        "    dust_qty: float = 0.000001,\n",
        "    mv_dust: float = 50.0,\n",
        "    timeout_sec: int = 90,\n",
        "    poll_sec: int = 2,\n",
        ") -> bool:\n",
        "    t0 = time.time()\n",
        "    symbols_u = [s.upper() for s in symbols]\n",
        "    while time.time() - t0 < timeout_sec:\n",
        "        flat_positions = True\n",
        "        try:\n",
        "            pos = trading_api.get_all_positions() or []\n",
        "            mv = {str(p.symbol).upper(): float(getattr(p, \"market_value\", 0.0) or 0.0) for p in pos}\n",
        "            qty = {str(p.symbol).upper(): float(getattr(p, \"qty\", 0.0) or 0.0) for p in pos}\n",
        "            still = []\n",
        "            for s in symbols_u:\n",
        "                if abs(qty.get(s, 0.0)) > dust_qty and abs(mv.get(s, 0.0)) > mv_dust:\n",
        "                    still.append(s)\n",
        "            if still:\n",
        "                flat_positions = False\n",
        "        except Exception:\n",
        "            flat_positions = False\n",
        "\n",
        "        open_orders = list_open_orders(trading_api, symbols=symbols_u)\n",
        "        flat_orders = (len(open_orders) == 0)\n",
        "\n",
        "        if flat_positions and flat_orders:\n",
        "            return True\n",
        "\n",
        "        time.sleep(max(0.25, float(poll_sec)))\n",
        "    return False\n",
        "\n",
        "def flatten_symbols_strict(\n",
        "    trading_api: TradingClient,\n",
        "    symbols: List[str],\n",
        "    dust_qty: float = 0.000001,\n",
        "    timeout_sec: int = 120,\n",
        "):\n",
        "    \"\"\"\n",
        "    Strict flatten:\n",
        "    1) cancel open orders for symbols\n",
        "    2) close positions for symbols (market close)\n",
        "    3) cancel again (to catch newly created orders)\n",
        "    4) wait until both positions AND orders are flat\n",
        "    \"\"\"\n",
        "    symbols_u = [s.upper() for s in symbols]\n",
        "\n",
        "    logging.warning(\"[START_FLAT] strict flatten begin for symbols=%s\", symbols_u)\n",
        "    try:\n",
        "        cancel_open_orders_for_symbols(trading_api, symbols_u)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    for s in symbols_u:\n",
        "        try:\n",
        "            trading_api.close_position(s)\n",
        "            logging.info(\"[%s] close_position submitted.\", s)\n",
        "        except Exception as e:\n",
        "            logging.info(\"[%s] close_position skipped/failed: %s\", s, e)\n",
        "\n",
        "    time.sleep(1.5)\n",
        "\n",
        "    try:\n",
        "        cancel_open_orders_for_symbols(trading_api, symbols_u)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    ok = wait_until_flat_positions_and_orders(\n",
        "        trading_api,\n",
        "        symbols_u,\n",
        "        dust_qty=dust_qty,\n",
        "        timeout_sec=timeout_sec,\n",
        "        poll_sec=2,\n",
        "    )\n",
        "\n",
        "    if ok:\n",
        "        logging.warning(\"[START_FLAT] strict flatten complete: FLAT ✅\")\n",
        "    else:\n",
        "        logging.warning(\"[START_FLAT] strict flatten timed out: NOT FULLY FLAT ⚠️ (check orders/positions)\")\n",
        "    return ok\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Rebalance logic (SIGNED MV)\n",
        "# ============================================================\n",
        "def compute_target_qty_by_cash(equity: float, price: float, target_weight: float) -> int:\n",
        "    if not np.isfinite(equity) or equity <= 0:\n",
        "        return 0\n",
        "    if not np.isfinite(price) or price <= 0:\n",
        "        return 0\n",
        "\n",
        "    w = float(target_weight)\n",
        "    cap = float(globals().get(\"WEIGHT_CAP\", 1.0))\n",
        "    if cap > 0:\n",
        "        w = max(-cap, min(cap, w))\n",
        "    if not bool(globals().get(\"ALLOW_SHORTS\", False)):\n",
        "        w = max(0.0, w)\n",
        "\n",
        "    target_notional = equity * w\n",
        "    qty_f = target_notional / price\n",
        "    return _truncate_toward_zero(qty_f)\n",
        "\n",
        "def rebalance_to_weight(\n",
        "    trading_api: TradingClient,\n",
        "    data_api: StockHistoricalDataClient,\n",
        "    symbol: str,\n",
        "    equity: float,\n",
        "    target_weight: float\n",
        ") -> dict:\n",
        "    price = get_last_price(trading_api, data_api, symbol)\n",
        "    if not np.isfinite(price) or price <= 0:\n",
        "        logging.warning(f\"[{symbol}] Price unavailable; skipping rebalance.\")\n",
        "        return dict(NO_ORDER)\n",
        "\n",
        "    tradable, fractionable, shortable = asset_flags(trading_api, symbol)\n",
        "    if not tradable:\n",
        "        logging.info(f\"[{symbol}] Not tradable; skipping.\")\n",
        "        return dict(NO_ORDER)\n",
        "\n",
        "    use_fractionals = bool(USE_FRACTIONALS and fractionable)\n",
        "\n",
        "    pos = get_position(trading_api, symbol)\n",
        "    have_qty = float(getattr(pos, \"qty\", 0.0) or 0.0) if pos else 0.0\n",
        "    have_mv = None\n",
        "    if pos is not None:\n",
        "        try:\n",
        "            have_mv = float(getattr(pos, \"market_value\", None))\n",
        "        except Exception:\n",
        "            have_mv = None\n",
        "\n",
        "    if have_mv is not None and np.isfinite(have_mv):\n",
        "        have_notional = float(have_mv)  # signed\n",
        "    else:\n",
        "        have_notional = float(have_qty) * float(price)\n",
        "\n",
        "    target_notional = float(equity) * float(target_weight)\n",
        "    delta_notional = target_notional - have_notional\n",
        "\n",
        "    # If flipping long->short, flatten long first (reduces rejection risk)\n",
        "    if (have_qty > 0) and (target_notional < 0):\n",
        "        logging.info(f\"[{symbol}] Flip long→short requested. Flattening long first (have_qty={have_qty}).\")\n",
        "        try:\n",
        "            flatten_symbols_strict(trading_api, [symbol], timeout_sec=90)\n",
        "        except Exception:\n",
        "            pass\n",
        "        return dict(NO_ORDER)\n",
        "\n",
        "    if abs(delta_notional) < 1e-9:\n",
        "        return dict(NO_ORDER)\n",
        "\n",
        "    delta_weight = abs(delta_notional) / max(float(equity), 1e-9)\n",
        "    if delta_weight < float(globals().get(\"DELTA_WEIGHT_MIN\", 0.0)):\n",
        "        return dict(NO_ORDER)\n",
        "\n",
        "    if use_fractionals:\n",
        "        dn = round_to_cents(abs(delta_notional))\n",
        "        if dn < float(globals().get(\"REBALANCE_MIN_NOTIONAL\", 0.0)):\n",
        "            return dict(NO_ORDER)\n",
        "\n",
        "        side = \"buy\" if delta_notional > 0 else \"sell\"\n",
        "        shorting = (target_notional < 0) and (side == \"sell\")\n",
        "        covering = (have_qty < 0) and (side == \"buy\")\n",
        "\n",
        "        if shorting:\n",
        "            if not shortable:\n",
        "                logging.info(f\"[{symbol}] Not shortable; skipping short rebalance.\")\n",
        "                return dict(NO_ORDER)\n",
        "            qty = max(1, int(math.floor(dn / price))) if price > 0 else 1\n",
        "            o = market_order_to_qty(trading_api, symbol, side=\"sell\", qty=qty)\n",
        "            return _order_info(o)\n",
        "\n",
        "        if covering:\n",
        "            qty = max(1, int(math.ceil(dn / price))) if price > 0 else 1\n",
        "            qty = min(int(abs(have_qty)), qty) if have_qty < 0 else qty\n",
        "            o = market_order_to_qty(trading_api, symbol, side=\"buy\", qty=qty)\n",
        "            return _order_info(o)\n",
        "\n",
        "        o = submit_fractional_rebalance(trading_api, symbol, delta_notional=delta_notional, price=price)\n",
        "        return _order_info(o)\n",
        "\n",
        "    want_qty = compute_target_qty_by_cash(equity, price, target_weight)\n",
        "    have_qty_int = _truncate_toward_zero(have_qty)\n",
        "    delta_qty = want_qty - have_qty_int\n",
        "    if delta_qty == 0:\n",
        "        return dict(NO_ORDER)\n",
        "\n",
        "    approx_delta_notional = abs(delta_qty) * price\n",
        "    if equity > 0 and approx_delta_notional / equity < float(globals().get(\"DELTA_WEIGHT_MIN\", 0.0)):\n",
        "        return dict(NO_ORDER)\n",
        "    if approx_delta_notional < float(globals().get(\"REBALANCE_MIN_NOTIONAL\", 0.0)):\n",
        "        return dict(NO_ORDER)\n",
        "\n",
        "    side = \"buy\" if delta_qty > 0 else \"sell\"\n",
        "    shorting = (target_notional < 0) and (side == \"sell\")\n",
        "    if shorting and not shortable:\n",
        "        logging.info(f\"[{symbol}] Not shortable; skipping short rebalance.\")\n",
        "        return dict(NO_ORDER)\n",
        "\n",
        "    o = market_order_to_qty(trading_api, symbol, side=side, qty=int(abs(delta_qty)))\n",
        "    return _order_info(o)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# (B) Hard cap enforcement (prevents runaway weights)\n",
        "# ============================================================\n",
        "def enforce_position_caps_if_violated(\n",
        "    trading_api: TradingClient,\n",
        "    data_api: StockHistoricalDataClient,\n",
        "    symbol: str,\n",
        "    equity: float,\n",
        "    cap: float,\n",
        ") -> dict:\n",
        "    try:\n",
        "        cap = float(cap)\n",
        "        if cap <= 0:\n",
        "            return dict(NO_ORDER)\n",
        "        if not np.isfinite(equity) or equity <= 0:\n",
        "            return dict(NO_ORDER)\n",
        "\n",
        "        pos = get_position(trading_api, symbol)\n",
        "        if not pos:\n",
        "            return dict(NO_ORDER)\n",
        "\n",
        "        mv = float(getattr(pos, \"market_value\", 0.0) or 0.0)  # signed\n",
        "        if not np.isfinite(mv) or abs(mv) < 1e-9:\n",
        "            return dict(NO_ORDER)\n",
        "\n",
        "        w = mv / equity\n",
        "        if abs(w) <= cap + 1e-6:\n",
        "            return dict(NO_ORDER)\n",
        "\n",
        "        target_w = float(np.sign(w) * cap)\n",
        "        logging.warning(\"[%s] HARD_CAP breach: actual_weight=%.3f exceeds cap=%.3f -> rebalance to %.3f\",\n",
        "                        symbol, w, cap, target_w)\n",
        "        return rebalance_to_weight(trading_api, data_api, symbol, equity, target_w)\n",
        "    except Exception as e:\n",
        "        logging.warning(\"[%s] enforce_position_caps_if_violated failed: %s\", symbol, e)\n",
        "        return dict(NO_ORDER)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# (D) Exposure caps: compute + enforcement\n",
        "# ============================================================\n",
        "def compute_gross_net_exposure(trading_api: TradingClient, symbols: Optional[List[str]] = None) -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    Returns (gross_exposure_pct, net_exposure_pct, equity).\n",
        "    gross = sum(abs(market_value)) / equity\n",
        "    net   = sum(market_value) / equity\n",
        "    If symbols provided, only includes those symbols.\n",
        "    \"\"\"\n",
        "    acct = trading_api.get_account()\n",
        "    equity = float(getattr(acct, \"equity\", float(\"nan\")))\n",
        "    if not np.isfinite(equity) or equity <= 0:\n",
        "        return float(\"nan\"), float(\"nan\"), equity\n",
        "\n",
        "    want = {s.upper() for s in symbols} if symbols else None\n",
        "    pos = trading_api.get_all_positions() or []\n",
        "    mvs = []\n",
        "    for p in pos:\n",
        "        try:\n",
        "            sym = str(getattr(p, \"symbol\", \"\")).upper()\n",
        "            if want is not None and sym not in want:\n",
        "                continue\n",
        "            mv = float(getattr(p, \"market_value\", 0.0) or 0.0)  # signed\n",
        "            if np.isfinite(mv):\n",
        "                mvs.append(mv)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    gross = float(np.nansum([abs(x) for x in mvs])) / equity if mvs else 0.0\n",
        "    net = float(np.nansum(mvs)) / equity if mvs else 0.0\n",
        "    return gross, net, equity\n",
        "\n",
        "def exposure_caps_update_and_flags(trading_api: TradingClient, symbols: List[str]) -> Tuple[bool, float, float]:\n",
        "    \"\"\"\n",
        "    Updates global exposure state. Returns (block_risk, gross, net).\n",
        "    block_risk=True => do NOT allow trades that increase gross or increase abs(net).\n",
        "    \"\"\"\n",
        "    global _EXPOSURE_CAPS_BLOCK_RISK, _EXPOSURE_LAST\n",
        "\n",
        "    gross, net, eq = compute_gross_net_exposure(trading_api, symbols=symbols)\n",
        "    _EXPOSURE_LAST[\"gross\"] = gross\n",
        "    _EXPOSURE_LAST[\"net\"] = net\n",
        "\n",
        "    gcap = float(globals().get(\"GROSS_CAP\", 1.0))\n",
        "    ncap = float(globals().get(\"NET_CAP\", 1.0))\n",
        "\n",
        "    breach = False\n",
        "    if np.isfinite(gross) and gcap > 0 and gross > gcap + 1e-9:\n",
        "        breach = True\n",
        "    if np.isfinite(net) and ncap > 0 and abs(net) > ncap + 1e-9:\n",
        "        breach = True\n",
        "\n",
        "    _EXPOSURE_CAPS_BLOCK_RISK = bool(breach)\n",
        "\n",
        "    if breach:\n",
        "        logging.warning(\"[EXPOSURE_CAP] breach: gross=%.3f (cap=%.3f) | net=%.3f (cap=%.3f) => BLOCK RISK-INCREASING TRADES\",\n",
        "                        gross, gcap, net, ncap)\n",
        "    else:\n",
        "        logging.info(\"[EXPOSURE_CAP] ok: gross=%.3f (cap=%.3f) | net=%.3f (cap=%.3f)\",\n",
        "                     gross, gcap, net, ncap)\n",
        "\n",
        "    return _EXPOSURE_CAPS_BLOCK_RISK, gross, net\n",
        "\n",
        "def trade_increases_risk(current_w: float, target_w: float) -> bool:\n",
        "    \"\"\"\n",
        "    Conservative rule:\n",
        "      - if |target| > |current| => gross likely increases\n",
        "      - if abs(target) == abs(current) but moves away from 0 in signed sense, also increases abs(net) for that symbol\n",
        "    \"\"\"\n",
        "    try:\n",
        "        cw = float(current_w)\n",
        "        tw = float(target_w)\n",
        "    except Exception:\n",
        "        return True\n",
        "    return abs(tw) > abs(cw) + 1e-9\n",
        "\n",
        "def maybe_emergency_flatten_on_exposure(trading_api: TradingClient, symbols: List[str]) -> bool:\n",
        "    \"\"\"\n",
        "    Optional \"big red button\":\n",
        "      if EMERGENCY_FLATTEN_ON_EXPOSURE=1 and caps breached -> strict flatten.\n",
        "    \"\"\"\n",
        "    if not bool(globals().get(\"EMERGENCY_FLATTEN_ON_EXPOSURE\", False)):\n",
        "        return False\n",
        "    if not bool(_EXPOSURE_CAPS_BLOCK_RISK):\n",
        "        return False\n",
        "    logging.warning(\"[EXPOSURE_CAP] EMERGENCY_FLATTEN_ON_EXPOSURE=1 -> flattening all configured symbols now.\")\n",
        "    try:\n",
        "        flatten_symbols_strict(trading_api, symbols, timeout_sec=180)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logging.warning(\"[EXPOSURE_CAP] emergency flatten failed: %s\", e)\n",
        "        return False\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Risk: TP/SL\n",
        "# ============================================================\n",
        "def check_tp_sl_and_maybe_flatten(trading_api: TradingClient, symbol: str) -> bool:\n",
        "    if TAKE_PROFIT_PCT <= 0 and STOP_LOSS_PCT <= 0:\n",
        "        return False\n",
        "    pos = get_position(trading_api, symbol)\n",
        "    if not pos:\n",
        "        return False\n",
        "    try:\n",
        "        plpc = float(pos.unrealized_plpc)\n",
        "    except Exception:\n",
        "        return False\n",
        "    if TAKE_PROFIT_PCT > 0 and plpc >= TAKE_PROFIT_PCT:\n",
        "        logging.info(f\"[{symbol}] TP hit ({plpc:.4f} >= {TAKE_PROFIT_PCT:.4f}). Flattening.\")\n",
        "        flatten_symbols_strict(trading_api, [symbol], timeout_sec=90)\n",
        "        return True\n",
        "    if STOP_LOSS_PCT > 0 and plpc <= -abs(STOP_LOSS_PCT):\n",
        "        logging.info(f\"[{symbol}] SL hit ({plpc:.4f} <= {-abs(STOP_LOSS_PCT):.4f}). Flattening.\")\n",
        "        flatten_symbols_strict(trading_api, [symbol], timeout_sec=90)\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Bars + features + obs\n",
        "# ============================================================\n",
        "def get_recent_bars(data_api: StockHistoricalDataClient, symbol: str, limit: int = 200, timeframe=None) -> pd.DataFrame:\n",
        "    timeframe = timeframe or globals().get(\"LIVE_TIMEFRAME\", TimeFrame.Hour)\n",
        "    feed = os.getenv(\"BARS_FEED\", \"\").strip() or None\n",
        "\n",
        "    def _normalize_df(resp) -> pd.DataFrame:\n",
        "        if not hasattr(resp, \"df\"):\n",
        "            return pd.DataFrame(columns=[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"])\n",
        "        df = resp.df.copy()\n",
        "        if df.empty:\n",
        "            return pd.DataFrame(columns=[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"])\n",
        "        if isinstance(df.index, pd.MultiIndex):\n",
        "            try:\n",
        "                df = df.xs(symbol, level=0)\n",
        "            except Exception:\n",
        "                df = df.reset_index(level=0, drop=True)\n",
        "        df.index = pd.to_datetime(df.index, utc=True, errors=\"coerce\")\n",
        "        df = df.rename(columns={\"open\": \"Open\", \"high\": \"High\", \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"})\n",
        "        cols = [c for c in [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"] if c in df.columns]\n",
        "        out = df[cols].sort_index()\n",
        "        return out.dropna(how=\"all\")\n",
        "\n",
        "    try:\n",
        "        req = StockBarsRequest(symbol_or_symbols=symbol, timeframe=timeframe, limit=int(limit), feed=feed)\n",
        "        resp = data_api.get_stock_bars(req)\n",
        "        df = _normalize_df(resp)\n",
        "        if not df.empty:\n",
        "            return df\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[{symbol}] get_stock_bars(limit={limit}) failed: {e}\")\n",
        "\n",
        "    try:\n",
        "        end_dt = datetime.now(timezone.utc).replace(microsecond=0)\n",
        "        start_dt = end_dt - timedelta(days=5)\n",
        "        req = StockBarsRequest(symbol_or_symbols=symbol, timeframe=timeframe, start=start_dt, end=end_dt, feed=feed)\n",
        "        resp = data_api.get_stock_bars(req)\n",
        "        df = _normalize_df(resp)\n",
        "        if not df.empty:\n",
        "            return df\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[{symbol}] get_stock_bars(start/end) failed: {e}\")\n",
        "\n",
        "    return pd.DataFrame(columns=[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"])\n",
        "\n",
        "def add_regime(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df[\"Vol20\"] = df[\"Close\"].pct_change().rolling(20).std()\n",
        "    df[\"Ret20\"] = df[\"Close\"].pct_change(20)\n",
        "    vol_hi = (df[\"Vol20\"] > df[\"Vol20\"].median()).astype(int)\n",
        "    trend_hi = (df[\"Ret20\"].abs() > df[\"Ret20\"].abs().median()).astype(int)\n",
        "    df[\"Regime4\"] = vol_hi * 2 + trend_hi\n",
        "    return df\n",
        "\n",
        "def denoise_wavelet(series: pd.Series, wavelet: str = \"db1\", level: int = 2) -> pd.Series:\n",
        "    try:\n",
        "        import pywt\n",
        "    except Exception:\n",
        "        return pd.Series(series).astype(float).ffill().bfill().ewm(span=5, adjust=False).mean()\n",
        "\n",
        "    s = pd.Series(series).astype(float).ffill().bfill()\n",
        "    arr = s.to_numpy()\n",
        "    try:\n",
        "        w = pywt.Wavelet(wavelet)\n",
        "        maxlvl = pywt.dwt_max_level(len(arr), w.dec_len)\n",
        "        lvl = int(max(0, min(level, maxlvl)))\n",
        "        if lvl < 1:\n",
        "            return s\n",
        "        coeffs = pywt.wavedec(arr, w, mode=\"symmetric\", level=lvl)\n",
        "        for i in range(1, len(coeffs)):\n",
        "            coeffs[i] = np.zeros_like(coeffs[i])\n",
        "        rec = pywt.waverec(coeffs, w, mode=\"symmetric\")\n",
        "        return pd.Series(rec[:len(arr)], index=s.index)\n",
        "    except Exception:\n",
        "        return s.ewm(span=5, adjust=False).mean()\n",
        "\n",
        "def add_features_live(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy().sort_index()\n",
        "    cols_ci = {c.lower(): c for c in df.columns}\n",
        "    rename = {}\n",
        "    for final, alts in {\n",
        "        \"Open\": [\"open\"],\n",
        "        \"High\": [\"high\"],\n",
        "        \"Low\": [\"low\"],\n",
        "        \"Close\": [\"close\", \"close*\", \"last\"],\n",
        "        \"Adj Close\": [\"adj close\", \"adj_close\", \"adjclose\", \"adjusted close\"],\n",
        "        \"Volume\": [\"volume\", \"vol\"],\n",
        "    }.items():\n",
        "        for a in [final.lower()] + alts:\n",
        "            if a in cols_ci:\n",
        "                rename[cols_ci[a]] = final\n",
        "                break\n",
        "    df = df.rename(columns=rename)\n",
        "    if \"Adj Close\" not in df.columns and \"Close\" in df.columns:\n",
        "        df[\"Adj Close\"] = df[\"Close\"]\n",
        "\n",
        "    df[\"SMA_20\"] = df[\"Close\"].rolling(20).mean()\n",
        "    df[\"STD_20\"] = df[\"Close\"].rolling(20).std()\n",
        "    df[\"Upper_Band\"] = df[\"SMA_20\"] + 2 * df[\"STD_20\"]\n",
        "    df[\"Lower_Band\"] = df[\"SMA_20\"] - 2 * df[\"STD_20\"]\n",
        "\n",
        "    df[\"Lowest_Low\"] = df[\"Low\"].rolling(14).min()\n",
        "    df[\"Highest_High\"] = df[\"High\"].rolling(14).max()\n",
        "    denom = (df[\"Highest_High\"] - df[\"Lowest_Low\"]).replace(0, np.nan)\n",
        "    df[\"Stoch\"] = ((df[\"Close\"] - df[\"Lowest_Low\"]) / denom) * 100\n",
        "\n",
        "    df[\"ROC\"] = df[\"Close\"].pct_change(10)\n",
        "    sign = np.sign(df[\"Close\"].diff().fillna(0))\n",
        "    df[\"OBV\"] = (sign * df[\"Volume\"].fillna(0)).cumsum()\n",
        "\n",
        "    tp = (df[\"High\"] + df[\"Low\"] + df[\"Close\"]) / 3.0\n",
        "    sma_tp = tp.rolling(20).mean()\n",
        "    md = (tp - sma_tp).abs().rolling(20).mean().replace(0, np.nan)\n",
        "    df[\"CCI\"] = (tp - sma_tp) / (0.015 * md)\n",
        "\n",
        "    df[\"EMA_10\"] = df[\"Close\"].ewm(span=10, adjust=False).mean()\n",
        "    df[\"EMA_50\"] = df[\"Close\"].ewm(span=50, adjust=False).mean()\n",
        "    ema12 = df[\"Close\"].ewm(span=12, adjust=False).mean()\n",
        "    ema26 = df[\"Close\"].ewm(span=26, adjust=False).mean()\n",
        "    df[\"MACD_Line\"] = ema12 - ema26\n",
        "    df[\"MACD_Signal\"] = df[\"MACD_Line\"].ewm(span=9, adjust=False).mean()\n",
        "\n",
        "    d = df[\"Close\"].diff()\n",
        "    gain = d.clip(lower=0)\n",
        "    loss = (-d.clip(upper=0))\n",
        "    avg_gain = gain.ewm(alpha=1 / 14, adjust=False).mean()\n",
        "    avg_loss = loss.ewm(alpha=1 / 14, adjust=False).mean()\n",
        "    rs = avg_gain / avg_loss.replace(0, np.nan)\n",
        "    df[\"RSI\"] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    tr = pd.concat([\n",
        "        (df[\"High\"] - df[\"Low\"]),\n",
        "        (df[\"High\"] - df[\"Close\"].shift()).abs(),\n",
        "        (df[\"Low\"] - df[\"Close\"].shift()).abs(),\n",
        "    ], axis=1).max(axis=1)\n",
        "    df[\"ATR\"] = tr.ewm(alpha=1 / 14, adjust=False).mean()\n",
        "\n",
        "    df[\"Volatility\"] = df[\"Close\"].pct_change().rolling(20).std()\n",
        "    df[\"Denoised_Close\"] = denoise_wavelet(df[\"Close\"])\n",
        "\n",
        "    df = add_regime(df)\n",
        "    df[\"SentimentScore\"] = 0.0\n",
        "    df[\"Delta\"] = df[\"Close\"].pct_change(1).fillna(0.0)\n",
        "    df[\"Gamma\"] = df[\"Delta\"].diff().fillna(0.0)\n",
        "\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    return df\n",
        "\n",
        "FEATURE_ALIASES = {\"SMA_50\": \"Rolling_Mean_50\", \"Rolling_Mean_50\": \"SMA_50\"}\n",
        "\n",
        "def resolve_feature_alias(name: str, df: pd.DataFrame) -> Optional[str]:\n",
        "    if name in df.columns:\n",
        "        return name\n",
        "    alt = FEATURE_ALIASES.get(name)\n",
        "    if alt and alt in df.columns:\n",
        "        return alt\n",
        "    return None\n",
        "\n",
        "def compute_art_feat_order(features_hint: Any, df: pd.DataFrame) -> List[str]:\n",
        "    if features_hint is None:\n",
        "        return [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
        "    feats = features_hint.get(\"features\", features_hint) if isinstance(features_hint, dict) else list(features_hint)\n",
        "    drop = {\"datetime\", \"symbol\", \"target\", \"return\"}\n",
        "    resolved = []\n",
        "    for f in feats:\n",
        "        if f in drop:\n",
        "            continue\n",
        "        col = resolve_feature_alias(f, df)\n",
        "        if col and pd.api.types.is_numeric_dtype(df[col]):\n",
        "            resolved.append(col)\n",
        "    return resolved\n",
        "\n",
        "def expected_obs_shape(model, vecnorm) -> Optional[tuple]:\n",
        "    for src in (model, vecnorm):\n",
        "        try:\n",
        "            shp = tuple(getattr(src, \"observation_space\", None).shape)\n",
        "            if shp:\n",
        "                return shp\n",
        "        except Exception:\n",
        "            pass\n",
        "    return None\n",
        "\n",
        "def _pick_columns_for_channels(features_hint: Any, df: pd.DataFrame, channels: int) -> List[str]:\n",
        "    ordered = compute_art_feat_order(features_hint, df)\n",
        "    if len(ordered) >= channels:\n",
        "        return ordered[:channels]\n",
        "    numeric = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
        "    pref = [\"Close\", \"Volume\", \"Adj Close\", \"Open\", \"High\", \"Low\"]\n",
        "    cols = [c for c in pref if c in numeric]\n",
        "    cols += [c for c in numeric if c not in cols]\n",
        "    cols = cols[:channels]\n",
        "    if cols:\n",
        "        while len(cols) < channels:\n",
        "            cols.append(cols[-1])\n",
        "    return cols[:channels]\n",
        "\n",
        "def prepare_observation_from_bars(\n",
        "    bars_df: pd.DataFrame,\n",
        "    features_hint: Any = None,\n",
        "    min_required_rows: int = 60,\n",
        "    expected_shape: Optional[tuple] = None,\n",
        "    symbol: str = \"\",\n",
        ") -> Tuple[np.ndarray, int]:\n",
        "    feats_df = add_features_live(bars_df).replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    ts = ensure_utc(pd.Timestamp.utcnow())\n",
        "    if not feats_df.empty:\n",
        "        try:\n",
        "            ts = ensure_utc(feats_df.index[-1])\n",
        "        except Exception:\n",
        "            pass\n",
        "    obs_ts = int(ts.timestamp())\n",
        "\n",
        "    if expected_shape is not None and len(expected_shape) == 2:\n",
        "        lookback, channels = int(expected_shape[0]), int(expected_shape[1])\n",
        "        cols = _pick_columns_for_channels(features_hint, feats_df, channels)\n",
        "        window_df = feats_df[cols].tail(lookback).fillna(0.0)\n",
        "        arr = window_df.to_numpy(dtype=np.float32)\n",
        "        if arr.shape[0] < lookback:\n",
        "            pad_rows = lookback - arr.shape[0]\n",
        "            arr = np.vstack([np.zeros((pad_rows, channels), dtype=np.float32), arr])\n",
        "        arr = arr[-lookback:, :channels]\n",
        "        return arr.reshape(lookback, channels), obs_ts\n",
        "\n",
        "    order = compute_art_feat_order(features_hint, feats_df)\n",
        "    feats_df = feats_df.dropna(subset=order) if order else feats_df\n",
        "    if len(feats_df) < max(20, min_required_rows):\n",
        "        raise ValueError(f\"Not enough bars to compute features robustly (have {len(feats_df)}).\")\n",
        "    last = feats_df.iloc[-1]\n",
        "    vals = []\n",
        "    for c in order:\n",
        "        v = last.get(c, np.nan)\n",
        "        vals.append(0.0 if (pd.isna(v) or v is None) else float(v))\n",
        "    return np.asarray(vals, dtype=np.float32), obs_ts\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Inference\n",
        "# ============================================================\n",
        "def action_to_weight(action) -> Tuple[float, float, float]:\n",
        "    a = float(np.asarray(action).reshape(-1)[0])\n",
        "    raw = a\n",
        "    cap = float(globals().get(\"WEIGHT_CAP\", 0.35))\n",
        "    target_w = float(np.clip(a, -1, 1)) * cap\n",
        "    conf = float(min(1.0, abs(a)))\n",
        "\n",
        "    if not bool(globals().get(\"ALLOW_SHORTS\", False)):\n",
        "        target_w = max(0.0, target_w)\n",
        "\n",
        "    if str(globals().get(\"SIZING_MODE\", \"linear\")).lower() == \"threshold\":\n",
        "        floor = float(globals().get(\"CONF_FLOOR\", 0.15))\n",
        "        if conf < floor:\n",
        "            target_w = 0.0\n",
        "        else:\n",
        "            scale = (conf - floor) / max(1e-9, (1.0 - floor))\n",
        "            target_w = np.sign(target_w) * cap * float(np.clip(scale, 0, 1))\n",
        "\n",
        "    return float(target_w), float(conf), float(raw)\n",
        "\n",
        "def infer_target_weight(model: PPO, vecnorm: Optional[VecNormalize], obs: np.ndarray) -> Tuple[float, float, float]:\n",
        "    x = np.asarray(obs, dtype=np.float32)\n",
        "\n",
        "    if vecnorm is not None and hasattr(vecnorm, \"normalize_obs\") and getattr(vecnorm, \"obs_rms\", None) is not None:\n",
        "        try:\n",
        "            x = vecnorm.normalize_obs(x)\n",
        "        except Exception:\n",
        "            try:\n",
        "                x = vecnorm.normalize_obs(np.expand_dims(x, axis=0))[0]\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    try:\n",
        "        action, _ = model.predict(x, deterministic=INF_DETERMINISTIC)\n",
        "    except Exception:\n",
        "        action, _ = model.predict(np.expand_dims(x, axis=0), deterministic=INF_DETERMINISTIC)\n",
        "        if isinstance(action, (list, np.ndarray)):\n",
        "            action = np.asarray(action)\n",
        "            if action.ndim > 0:\n",
        "                action = action[0]\n",
        "\n",
        "    return action_to_weight(action)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# (C) Market close behavior (Colab-safe)\n",
        "# ============================================================\n",
        "def _sleep_until_open_or_exit(trading_api: TradingClient) -> bool:\n",
        "    \"\"\"\n",
        "    Returns True if we should continue looping, False if we should exit (Colab-safe default).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        clock = trading_api.get_clock()\n",
        "        if getattr(clock, \"is_open\", False):\n",
        "            return True\n",
        "\n",
        "        nxt = pd.to_datetime(getattr(clock, \"next_open\"), utc=True, errors=\"coerce\")\n",
        "        if pd.isna(nxt):\n",
        "            time.sleep(60)\n",
        "            return True\n",
        "\n",
        "        wait = max(1, int((nxt - now_utc()).total_seconds()))\n",
        "        logging.info(\"Market closed. Next open in %ds.\", wait)\n",
        "\n",
        "        if IN_COLAB and _to_bool(os.getenv(\"COLAB_EXIT_WHEN_CLOSED\", \"1\")):\n",
        "            logging.info(\"IN_COLAB and COLAB_EXIT_WHEN_CLOSED=1 -> exiting cleanly instead of long sleep.\")\n",
        "            return False\n",
        "\n",
        "        chunk = int(os.getenv(\"CLOSED_SLEEP_CHUNK_SEC\", \"120\"))\n",
        "        remaining = wait\n",
        "        while remaining > 0:\n",
        "            s = min(chunk, remaining)\n",
        "            logging.info(\"[CLOSED] sleeping %ds (remaining %ds)\", s, remaining)\n",
        "            time.sleep(s)\n",
        "            remaining -= s\n",
        "        return True\n",
        "\n",
        "    except Exception:\n",
        "        time.sleep(60)\n",
        "        return True\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# One-symbol live step\n",
        "# ============================================================\n",
        "def run_live_once_for_symbol(\n",
        "    trading_api: TradingClient,\n",
        "    data_api: StockHistoricalDataClient,\n",
        "    symbol: str,\n",
        "    model: PPO,\n",
        "    vecnorm: Optional[VecNormalize],\n",
        "    features_hint: Optional[dict] = None,\n",
        "    cycle_equity: Optional[float] = None,\n",
        "):\n",
        "    # If exposure caps are breached, we allow ONLY de-risking trades.\n",
        "    block_risk = bool(globals().get(\"_EXPOSURE_CAPS_BLOCK_RISK\", False)) or bool(_EXPOSURE_CAPS_BLOCK_RISK)\n",
        "\n",
        "    shape = expected_obs_shape(model, vecnorm)\n",
        "    lookback = int(shape[0]) if (shape and len(shape) == 2) else None\n",
        "    bars_need = max(200, (lookback or 0) * 3)\n",
        "\n",
        "    bars_df = get_recent_bars(data_api, symbol, limit=bars_need, timeframe=LIVE_TIMEFRAME)\n",
        "    if bars_df is None or bars_df.empty:\n",
        "        logging.warning(\"[%s] No recent bars; skipping.\", symbol)\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        latest_bar_time = ensure_utc(pd.Timestamp(bars_df.index[-1]))\n",
        "    except Exception:\n",
        "        latest_bar_time = None\n",
        "\n",
        "    if latest_bar_time is not None:\n",
        "        prev = _LAST_BAR_TIME_SEEN.get(symbol)\n",
        "\n",
        "        if str(globals().get(\"DATA_TIMEFRAME\", \"1H\")).upper() in (\"1H\", \"1HR\", \"60MIN\", \"1HOUR\"):\n",
        "            if not is_hour_close(latest_bar_time):\n",
        "                eq = float(cycle_equity) if cycle_equity is not None else float(get_account_equity(trading_api))\n",
        "                px = float(bars_df[\"Close\"].iloc[-1])\n",
        "                if _LAST_BAR_TIME_SKIPPED.get(symbol) != latest_bar_time:\n",
        "                    _LAST_BAR_TIME_SKIPPED[symbol] = latest_bar_time\n",
        "                    log_trade_symbol(symbol, latest_bar_time, _LAST_RAW_A.get(symbol, 0.0),\n",
        "                                     _LAST_TARGET_W.get(symbol, 0.0), _LAST_CONF.get(symbol, 0.0),\n",
        "                                     px, eq, DRY_RUN, note=\"heartbeat_skip_not_hour_close\")\n",
        "                return\n",
        "\n",
        "        if prev is not None and latest_bar_time <= prev:\n",
        "            eq = float(cycle_equity) if cycle_equity is not None else float(get_account_equity(trading_api))\n",
        "            px = float(bars_df[\"Close\"].iloc[-1])\n",
        "            if _LAST_BAR_TIME_SKIPPED.get(symbol) != latest_bar_time:\n",
        "                _LAST_BAR_TIME_SKIPPED[symbol] = latest_bar_time\n",
        "                log_trade_symbol(symbol, latest_bar_time, _LAST_RAW_A.get(symbol, 0.0),\n",
        "                                 _LAST_TARGET_W.get(symbol, 0.0), _LAST_CONF.get(symbol, 0.0),\n",
        "                                 px, eq, DRY_RUN, note=\"heartbeat_skip_same_bar\")\n",
        "            return\n",
        "\n",
        "        _LAST_BAR_TIME_SEEN[symbol] = latest_bar_time\n",
        "        _LAST_BAR_TIME_SKIPPED.pop(symbol, None)\n",
        "\n",
        "    block_until = _REENTRY_BLOCK_UNTIL.get(symbol, 0.0)\n",
        "    if time.time() < block_until:\n",
        "        remaining = int(max(0, block_until - time.time()))\n",
        "        logging.info(f\"[{symbol}] Re-entry cooldown active ({remaining}s left); skipping.\")\n",
        "        eq = float(cycle_equity) if cycle_equity is not None else float(get_account_equity(trading_api))\n",
        "        px = float(bars_df[\"Close\"].iloc[-1])\n",
        "        log_trade_symbol(symbol, bars_df.index[-1], 0.0, 0.0, 0.0, px, eq, DRY_RUN, note=\"reentry_cooldown\")\n",
        "        return\n",
        "\n",
        "    min_rows_needed = max(20, int(shape[0]) if (shape and len(shape) == 2) else 60)\n",
        "    obs, obs_ts = prepare_observation_from_bars(\n",
        "        bars_df,\n",
        "        features_hint=features_hint,\n",
        "        min_required_rows=min_rows_needed,\n",
        "        expected_shape=shape,\n",
        "        symbol=symbol,\n",
        "    )\n",
        "\n",
        "    _now_ts = utc_ts(now_utc())\n",
        "    if _now_ts - obs_ts >= STALE_MAX_SEC:\n",
        "        eq = float(cycle_equity) if cycle_equity is not None else float(get_account_equity(trading_api))\n",
        "        px = float(bars_df[\"Close\"].iloc[-1])\n",
        "        logging.info(f\"[{symbol}] Observation stale; skipping.\")\n",
        "        log_trade_symbol(symbol, bars_df.index[-1], 0.0, 0.0, 0.0, px, eq, DRY_RUN, note=\"skip_stale\")\n",
        "        return\n",
        "\n",
        "    if check_tp_sl_and_maybe_flatten(trading_api, symbol):\n",
        "        return\n",
        "\n",
        "    target_w, conf, raw = infer_target_weight(model, vecnorm, obs)\n",
        "\n",
        "    _LAST_TARGET_W[symbol] = float(target_w)\n",
        "    _LAST_CONF[symbol] = float(conf)\n",
        "    _LAST_RAW_A[symbol] = float(raw)\n",
        "\n",
        "    logging.info(\"[%s] predict() ok → raw=%.4f target_w=%.4f conf=%.3f\", symbol, raw, target_w, conf)\n",
        "\n",
        "    eq = float(cycle_equity) if cycle_equity is not None else float(get_account_equity(trading_api))\n",
        "    px = float(bars_df[\"Close\"].iloc[-1])\n",
        "    pos = get_position(trading_api, symbol)\n",
        "\n",
        "    have_mv = float(getattr(pos, \"market_value\", 0.0) or 0.0) if pos else 0.0\n",
        "    cur_w = (have_mv / eq) if (np.isfinite(eq) and eq > 0) else 0.0\n",
        "\n",
        "    have_qty = float(getattr(pos, \"qty\", 0.0) or 0.0) if pos else 0.0\n",
        "    has_pos = (abs(have_qty) > 1e-9)\n",
        "\n",
        "    # gates\n",
        "    RAW_POS_MIN_LOCAL = float(globals().get(\"RAW_POS_MIN\", 0.0))\n",
        "    if target_w > 0 and raw < RAW_POS_MIN_LOCAL:\n",
        "        log_trade_symbol(symbol, bars_df.index[-1], raw, target_w, conf, px, eq, DRY_RUN, note=\"raw_gate_long\")\n",
        "        return\n",
        "\n",
        "    RAW_NEG_GATE = float(globals().get(\"RAW_NEG_MAX\", 0.0))\n",
        "    if target_w < 0 and abs(raw) < RAW_NEG_GATE:\n",
        "        log_trade_symbol(symbol, bars_df.index[-1], raw, target_w, conf, px, eq, DRY_RUN, note=\"raw_gate_short\")\n",
        "        return\n",
        "\n",
        "    if abs(target_w) <= EXIT_WEIGHT_MAX and pos:\n",
        "        logging.info(f\"[{symbol}] Model near-flat (≤{EXIT_WEIGHT_MAX:.3f}); flattening.\")\n",
        "        flatten_symbols_strict(trading_api, [symbol], timeout_sec=90)\n",
        "        log_trade_symbol(symbol, bars_df.index[-1], raw, target_w, conf, px, eq, DRY_RUN, note=\"flatten\")\n",
        "        return\n",
        "\n",
        "    wants_trade = (abs(target_w) >= ENTER_WEIGHT_MIN and conf >= ENTER_CONF_MIN)\n",
        "    if not wants_trade:\n",
        "        log_trade_symbol(symbol, bars_df.index[-1], raw, target_w, conf, px, eq, DRY_RUN, note=\"no_trade_gate\")\n",
        "        return\n",
        "\n",
        "    # (D) exposure-cap enforcement: if caps breached, block risk-increasing trades\n",
        "    if block_risk and trade_increases_risk(cur_w, target_w):\n",
        "        log_trade_symbol(symbol, bars_df.index[-1], raw, target_w, conf, px, eq, DRY_RUN, note=\"exposure_cap_block_risk_increase\")\n",
        "        return\n",
        "\n",
        "    event_gap = _SEED_COOLDOWN_SEC if (SEED_FIRST_SHARE and not has_pos) else 30\n",
        "    if not begin_order_event(symbol, event_gap):\n",
        "        note = \"order_event_cooldown_seed\" if (SEED_FIRST_SHARE and not has_pos) else \"order_event_cooldown_rebalance\"\n",
        "        log_trade_symbol(symbol, bars_df.index[-1], raw, target_w, conf, px, eq, DRY_RUN, note=note)\n",
        "        return\n",
        "\n",
        "    tradable, fractionable, _shortable = _asset_flags(symbol)\n",
        "    if not tradable:\n",
        "        log_trade_symbol(symbol, bars_df.index[-1], raw, target_w, conf, px, eq, DRY_RUN, note=\"not_tradable\")\n",
        "        return\n",
        "\n",
        "    # (B) Hard-cap enforcement BEFORE policy rebalance\n",
        "    cap = float(globals().get(\"WEIGHT_CAP\", 0.40))\n",
        "    cap_fix = enforce_position_caps_if_violated(trading_api, data_api, symbol, eq, cap)\n",
        "    if int(cap_fix.get(\"order_submitted\", 0)) == 1:\n",
        "        stamp_order_event(symbol)\n",
        "        log_trade_symbol(symbol, bars_df.index[-1], raw, target_w, conf, px, eq, DRY_RUN,\n",
        "                         note=\"hard_cap_enforced_before_policy\",\n",
        "                         order_submitted=cap_fix.get(\"order_submitted\", 0),\n",
        "                         order_id=cap_fix.get(\"order_id\", \"\"),\n",
        "                         order_status=cap_fix.get(\"order_status\", \"\"),\n",
        "                         filled_qty=cap_fix.get(\"filled_qty\", \"\"))\n",
        "        return\n",
        "\n",
        "    # Normal rebalance\n",
        "    order_info = rebalance_to_weight(trading_api, data_api, symbol, eq, target_w)\n",
        "    if int(order_info.get(\"order_submitted\", 0)) == 1 or DRY_RUN:\n",
        "        stamp_order_event(symbol)\n",
        "\n",
        "    log_trade_symbol(symbol, bars_df.index[-1], raw, target_w, conf, px, eq, DRY_RUN,\n",
        "                     note=\"rebalance_only\",\n",
        "                     order_submitted=order_info.get(\"order_submitted\", 0),\n",
        "                     order_id=order_info.get(\"order_id\", \"\"),\n",
        "                     order_status=order_info.get(\"order_status\", \"\"),\n",
        "                     filled_qty=order_info.get(\"filled_qty\", \"\"))\n",
        "    return\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Live runner\n",
        "# ============================================================\n",
        "def run_live(tickers: List[str], trading_api: TradingClient, data_api: StockHistoricalDataClient):\n",
        "    def minutes_to_close(trading_api: TradingClient) -> Optional[int]:\n",
        "        clk = trading_api.get_clock()\n",
        "        if getattr(clk, \"is_open\", False):\n",
        "            close = pd.to_datetime(clk.next_close, utc=True)\n",
        "            return int(max(0, (close - now_utc()).total_seconds() // 60))\n",
        "        return None\n",
        "\n",
        "    per_ticker: Dict[str, Tuple[PPO, Optional[VecNormalize], Optional[dict]]] = {}\n",
        "    best = (globals().get(\"BEST_WINDOW_ENV\") or None)\n",
        "\n",
        "    for t in tickers:\n",
        "        try:\n",
        "            picks = pick_artifacts_for_ticker(t, os.getenv(\"ARTIFACTS_DIR\", str(ARTIFACTS_DIR)), best_window=best)\n",
        "            model = load_ppo_model(picks[\"model\"])\n",
        "            vecnorm = load_vecnormalize(picks.get(\"vecnorm\"))\n",
        "            if vecnorm is not None and hasattr(vecnorm, \"training\"):\n",
        "                vecnorm.training = False\n",
        "            if vecnorm is not None and hasattr(vecnorm, \"norm_reward\"):\n",
        "                vecnorm.norm_reward = False\n",
        "            feats = load_features(picks.get(\"features\"))\n",
        "            per_ticker[t] = (model, vecnorm, feats)\n",
        "            logging.info(\"[%s] Artifacts loaded and ready.\", t)\n",
        "        except Exception as e:\n",
        "            logging.exception(\"[%s] Failed to load artifacts: %s\", t, e)\n",
        "\n",
        "    if not per_ticker:\n",
        "        raise RuntimeError(\"No models loaded for any ticker. Check artifacts directory and names.\")\n",
        "\n",
        "    loaded_syms = list(per_ticker.keys())\n",
        "    logging.info(\"Starting live execution for (loaded): %s\", loaded_syms)\n",
        "\n",
        "    global _last_kill_ts\n",
        "    cycle = 0\n",
        "    last_plot_ts = 0\n",
        "    flattened_today = False\n",
        "    did_start_flat = False\n",
        "\n",
        "    logging.info(\"Starting live trading loop\")\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            if not ensure_market_open(trading_api):\n",
        "                flattened_today = False\n",
        "                globals()[\"SESSION_OPEN_EQUITY\"] = None\n",
        "                if not _sleep_until_open_or_exit(trading_api):\n",
        "                    break\n",
        "                continue\n",
        "\n",
        "            if globals().get(\"SESSION_OPEN_EQUITY\") is None:\n",
        "                try:\n",
        "                    globals()[\"SESSION_OPEN_EQUITY\"] = float(trading_api.get_account().equity)\n",
        "                    logging.info(\"Session open equity anchor set: %.2f\", globals()[\"SESSION_OPEN_EQUITY\"])\n",
        "                except Exception as e:\n",
        "                    logging.debug(\"Could not set SESSION_OPEN_EQUITY: %s\", e)\n",
        "\n",
        "            t_cycle_start = time.perf_counter()\n",
        "\n",
        "            try:\n",
        "                cycle_equity = float(trading_api.get_account().equity)\n",
        "            except Exception as e:\n",
        "                logging.warning(\"Could not fetch equity: %s\", e)\n",
        "                cycle_equity = float(\"nan\")\n",
        "\n",
        "            print(f\"[HEARTBEAT] {now_utc().isoformat()} cycle={cycle} equity={cycle_equity:,.2f}\", flush=True)\n",
        "            _ = print_position_summary(trading_api)\n",
        "\n",
        "            # (D) exposure caps: update state once per cycle (on loaded symbols)\n",
        "            try:\n",
        "                block_risk, gross, net = exposure_caps_update_and_flags(trading_api, loaded_syms)\n",
        "                globals()[\"_EXPOSURE_CAPS_BLOCK_RISK\"] = bool(block_risk)\n",
        "                # Optional emergency flatten\n",
        "                if maybe_emergency_flatten_on_exposure(trading_api, loaded_syms):\n",
        "                    for s in loaded_syms:\n",
        "                        _REENTRY_BLOCK_UNTIL[s] = time.time() + min(int(os.getenv(\"REENTRY_COOLDOWN_SEC\", \"300\")), 120)\n",
        "            except Exception as e:\n",
        "                logging.warning(\"[EXPOSURE_CAP] update failed: %s\", e)\n",
        "                globals()[\"_EXPOSURE_CAPS_BLOCK_RISK\"] = False\n",
        "\n",
        "            # (A) START_FLAT strict (once, and only if there is anything to flatten)\n",
        "            if (cycle == 0) and (not did_start_flat):\n",
        "                did_start_flat = True\n",
        "                start_flat = _to_bool(os.getenv(\"START_FLAT\", \"1\"))\n",
        "\n",
        "                need_flatten = False\n",
        "                mv_dust = float(os.getenv(\"START_FLAT_MV_DUST\", \"50.0\"))\n",
        "\n",
        "                try:\n",
        "                    pos = trading_api.get_all_positions() or []\n",
        "                    mv_map = {str(p.symbol).upper(): float(getattr(p, \"market_value\", 0.0) or 0.0) for p in pos}\n",
        "                    for s in loaded_syms:\n",
        "                        if abs(mv_map.get(s.upper(), 0.0)) > mv_dust:\n",
        "                            need_flatten = True\n",
        "                            break\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "                try:\n",
        "                    oo = list_open_orders(trading_api, symbols=loaded_syms)\n",
        "                    if oo:\n",
        "                        need_flatten = True\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "                if start_flat and need_flatten:\n",
        "                    logging.warning(\"START_FLAT=1: strict flatten (cancel orders + close positions + wait flat).\")\n",
        "                    ok = flatten_symbols_strict(\n",
        "                        trading_api,\n",
        "                        loaded_syms,\n",
        "                        dust_qty=float(os.getenv(\"DUST_QTY\", \"0.000001\")),\n",
        "                        timeout_sec=int(os.getenv(\"START_FLAT_TIMEOUT_SEC\", \"150\")),\n",
        "                    )\n",
        "                    logging.info(\"Flatten wait complete: %s\", ok)\n",
        "                    for s in loaded_syms:\n",
        "                        _REENTRY_BLOCK_UNTIL[s] = time.time() + min(int(os.getenv(\"REENTRY_COOLDOWN_SEC\", \"300\")), 60)\n",
        "\n",
        "            # per-symbol step\n",
        "            for sym, (model, vecnorm, feat_hint) in per_ticker.items():\n",
        "                t_sym_start = time.perf_counter()\n",
        "                try:\n",
        "                    _call_with_timeout(run_live_once_for_symbol, 15, trading_api, data_api, sym, model, vecnorm, feat_hint, cycle_equity)\n",
        "                except Exception as e:\n",
        "                    logging.warning(\"[%s] symbol step timeout/fail: %s\", sym, e)\n",
        "                finally:\n",
        "                    logging.info(\"[TIMER] %s symbol work: %.3fs\", sym, time.perf_counter() - t_sym_start)\n",
        "\n",
        "            # equity snapshot + run summary\n",
        "            maybe_log_equity_snapshot(trading_api_in=trading_api, reason=(\"trade\" if globals().get(\"_TRADE_EVENT_FLAG\", False) else \"cycle\"))\n",
        "            try:\n",
        "                append_run_summary(trading_api, loaded_syms, RESULTS_DIR, latest_dir=LATEST_DIR)\n",
        "            except Exception as e:\n",
        "                logging.warning(\"append_run_summary failed: %s\", e)\n",
        "                try:\n",
        "                    append_run_summary(trading_api, loaded_syms, RESULTS_DIR, latest_dir=LATEST_DIR, error=f\"callsite:{e}\")\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "            # Kill-switch\n",
        "            try:\n",
        "                anchor = globals().get(\"SESSION_OPEN_EQUITY\", None)\n",
        "                if anchor is not None:\n",
        "                    eq_now = float(trading_api.get_account().equity)\n",
        "                    dd = (eq_now / max(1e-9, float(anchor))) - 1.0\n",
        "                    max_dd = float(os.getenv(\"MAX_DAILY_DRAWDOWN_PCT\", str(globals().get(\"MAX_DAILY_DRAWDOWN_PCT\", 0.05))))\n",
        "                    if dd <= -abs(max_dd):\n",
        "                        if time.time() - _last_kill_ts > 60:\n",
        "                            logging.warning(\"KILL-SWITCH: daily drawdown %.2f%% reached. Flattening & pausing.\", 100.0 * dd)\n",
        "                            flatten_symbols_strict(trading_api, loaded_syms, timeout_sec=150)\n",
        "                            _last_kill_ts = time.time()\n",
        "                            try:\n",
        "                                maybe_log_equity_snapshot(trading_api_in=trading_api, reason=\"trade\")\n",
        "                                append_run_summary(trading_api, loaded_syms, RESULTS_DIR, latest_dir=LATEST_DIR, error=\"kill_switch_flatten\")\n",
        "                            except Exception:\n",
        "                                pass\n",
        "                            cooldown_min = int(os.getenv(\"KILL_SWITCH_COOLDOWN_MIN\", str(globals().get(\"KILL_SWITCH_COOLDOWN_MIN\", 30))))\n",
        "                            if not DRY_RUN:\n",
        "                                time.sleep(60 * cooldown_min)\n",
        "                            continue\n",
        "            except Exception as e:\n",
        "                logging.debug(\"kill-switch check failed: %s\", e)\n",
        "\n",
        "            # Flatten into close\n",
        "            m2c = minutes_to_close(trading_api)\n",
        "            if FLATTEN_INTO_CLOSE and not flattened_today and m2c is not None and m2c <= 5:\n",
        "                logging.info(\"Flattening into close.\")\n",
        "                flatten_symbols_strict(trading_api, loaded_syms, timeout_sec=150)\n",
        "                for s in loaded_syms:\n",
        "                    _REENTRY_BLOCK_UNTIL[s] = time.time() + int(os.getenv(\"REENTRY_COOLDOWN_SEC\", \"300\"))\n",
        "                maybe_log_equity_snapshot(trading_api_in=trading_api, reason=\"close\")\n",
        "                flattened_today = True\n",
        "                if bool(globals().get(\"EXIT_AFTER_CLOSE\", False)):\n",
        "                    logging.info(\"EXIT_AFTER_CLOSE=True — exiting live loop after close flatten.\")\n",
        "                    break\n",
        "\n",
        "            # plots + metrics\n",
        "            now_ts = time.time()\n",
        "            if now_ts - last_plot_ts >= 900:\n",
        "                try:\n",
        "                    plot_equity_curve(from_equity_csv=True)\n",
        "                    df = pd.read_csv(EQUITY_LOG_CSV, parse_dates=[\"datetime_utc\"])\n",
        "                    m = compute_performance_metrics(df)\n",
        "                    logging.info(\"Perf: cum_return=%.2f%% | sharpe=%.2f | maxDD=%.2f%%\",\n",
        "                                 100 * m[\"cum_return\"], m[\"sharpe\"], 100 * m[\"max_drawdown\"])\n",
        "                except Exception as e:\n",
        "                    logging.warning(\"Plot/metrics failed: %s\", e)\n",
        "                last_plot_ts = now_ts\n",
        "\n",
        "            logging.info(\"[TIMER] full-cycle active time: %.3fs (cooldown=%d min)\",\n",
        "                         time.perf_counter() - t_cycle_start, COOLDOWN_MIN)\n",
        "\n",
        "            cycle += 1\n",
        "            if (cycle % 12) == 0:\n",
        "                gc.collect()\n",
        "\n",
        "            _sleep_to_next_minute_block(COOLDOWN_MIN)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        logging.info(\"KeyboardInterrupt: stopping live loop.\")\n",
        "    except Exception as e:\n",
        "        logging.exception(\"Live loop exception: %s\", e)\n",
        "        try:\n",
        "            log_equity_snapshot(trading_api_in=trading_api)\n",
        "        except Exception:\n",
        "            pass\n",
        "    finally:\n",
        "        global _TIMEOUT_EXEC\n",
        "        try:\n",
        "            _TIMEOUT_EXEC.shutdown(wait=False, cancel_futures=True)\n",
        "        except Exception:\n",
        "            pass\n",
        "        _TIMEOUT_EXEC = ThreadPoolExecutor(max_workers=8)\n",
        "\n",
        "        try:\n",
        "            if FORCE_FLATTEN_ON_EXIT:\n",
        "                flatten_symbols_strict(trading_api, tickers, timeout_sec=180)\n",
        "        except Exception as e:\n",
        "            logging.warning(\"Flatten-on-exit skipped: %s\", e)\n",
        "\n",
        "        try:\n",
        "            maybe_log_equity_snapshot(trading_api_in=trading_api, reason=\"finalize\")\n",
        "            plot_equity_curve(from_equity_csv=True)\n",
        "        except Exception as e:\n",
        "            logging.warning(\"Finalization failed: %s\", e)\n",
        "\n",
        "        logging.info(\"Live loop exited cleanly.\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Logging setup after paths\n",
        "# ============================================================\n",
        "def setup_logging_after_paths():\n",
        "    warnings.filterwarnings(\"default\")\n",
        "    level = getattr(logging, os.getenv(\"LOG_LEVEL\", \"INFO\").upper(), logging.INFO)\n",
        "\n",
        "    root = logging.getLogger()\n",
        "    root.handlers.clear()\n",
        "    root.setLevel(level)\n",
        "\n",
        "    fmt = logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\")\n",
        "\n",
        "    sh = logging.StreamHandler(sys.stdout)\n",
        "    sh.setLevel(level)\n",
        "    sh.setFormatter(fmt)\n",
        "    root.addHandler(sh)\n",
        "\n",
        "    log_path = RESULTS_DIR / \"live_loop.log\"\n",
        "    fh = logging.FileHandler(log_path)\n",
        "    fh.setLevel(level)\n",
        "    fh.setFormatter(fmt)\n",
        "    root.addHandler(fh)\n",
        "\n",
        "    try:\n",
        "        sys.stdout.reconfigure(line_buffering=True)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def log_config_banner():\n",
        "    try:\n",
        "        artifacts_list = sorted(p.name for p in ARTIFACTS_DIR.iterdir()) if ARTIFACTS_DIR.exists() else []\n",
        "    except Exception:\n",
        "        artifacts_list = []\n",
        "    logging.info(\"EXIT_AFTER_CLOSE      : %s\", os.getenv(\"EXIT_AFTER_CLOSE\", \"0\"))\n",
        "    logging.info(\"FORCE_FIRST_BUY       : %s\", FORCE_FIRST_BUY)\n",
        "    logging.info(\"FORCE_FLATTEN_ON_EXIT : %s\", FORCE_FLATTEN_ON_EXIT)\n",
        "    logging.info(\"CONFIG\")\n",
        "    logging.info(\"Project root          : %s\", PROJECT_ROOT)\n",
        "    logging.info(\"ARTIFACTS_DIR         : %s\", ARTIFACTS_DIR)\n",
        "    logging.info(\"RESULTS_DIR           : %s\", RESULTS_DIR)\n",
        "    logging.info(\"Tickers               : %s\", TICKERS)\n",
        "    logging.info(\"API base              : %s\", BASE_URL)\n",
        "    logging.info(\"AUTO_RUN_LIVE         : %s\", os.getenv(\"AUTO_RUN_LIVE\", \"\"))\n",
        "    logging.info(\"INF_DETERMINISTIC     : %s\", INF_DETERMINISTIC)\n",
        "    logging.info(\"ALLOW_SHORTS          : %s\", ALLOW_SHORTS)\n",
        "    logging.info(\"FLATTEN_INTO_CLOSE    : %s\", FLATTEN_INTO_CLOSE)\n",
        "    logging.info(\"DRY_RUN=%s | BARS_FEED=%s | USE_FRACTIONALS=%s | COOLDOWN_MIN=%s | STALE_MAX_SEC=%s\",\n",
        "                 DRY_RUN, BARS_FEED, USE_FRACTIONALS, COOLDOWN_MIN, STALE_MAX_SEC)\n",
        "    logging.info(\"DATA_TIMEFRAME        : %s (model bars)\", os.getenv(\"DATA_TIMEFRAME\", \"1H\"))\n",
        "    logging.info(\"EQUITY_TIMEFRAME      : %s (equity reporting)\", os.getenv(\"EQUITY_TIMEFRAME\", \"5Min\"))\n",
        "    logging.info(\"MAX_DD_PCT: %.3f | KILL_SWITCH_COOLDOWN_MIN: %s\",\n",
        "                 float(globals().get(\"MAX_DAILY_DRAWDOWN_PCT\", 0.05)),\n",
        "                 os.getenv(\"KILL_SWITCH_COOLDOWN_MIN\", str(globals().get(\"KILL_SWITCH_COOLDOWN_MIN\", 30))))\n",
        "    logging.info(\"WEIGHT_CAP: %.3f | SIZING_MODE: %s | ENTER_CONF_MIN: %.3f | ENTER_WEIGHT_MIN: %.3f | EXIT_WEIGHT_MAX: %.3f | REBALANCE_MIN_NOTIONAL: %.2f\",\n",
        "                 WEIGHT_CAP, SIZING_MODE, ENTER_CONF_MIN, ENTER_WEIGHT_MIN, EXIT_WEIGHT_MAX, REBALANCE_MIN_NOTIONAL)\n",
        "    logging.info(\"TAKE_PROFIT_PCT: %.3f | STOP_LOSS_PCT: %.3f | BEST_WINDOW_ENV: %s\",\n",
        "                 TAKE_PROFIT_PCT, STOP_LOSS_PCT, (BEST_WINDOW_ENV or \"\"))\n",
        "    logging.info(\"GROSS_CAP: %.3f | NET_CAP: %.3f | EMERGENCY_FLATTEN_ON_EXPOSURE: %s\",\n",
        "                 float(globals().get(\"GROSS_CAP\", 1.0)), float(globals().get(\"NET_CAP\", 1.0)),\n",
        "                 bool(globals().get(\"EMERGENCY_FLATTEN_ON_EXPOSURE\", False)))\n",
        "    if artifacts_list:\n",
        "        logging.info(\"Artifacts present (%d): %s\", len(artifacts_list), \", \".join(artifacts_list))\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Artifacts housekeeping\n",
        "# ============================================================\n",
        "def dedupe_artifacts_dir(artifacts_dir: Path) -> None:\n",
        "    dupes = sorted(artifacts_dir.glob(\"* (1).*\"))\n",
        "    if not dupes:\n",
        "        logging.info(\"No duplicate (1) artifacts found.\")\n",
        "        return\n",
        "    backup = artifacts_dir / \"_dupes_backup\"\n",
        "    backup.mkdir(parents=True, exist_ok=True)\n",
        "    for p in dupes:\n",
        "        try:\n",
        "            p.rename(backup / p.name)\n",
        "        except Exception:\n",
        "            pass\n",
        "    logging.warning(\"Moved %d duplicate artifacts to %s\", len(dupes), backup)\n",
        "\n",
        "def _is_colab_runtime() -> bool:\n",
        "    # IN_COLAB is already set earlier in your file, but keep this robust.\n",
        "    try:\n",
        "        return bool(globals().get(\"IN_COLAB\", False))\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def colab_interactive_upload_before_init(project_root: Path) -> None:\n",
        "    if not _is_colab_runtime():\n",
        "        return\n",
        "\n",
        "    # Allow opt-out without changing any non-Colab behavior.\n",
        "    if str(os.getenv(\"COLAB_INTERACTIVE_UPLOAD\", \"1\")).strip().lower() in (\"0\", \"false\", \"no\", \"off\"):\n",
        "        logging.info(\"COLAB_INTERACTIVE_UPLOAD=0 -> skipping Colab upload dialogs.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        from google.colab import files  # type: ignore\n",
        "    except Exception:\n",
        "        logging.warning(\"Colab detected but google.colab.files unavailable; skipping upload dialogs.\")\n",
        "        return\n",
        "\n",
        "    project_root = Path(project_root)\n",
        "    project_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # IMPORTANT: ARTIFACTS_DIR is not configured yet, so we use env/default path now.\n",
        "    artifacts_dir = Path(os.getenv(\"ARTIFACTS_DIR\", str(project_root / \"artifacts\")))\n",
        "    artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # ---- (1) Upload .env\n",
        "    logging.info(\"[COLAB UPLOAD] Please upload your .env file (contains Alpaca API keys).\")\n",
        "    logging.info(\"[COLAB UPLOAD] If you already have %s, you can cancel/skip.\", str(project_root / \".env\"))\n",
        "    try:\n",
        "        uploaded_env = files.upload()  # opens upload dialog\n",
        "    except Exception as e:\n",
        "        logging.warning(\"[COLAB UPLOAD] .env upload dialog failed: %s\", e)\n",
        "        uploaded_env = {}\n",
        "\n",
        "    env_saved = False\n",
        "    if uploaded_env:\n",
        "        # Prefer a file literally named \".env\", otherwise take the first upload.\n",
        "        chosen_name = \".env\" if \".env\" in uploaded_env else next(iter(uploaded_env.keys()))\n",
        "        try:\n",
        "            env_bytes = uploaded_env[chosen_name]\n",
        "            env_path = project_root / \".env\"\n",
        "            env_path.write_bytes(env_bytes)\n",
        "            logging.info(\"[COLAB UPLOAD] Saved .env -> %s\", str(env_path.resolve()))\n",
        "            env_saved = True\n",
        "        except Exception as e:\n",
        "            logging.warning(\"[COLAB UPLOAD] Failed saving .env: %s\", e)\n",
        "\n",
        "    if not env_saved:\n",
        "        logging.info(\"[COLAB UPLOAD] No .env uploaded (or save failed). Will proceed with existing env/drive/local .env resolution.\")\n",
        "\n",
        "    # ---- (2) Upload artifacts\n",
        "    logging.info(\"[COLAB UPLOAD] Now upload PPO artifact files (model .zip, vecnorm .pkl, features .json, etc.).\")\n",
        "    logging.info(\"[COLAB UPLOAD] Target artifacts directory: %s\", str(artifacts_dir.resolve()))\n",
        "    try:\n",
        "        uploaded_artifacts = files.upload()  # opens upload dialog\n",
        "    except Exception as e:\n",
        "        logging.warning(\"[COLAB UPLOAD] Artifact upload dialog failed: %s\", e)\n",
        "        uploaded_artifacts = {}\n",
        "\n",
        "    saved_files: List[str] = []\n",
        "    skipped_files: List[str] = []\n",
        "\n",
        "    if uploaded_artifacts:\n",
        "        for name, data in uploaded_artifacts.items():\n",
        "            try:\n",
        "                # If user accidentally re-uploads .env here, keep it out of ARTIFACTS_DIR\n",
        "                if str(name).strip() == \".env\":\n",
        "                    skipped_files.append(name)\n",
        "                    continue\n",
        "\n",
        "                out_path = artifacts_dir / Path(name).name\n",
        "                out_path.write_bytes(data)\n",
        "                saved_files.append(out_path.name)\n",
        "            except Exception as e:\n",
        "                logging.warning(\"[COLAB UPLOAD] Failed saving artifact %s: %s\", name, e)\n",
        "\n",
        "    if saved_files:\n",
        "        logging.info(\"[COLAB UPLOAD] Saved %d artifact file(s) into %s\",\n",
        "                     len(saved_files), str(artifacts_dir.resolve()))\n",
        "        logging.info(\"[COLAB UPLOAD] Files: %s\", \", \".join(saved_files))\n",
        "    else:\n",
        "        logging.info(\"[COLAB UPLOAD] No artifacts uploaded. Will proceed using existing ARTIFACTS_DIR contents.\")\n",
        "\n",
        "    if skipped_files:\n",
        "        logging.info(\"[COLAB UPLOAD] Skipped files (not treated as artifacts): %s\", \", \".join(skipped_files))\n",
        "\n",
        "# ============================================================\n",
        "# Main\n",
        "# ============================================================\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    dedupe_artifacts_dir(Path(os.getenv(\"ARTIFACTS_DIR\", str(PROJECT_ROOT / \"artifacts\"))))\n",
        "    colab_interactive_upload_before_init(PROJECT_ROOT)\n",
        "    cfg = configure_knobs(overrides={\n",
        "        # data freshness\n",
        "        \"BARS_FEED\": \"\",\n",
        "        \"STALE_MAX_SEC\": 4200,\n",
        "\n",
        "        # sizing\n",
        "        \"SIZING_MODE\": \"linear\",\n",
        "        \"CONF_FLOOR\": 0.00,\n",
        "        \"WEIGHT_CAP\": 0.40,\n",
        "\n",
        "        # entry/exit sensitivity\n",
        "        \"ENTER_CONF_MIN\": 0.02,\n",
        "        \"ENTER_WEIGHT_MIN\": 0.002,\n",
        "        \"EXIT_WEIGHT_MAX\": 0.001,\n",
        "        \"DELTA_WEIGHT_MIN\": 0.002,\n",
        "        \"REBALANCE_MIN_NOTIONAL\": 25.00,\n",
        "\n",
        "        # posture\n",
        "        \"ALLOW_SHORTS\": True,\n",
        "        \"COOLDOWN_MIN\": 10,\n",
        "\n",
        "        # raw-action gates\n",
        "        \"RAW_POS_MIN\": 0.00,\n",
        "        \"RAW_NEG_MAX\": 0.00,\n",
        "\n",
        "        # risk\n",
        "        \"TAKE_PROFIT_PCT\": 0.05,\n",
        "        \"STOP_LOSS_PCT\": 0.02,\n",
        "\n",
        "        # logging cadence\n",
        "        \"EQUITY_LOG_THROTTLE_SEC\": 300,\n",
        "        \"SKIP_EQUITY_WHEN_DRY_RUN\": False,\n",
        "\n",
        "        # kill-switch\n",
        "        \"MAX_DAILY_DRAWDOWN_PCT\": 0.05,\n",
        "\n",
        "        # (D) caps\n",
        "        \"GROSS_CAP\": float(os.getenv(\"GROSS_CAP\", \"1.00\")),\n",
        "        \"NET_CAP\": float(os.getenv(\"NET_CAP\", \"0.80\")),\n",
        "        \"EMERGENCY_FLATTEN_ON_EXPOSURE\": _to_bool(os.getenv(\"EMERGENCY_FLATTEN_ON_EXPOSURE\", \"0\")),\n",
        "    })\n",
        "    globals()[\"cfg\"] = cfg\n",
        "\n",
        "    setup_logging_after_paths()\n",
        "\n",
        "    tf_key = normalize_tf_key(cfg.DATA_TIMEFRAME)\n",
        "    LIVE_TIMEFRAME = _TF_MAP.get(tf_key, TimeFrame.Hour)\n",
        "    globals()[\"LIVE_TIMEFRAME\"] = LIVE_TIMEFRAME\n",
        "\n",
        "    trained_key = normalize_tf_key(str(cfg.TRAIN_TIMEFRAME))\n",
        "    live_key = normalize_tf_key(str(cfg.DATA_TIMEFRAME))\n",
        "    if trained_key != live_key:\n",
        "        logging.warning(\"Timeframe mismatch: trained=%s live=%s. Only change DATA_TIMEFRAME if you retrained.\",\n",
        "                        cfg.TRAIN_TIMEFRAME, cfg.DATA_TIMEFRAME)\n",
        "\n",
        "    if cfg.AUTO_RUN_LIVE:\n",
        "        assert \"paper-api\" in BASE_URL.lower()\n",
        "\n",
        "    log_config_banner()\n",
        "    logging.info(\"DATA_TIMEFRAME=%s -> LIVE_TIMEFRAME=%s\", cfg.DATA_TIMEFRAME, LIVE_TIMEFRAME)\n",
        "\n",
        "    # Save run config snapshot\n",
        "    try:\n",
        "        cfg_path = RESULTS_DIR / \"run_config.json\"\n",
        "        payload = {\n",
        "            \"time\": utcnow_iso(),\n",
        "            \"tickers\": TICKERS,\n",
        "            \"dry_run\": DRY_RUN,\n",
        "            \"bars_feed\": BARS_FEED,\n",
        "            \"weight_cap\": WEIGHT_CAP,\n",
        "            \"enter_conf_min\": ENTER_CONF_MIN,\n",
        "            \"enter_weight_min\": ENTER_WEIGHT_MIN,\n",
        "            \"exit_weight_max\": EXIT_WEIGHT_MAX,\n",
        "            \"rebalance_min_notional\": REBALANCE_MIN_NOTIONAL,\n",
        "            \"delta_weight_min\": DELTA_WEIGHT_MIN,\n",
        "            \"tp\": TAKE_PROFIT_PCT,\n",
        "            \"sl\": STOP_LOSS_PCT,\n",
        "            \"allow_shorts\": ALLOW_SHORTS,\n",
        "            \"gross_cap\": float(globals().get(\"GROSS_CAP\", 1.0)),\n",
        "            \"net_cap\": float(globals().get(\"NET_CAP\", 1.0)),\n",
        "            \"emergency_flatten_on_exposure\": bool(globals().get(\"EMERGENCY_FLATTEN_ON_EXPOSURE\", False)),\n",
        "        }\n",
        "        tmp = cfg_path.with_suffix(\".tmp\")\n",
        "        tmp.write_text(json.dumps(payload, indent=2))\n",
        "        tmp.replace(cfg_path)\n",
        "    except Exception as e:\n",
        "        logging.warning(\"Could not write run_config.json: %s\", e)\n",
        "\n",
        "    assert \"paper-api\" in BASE_URL.lower(), f\"Refusing to trade: BASE_URL is not paper ({BASE_URL})\"\n",
        "\n",
        "    trading_api, data_api = init_clients()\n",
        "    globals()[\"trading_api\"] = trading_api\n",
        "    globals()[\"data_api\"] = data_api\n",
        "\n",
        "    acct = trading_api.get_account()\n",
        "    logging.info(\"SHORT CHECK (pre): ALLOW_SHORTS=%s | acct.shorting_enabled=%s\",\n",
        "                 globals().get(\"ALLOW_SHORTS\", None), getattr(acct, \"shorting_enabled\", None))\n",
        "    for s in TICKERS:\n",
        "        try:\n",
        "            a = trading_api.get_asset(s)\n",
        "            logging.info(\"[%s] asset.shortable=%s | tradable=%s | fractionable=%s\",\n",
        "                         s, getattr(a, \"shortable\", None), getattr(a, \"tradable\", None), getattr(a, \"fractionable\", None))\n",
        "        except Exception as e:\n",
        "            logging.info(\"[%s] get_asset failed: %s\", s, e)\n",
        "\n",
        "    assert not bool(getattr(acct, \"trading_blocked\", False)), f\"Trading is blocked: {getattr(acct,'status','')}\"\n",
        "    logging.info(\"Account status: %s | equity=%s | cash=%s\", acct.status, acct.equity, acct.cash)\n",
        "\n",
        "    if cfg.AUTO_RUN_LIVE:\n",
        "        run_live(TICKERS, trading_api, data_api)\n",
        "    else:\n",
        "        logging.info(\"AUTO_RUN_LIVE disabled; live loop not started.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5X68r85om00"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from alpaca.data.timeframe import TimeFrame\n",
        "\n",
        "RESULTS_DIR = Path(globals().get(\"RESULTS_DIR\", os.getenv(\"RESULTS_DIR\", \".\")))\n",
        "LATEST_DIR  = Path(globals().get(\"LATEST_DIR\",  os.getenv(\"LATEST_DIR\",  str(RESULTS_DIR))))\n",
        "\n",
        "eq_candidates = [\n",
        "    globals().get(\"EQUITY_LOG_CSV\"),\n",
        "    globals().get(\"EQUITY_LOG_LATEST\"),\n",
        "    RESULTS_DIR / \"equity_log.csv\",\n",
        "    LATEST_DIR / \"equity_log.csv\",\n",
        "]\n",
        "\n",
        "def _first_existing(paths):\n",
        "    for p in paths:\n",
        "        if p:\n",
        "            p = Path(p)\n",
        "            if p.exists() and p.is_file():\n",
        "                return p\n",
        "    return None\n",
        "\n",
        "eq_path = _first_existing(eq_candidates)\n",
        "if eq_path is None:\n",
        "    all_eq = list(RESULTS_DIR.glob(\"equity_log*.csv\")) + list(LATEST_DIR.glob(\"equity_log*.csv\"))\n",
        "    eq_path = max(all_eq, key=lambda p: p.stat().st_mtime, default=None)\n",
        "\n",
        "if eq_path and eq_path.exists():\n",
        "    print(f\"[equity source] {eq_path}\")\n",
        "    try:\n",
        "        eq = pd.read_csv(eq_path, parse_dates=[\"datetime_utc\"]).sort_values(\"datetime_utc\")\n",
        "        if not eq.empty:\n",
        "            r = eq[\"equity\"].pct_change().dropna()\n",
        "            sharpe_h = (r.mean() / (r.std() + 1e-12)) * np.sqrt(252 * 6.5) if len(r) else float(\"nan\")\n",
        "            print(\n",
        "                f\"\\nEquity summary — last: ${eq['equity'].iloc[-1]:,.2f} | \"\n",
        "                f\"n={len(eq)} pts | Sharpe(h): {sharpe_h:.2f} | src={eq_path}\"\n",
        "            )\n",
        "        else:\n",
        "            print(f\"No rows in equity log: {eq_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not summarize equity ({eq_path}): {e}\")\n",
        "else:\n",
        "    print(\"No equity_log*.csv found in RESULTS_DIR/LATEST_DIR.\")\n",
        "\n",
        "rs_candidates = [\n",
        "    RESULTS_DIR / \"run_summary.csv\",\n",
        "    LATEST_DIR / \"run_summary.csv\",\n",
        "]\n",
        "\n",
        "rs_path = _first_existing(rs_candidates)\n",
        "if rs_path and rs_path.exists():\n",
        "    print(f\"\\n[run_summary source] {rs_path}\")\n",
        "    try:\n",
        "        rs = pd.read_csv(rs_path)\n",
        "        if not rs.empty:\n",
        "            last = rs.iloc[-1]\n",
        "            print(\n",
        "                \"Run summary — \"\n",
        "                f\"last dt={last.get('datetime_utc', '')} | \"\n",
        "                f\"equity={last.get('equity', np.nan)} | \"\n",
        "                f\"cash={last.get('cash', np.nan)} | \"\n",
        "                f\"gross_exposure_pct={last.get('gross_exposure_pct', np.nan)} | \"\n",
        "                f\"net_exposure_pct={last.get('net_exposure_pct', np.nan)} | \"\n",
        "                f\"error={str(last.get('error',''))[:120]}\"\n",
        "            )\n",
        "            print(f\"Rows (decision ticks) logged: {len(rs)}\")\n",
        "        else:\n",
        "            print(\"run_summary.csv exists but is empty.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not summarize run_summary ({rs_path}): {e}\")\n",
        "else:\n",
        "    print(\"\\nNo run_summary.csv found yet (it is created after the first live cycle completes).\")\n",
        "\n",
        "def _resolve_tickers():\n",
        "    g = globals().get(\"TICKERS\", None)\n",
        "    # Base tickers from globals or env\n",
        "    if isinstance(g, (list, tuple, set)):\n",
        "        base = [str(x).upper() for x in g]\n",
        "    else:\n",
        "        env_val = os.getenv(\"TICKERS\", (g if isinstance(g, str) else \"\"))\n",
        "        base = [t.strip().upper() for t in str(env_val).split(\",\") if t.strip()]\n",
        "\n",
        "    discovered = [\n",
        "        p.stem.replace(\"trade_log_\", \"\").upper()\n",
        "        for p in list(RESULTS_DIR.glob(\"trade_log_*.csv\")) + list(LATEST_DIR.glob(\"trade_log_*.csv\"))\n",
        "    ]\n",
        "\n",
        "    # Filter out the master aggregate file so it doesn't become a \"ticker\"\n",
        "    discovered = [t for t in discovered if t != \"MASTER\"]\n",
        "\n",
        "    ticks = sorted(set(base) | set(discovered))\n",
        "    return ticks if ticks else [\"UNH\", \"GE\"]\n",
        "\n",
        "tickers_to_report = _resolve_tickers()\n",
        "print(\"Tickers to report:\", tickers_to_report)\n",
        "\n",
        "print(\"\\nTrade Summary:\")\n",
        "for ticker in tickers_to_report:\n",
        "    trade_candidates = [\n",
        "        RESULTS_DIR / f\"trade_log_{ticker}.csv\",\n",
        "        LATEST_DIR / f\"trade_log_{ticker}.csv\",\n",
        "    ]\n",
        "    log_path = _first_existing(trade_candidates)\n",
        "    if not log_path:\n",
        "        #Tolerate Drive duplicates like \"trade_log_XYZ (1).csv\"\n",
        "        any_logs = list(RESULTS_DIR.glob(f\"trade_log_{ticker}*.csv\")) + \\\n",
        "                   list(LATEST_DIR.glob(f\"trade_log_{ticker}*.csv\"))\n",
        "        log_path = max(any_logs, key=lambda p: p.stat().st_mtime, default=None)\n",
        "\n",
        "    if not log_path or not log_path.exists():\n",
        "        print(f\"{ticker}: no trades logged yet.\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(\n",
        "            log_path,\n",
        "            on_bad_lines=\"skip\",\n",
        "            engine=\"python\",\n",
        "            parse_dates=[\"log_time\", \"bar_time\"],\n",
        "        )\n",
        "        key = \"signal\" if \"signal\" in df.columns else (\"action\" if \"action\" in df.columns else None)\n",
        "        if key:\n",
        "            counts = df[key].value_counts(dropna=False).to_dict()\n",
        "            print(f\"{ticker}: {counts} | src={log_path.name}\")\n",
        "        else:\n",
        "            print(f\"{ticker}: log present but missing 'signal'/'action' columns. src={log_path.name}\")\n",
        "\n",
        "        if \"confidence\" in df.columns and df[\"confidence\"].notna().any():\n",
        "            plt.figure(figsize=(8, 3.5))\n",
        "            df[\"confidence\"].dropna().plot(kind=\"hist\", bins=10, edgecolor=\"black\")\n",
        "            plt.title(f\"{ticker} - Confidence Distribution\")\n",
        "            plt.xlabel(\"confidence\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        for col in [\"weight\", \"raw_action\"]:\n",
        "            if col in df.columns and df[col].notna().any():\n",
        "                s = df[col].dropna()\n",
        "                print(\n",
        "                    f\"{ticker} {col}: mean={s.mean():.3f}, std={s.std():.3f}, \"\n",
        "                    f\"min={s.min():.3f}, max={s.max():.3f}\"\n",
        "                )\n",
        "    except Exception as e:\n",
        "        print(f\"{ticker}: could not summarize trades ({log_path}): {e}\")\n",
        "\n",
        "try:\n",
        "    api = api if \"api\" in globals() else init_alpaca()\n",
        "    positions = api.list_positions()\n",
        "    total_market_value = 0.0\n",
        "    print(\"\\nPosition Summary:\")\n",
        "    for p in positions:\n",
        "        mv = float(p.market_value)\n",
        "        total_market_value += mv\n",
        "        print(f\"  {p.symbol}: {p.qty} shares @ ${float(p.current_price):.2f} | Value: ${mv:,.2f}\")\n",
        "    print(f\"\\nTotal Market Value: ${total_market_value:,.2f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not summarize positions: {e}\")\n",
        "\n",
        "def count_filled_orders_since(api, symbol: str, days: int = 14) -> int:\n",
        "    # alpaca-py expects a datetime (not ISO string) for \"after\"\n",
        "    after_dt = datetime.now(timezone.utc) - timedelta(days=days)\n",
        "\n",
        "    req = GetOrdersRequest(\n",
        "        status=OrderStatus.ALL,   # get everything, then filter\n",
        "        after=after_dt,\n",
        "        nested=True\n",
        "    )\n",
        "\n",
        "    orders = api.get_orders(req)  #alpaca-py\n",
        "\n",
        "    sym = str(symbol).upper()\n",
        "    filled_statuses = {\"filled\", \"partially_filled\"}\n",
        "\n",
        "    return sum(\n",
        "        1 for o in (orders or [])\n",
        "        if str(getattr(o, \"symbol\", \"\")).upper() == sym\n",
        "        and str(getattr(o, \"status\", \"\")).lower() in filled_statuses\n",
        "    )\n",
        "\n",
        "try:\n",
        "    api_chk = api if \"api\" in globals() else init_alpaca()\n",
        "    for sym in tickers_to_report:\n",
        "        n = count_filled_orders_since(api_chk, sym, days=14)\n",
        "        print(f\"{sym}: {n} filled trades in last 14 days\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not fetch filled orders: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaKYNmKdOwOt"
      },
      "outputs": [],
      "source": [
        "#--- Export locally & download to your computer (Colab) ---\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "from google.colab import files   #<-- NEW: for browser download\n",
        "import shutil, time, pandas as pd\n",
        "\n",
        "#Drive root (same as before, to read your results)\n",
        "ROOT = Path(\"/content/drive/MyDrive/AlpacaPaper\")\n",
        "TODAY = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "#Original sources in Drive (unchanged)\n",
        "SRC_RESULTS = ROOT / \"results\" / TODAY         #e.g., /.../results/2025-10-13\n",
        "SRC_EXPORT  = ROOT / \"results_export\" / TODAY  #rescue export folder (if used)\n",
        "\n",
        "#=== CHANGE: write/export to LOCAL staging (in Colab VM), not Drive ===\n",
        "DEST = Path(\"/content\") / \"exports\" / f\"{TODAY}_export\"\n",
        "DEST.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def copy_all(src_dir, dest_dir):\n",
        "    if src_dir.exists():\n",
        "        for p in src_dir.glob(\"*\"):\n",
        "            if p.is_file():\n",
        "                shutil.copy2(p, dest_dir / p.name)\n",
        "                print(\"Copied:\", p.name, \"from\", src_dir.name)\n",
        "    else:\n",
        "        print(\"Missing source:\", src_dir)\n",
        "\n",
        "#Copy from both possible sources into local /content/exports/<today>_export\n",
        "copy_all(SRC_RESULTS, DEST)\n",
        "copy_all(SRC_EXPORT, DEST)\n",
        "rs_local = DEST / \"run_summary.csv\"\n",
        "print(\"Included run_summary.csv in export:\", rs_local.exists(), \"| path:\", rs_local)\n",
        "\n",
        "#Build/refresh trade_log_master.csv from per-symbol logs (in LOCAL DEST)\n",
        "sym_logs = list(DEST.glob(\"trade_log_*.csv\"))\n",
        "if sym_logs:\n",
        "    frames = []\n",
        "    for p in sym_logs:\n",
        "        try:\n",
        "            df = pd.read_csv(p)\n",
        "            df[\"symbol_file\"] = p.stem.replace(\"trade_log_\", \"\")\n",
        "            frames.append(df)\n",
        "        except Exception as e:\n",
        "            print(\"Skip\", p.name, \"->\", e)\n",
        "    if frames:\n",
        "        master = pd.concat(frames, ignore_index=True, sort=False)\n",
        "        master_path = DEST / \"trade_log_master.csv\"\n",
        "        master.to_csv(master_path, index=False)\n",
        "        print(\"Wrote:\", master_path)\n",
        "\n",
        "#Zip LOCALLY under /content and trigger a browser download\n",
        "zip_base = Path(\"/content\") / f\"results_{TODAY}_{int(time.time())}\"\n",
        "archive_path = shutil.make_archive(str(zip_base), \"zip\", DEST)\n",
        "archive_path = str(Path(archive_path))  #ensure string for files.download\n",
        "\n",
        "print(\"ZIP ->\", archive_path)\n",
        "\n",
        "#OPTIONAL: also keep a copy in Drive (uncomment if wanted)\n",
        "#shutil.copy2(archive_path, ROOT / \"results\" / Path(archive_path).name)\n",
        "\n",
        "#Prompt download to your computer\n",
        "files.download(archive_path)\n",
        "\n",
        "#Show what's in the LOCAL export folder\n",
        "print(\"\\nLocal export now contains:\")\n",
        "for p in sorted(DEST.iterdir()):\n",
        "    print(\" -\", p.name)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}