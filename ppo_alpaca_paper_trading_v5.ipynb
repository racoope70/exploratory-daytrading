{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/racoope70/exploratory-daytrading/blob/main/ppo_alpaca_paper_trading_v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-DEy5gEqqEi",
        "outputId": "d872dc78-9911-4f7b-9db4-b0a364904778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping stable-baselines3 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping shimmy as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: gymnasium 1.2.3\n",
            "Uninstalling gymnasium-1.2.3:\n",
            "  Successfully uninstalled gymnasium-1.2.3\n",
            "Found existing installation: gym 0.25.2\n",
            "Uninstalling gym-0.25.2:\n",
            "  Successfully uninstalled gym-0.25.2\n",
            "\u001b[33mWARNING: Skipping autorom as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping AutoROM.accept-rom-license as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: ale-py 0.11.2\n",
            "Uninstalling ale-py-0.11.2:\n",
            "  Successfully uninstalled ale-py-0.11.2\n",
            "Collecting gymnasium==0.29.1\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting shimmy==1.3.0\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting stable-baselines3==2.3.0\n",
            "  Downloading stable_baselines3-2.3.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (0.0.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.3.0) (2.9.0+cpu)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.3.0) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.3.0) (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (2025.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (26.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3==2.3.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3==2.3.0) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==2.3.0) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13->stable-baselines3==2.3.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13->stable-baselines3==2.3.0) (3.0.3)\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'stable-baselines3' candidate (version 2.3.0 at https://files.pythonhosted.org/packages/51/0b/6539076ed58343f1404dea0462167b079b5264508b8e5bbed01cea9f66b8/stable_baselines3-2.3.0-py3-none-any.whl (from https://pypi.org/simple/stable-baselines3/) (requires-python:>=3.8))\n",
            "Reason for being yanked: Loading broken with PyTorch 1.13\u001b[0m\u001b[33m\n",
            "\u001b[0mDownloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Downloading stable_baselines3-2.3.0-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.1/182.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gymnasium, shimmy, stable-baselines3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires ale-py>=0.10.1, which is not installed.\n",
            "dopamine-rl 4.1.2 requires gym<=0.25.2, which is not installed.\n",
            "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gymnasium-0.29.1 shimmy-1.3.0 stable-baselines3-2.3.0\n",
            "Collecting alpaca-trade-api\n",
            "  Downloading alpaca_trade_api-3.2.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Collecting gym-anytrading\n",
            "  Downloading gym_anytrading-2.0.0-py3-none-any.whl.metadata (292 bytes)\n",
            "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.0.2)\n",
            "Requirement already satisfied: requests<3,>2 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.32.4)\n",
            "Collecting urllib3<2,>1.24 (from alpaca-trade-api)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: websocket-client<2,>=0.56.0 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (1.9.0)\n",
            "Collecting websockets<11,>=9.0 (from alpaca-trade-api)\n",
            "  Downloading websockets-10.4.tar.gz (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting msgpack==1.0.3 (from alpaca-trade-api)\n",
            "  Downloading msgpack-1.0.3.tar.gz (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (3.13.3)\n",
            "Collecting PyYAML==6.0.1 (from alpaca-trade-api)\n",
            "  Downloading PyYAML-6.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: deprecation==2.1.0 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from deprecation==2.1.0->alpaca-trade-api) (26.0)\n",
            "Requirement already satisfied: gymnasium>=0.29.1 in /usr/local/lib/python3.12/dist-packages (from gym-anytrading) (0.29.1)\n",
            "Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.12/dist-packages (from gym-anytrading) (3.10.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.22.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=0.29.1->gym-anytrading) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=0.29.1->gym-anytrading) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=0.29.1->gym-anytrading) (0.0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2025.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>2->alpaca-trade-api) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>2->alpaca-trade-api) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>2->alpaca-trade-api) (2026.1.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.1->gym-anytrading) (1.17.0)\n",
            "Downloading alpaca_trade_api-3.2.0-py3-none-any.whl (34 kB)\n",
            "Downloading PyYAML-6.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (724 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m725.0/725.0 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gym_anytrading-2.0.0-py3-none-any.whl (172 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.2/172.2 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: msgpack, ta, websockets\n",
            "  Building wheel for msgpack (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for msgpack: filename=msgpack-1.0.3-cp312-cp312-linux_x86_64.whl size=15688 sha256=19ff530df2843503271370840e39b0e3940d0c5f9aaa6b3383db7379d6b0acab\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/bd/3f/f043e8f634db9c90ae128d631f43ae9990eef01274a63291f9\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=8713c02a67467d0f7e68f8fea3094e098b3120c3c3dc1d34a02603b3260874a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/a1/5f/c6b85a7d9452057be4ce68a8e45d77ba34234a6d46581777c6\n",
            "  Building wheel for websockets (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for websockets: filename=websockets-10.4-cp312-cp312-linux_x86_64.whl size=107327 sha256=066dddc543059a181bc115add02fe4c7661793dea85222045f5eec1f40153a1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/cf/6d/5d7e4c920cb41925a178b2d2621889c520d648bab487b1d7fd\n",
            "Successfully built msgpack ta websockets\n",
            "Installing collected packages: msgpack, websockets, urllib3, PyYAML, ta, gym-anytrading, alpaca-trade-api\n",
            "  Attempting uninstall: msgpack\n",
            "    Found existing installation: msgpack 1.1.2\n",
            "    Uninstalling msgpack-1.1.2:\n",
            "      Successfully uninstalled msgpack-1.1.2\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0.3\n",
            "    Uninstalling PyYAML-6.0.3:\n",
            "      Successfully uninstalled PyYAML-6.0.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 1.0.2 requires websockets>=14.0, but you have websockets 10.4 which is incompatible.\n",
            "yfinance 0.2.66 requires websockets>=13.0, but you have websockets 10.4 which is incompatible.\n",
            "gradio-client 1.14.0 requires websockets<16.0,>=13.0, but you have websockets 10.4 which is incompatible.\n",
            "google-adk 1.24.0 requires PyYAML<7.0.0,>=6.0.2, but you have pyyaml 6.0.1 which is incompatible.\n",
            "google-adk 1.24.0 requires websockets<16.0.0,>=15.0.1, but you have websockets 10.4 which is incompatible.\n",
            "google-genai 1.62.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyYAML-6.0.1 alpaca-trade-api-3.2.0 gym-anytrading-2.0.0 msgpack-1.0.3 ta-0.11.0 urllib3-1.26.20 websockets-10.4\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#Clean any partials\n",
        "!pip uninstall -y stable-baselines3 shimmy gymnasium gym autorom AutoROM.accept-rom-license ale-py\n",
        "\n",
        "#Install the compatible trio (no [extra] to avoid Atari deps)\n",
        "!pip install \"gymnasium==0.29.1\" \"shimmy==1.3.0\" \"stable-baselines3==2.3.0\"\n",
        "\n",
        "#Your other libs (safe to keep separate)\n",
        "!pip install alpaca-trade-api ta python-dotenv gym-anytrading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wyc0Dr0D86cA",
        "outputId": "ecdf1171-3f38-4175-e77d-be3d5bb78911"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch: 2.9.0+cpu\n",
            "gymnasium: 0.29.1\n",
            "shimmy: 1.3.0\n",
            "stable-baselines3: 2.3.0\n",
            "alpaca-trade-api: 3.2.0\n",
            "websockets: 10.4\n",
            "pywavelets: 1.8.0\n"
          ]
        }
      ],
      "source": [
        "import torch, gymnasium, shimmy, stable_baselines3 as sb3\n",
        "import alpaca_trade_api, websockets, pywt\n",
        "\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"gymnasium:\", gymnasium.__version__)\n",
        "print(\"shimmy:\", shimmy.__version__)\n",
        "print(\"stable-baselines3:\", sb3.__version__)\n",
        "print(\"alpaca-trade-api:\", alpaca_trade_api.__version__)\n",
        "print(\"websockets:\", websockets.__version__)\n",
        "print(\"pywavelets:\", pywt.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab: install into the current kernel\n",
        "!pip -q install alpaca-trade-api==3.2.0 python-dotenv ta gym-anytrading"
      ],
      "metadata": {
        "id": "mZ_pfdBTIZAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import alpaca_trade_api\n",
        "print(\"alpaca-trade-api:\", alpaca_trade_api.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lo8MIXvHIaou",
        "outputId": "4165274e-4199-4068-c84d-28c1c847a8ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alpaca-trade-api: 3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cM2YQVmcZjEu",
        "outputId": "24c47eaf-1089-478a-9c94-e1bb0163aa85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Upload your .env (or Alpaca_keys.env.txt). Cancel if already on Drive.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3c2a017f-d911-4246-926f-c970e02453bc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3c2a017f-d911-4246-926f-c970e02453bc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Alpaca_keys.env.txt to Alpaca_keys.env.txt\n",
            "Saved env → /content/drive/MyDrive/AlpacaPaper/.env\n",
            "Upload your artifacts (ppo_*_model.zip, *_vecnorm*.pkl, *_features*.json or .txt).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-691fd1ef-87dd-46e3-a56b-d45957a04ab9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-691fd1ef-87dd-46e3-a56b-d45957a04ab9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ppo_GE_window1_features.json to ppo_GE_window1_features.json\n",
            "Saving ppo_GE_window1_model_info.json to ppo_GE_window1_model_info.json\n",
            "Saving ppo_GE_window1_model.zip to ppo_GE_window1_model.zip\n",
            "Saving ppo_GE_window1_probability_config.json to ppo_GE_window1_probability_config.json\n",
            "Saving ppo_GE_window1_vecnorm.pkl to ppo_GE_window1_vecnorm.pkl\n",
            "Saving ppo_UNH_window3_features.json to ppo_UNH_window3_features.json\n",
            "Saving ppo_UNH_window3_model_info.json to ppo_UNH_window3_model_info.json\n",
            "Saving ppo_UNH_window3_model.zip to ppo_UNH_window3_model.zip\n",
            "Saving ppo_UNH_window3_probability_config.json to ppo_UNH_window3_probability_config.json\n",
            "Saving ppo_UNH_window3_vecnorm.pkl to ppo_UNH_window3_vecnorm.pkl\n",
            "Artifacts now in: ['ppo_CVX_window1_features.json', 'ppo_CVX_window1_model.zip', 'ppo_CVX_window1_model_info.json', 'ppo_CVX_window1_probability_config.json', 'ppo_CVX_window1_vecnorm.pkl', 'ppo_GE_window1_features.json', 'ppo_GE_window1_model.zip', 'ppo_GE_window1_model_info.json', 'ppo_GE_window1_probability_config.json', 'ppo_GE_window1_vecnorm.pkl', 'ppo_UNH_window3_features.json', 'ppo_UNH_window3_model.zip', 'ppo_UNH_window3_model_info.json', 'ppo_UNH_window3_probability_config.json', 'ppo_UNH_window3_vecnorm.pkl']\n",
            "2026-02-11 20:35:59,096 | INFO | EXIT_AFTER_CLOSE      : 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-11 20:35:59,767 | INFO | FORCE_FIRST_BUY       : True\n",
            "2026-02-11 20:35:59,787 | INFO | FORCE_FLATTEN_ON_EXIT : False\n",
            "2026-02-11 20:35:59,793 | INFO | CONFIG\n",
            "2026-02-11 20:35:59,796 | INFO | Project root          : /content/drive/MyDrive/AlpacaPaper\n",
            "2026-02-11 20:35:59,799 | INFO | ARTIFACTS_DIR         : /content/drive/MyDrive/AlpacaPaper/artifacts\n",
            "2026-02-11 20:35:59,802 | INFO | RESULTS_DIR           : /content/drive/MyDrive/AlpacaPaper/results/2026-02-11\n",
            "2026-02-11 20:35:59,803 | INFO | Tickers               : ['UNH', 'GE']\n",
            "2026-02-11 20:35:59,807 | INFO | API base              : https://paper-api.alpaca.markets\n",
            "2026-02-11 20:35:59,809 | INFO | AUTO_RUN_LIVE         : 1\n",
            "2026-02-11 20:35:59,811 | INFO | INF_DETERMINISTIC     : True\n",
            "2026-02-11 20:35:59,813 | INFO | ALLOW_SHORTS          : True\n",
            "2026-02-11 20:35:59,814 | INFO | FLATTEN_INTO_CLOSE    : False\n",
            "2026-02-11 20:35:59,817 | INFO | REENTRY_COOLDOWN_SEC  : 300\n",
            "2026-02-11 20:35:59,819 | INFO | DRY_RUN=False | BARS_FEED= | USE_FRACTIONALS=True | COOLDOWN_MIN=10 | STALE_MAX_SEC=4200\n",
            "2026-02-11 20:35:59,821 | INFO | DEBUG_FORCE_SEED_IF_IDLE=0 | DEBUG_SEED_IDLE_CYCLES=10\n",
            "2026-02-11 20:35:59,823 | INFO | PH_TIMEOUT_SEC        : 8\n",
            "2026-02-11 20:35:59,824 | INFO | DATA_TIMEFRAME        : 1H (model bars)\n",
            "2026-02-11 20:35:59,825 | INFO | EQUITY_TIMEFRAME      : 5Min (equity reporting)\n",
            "2026-02-11 20:35:59,826 | INFO | MAX_DD_PCT: 0.050 | KILL_SWITCH_COOLDOWN_MIN: 30\n",
            "2026-02-11 20:35:59,827 | INFO | WEIGHT_CAP: 0.400 | SIZING_MODE: linear | ENTER_CONF_MIN: 0.020 | ENTER_WEIGHT_MIN: 0.002 | EXIT_WEIGHT_MAX: 0.001 | REBALANCE_MIN_NOTIONAL: 5.00\n",
            "2026-02-11 20:35:59,831 | INFO | TAKE_PROFIT_PCT: 0.050 | STOP_LOSS_PCT: 0.020 | BEST_WINDOW_ENV: \n",
            "2026-02-11 20:35:59,834 | INFO | DELTA_WEIGHT_MIN: 0.001 | RAW_POS_MIN: 0.000 | RAW_NEG_MAX: 0.000\n",
            "2026-02-11 20:35:59,836 | INFO | Artifacts present (15): ppo_CVX_window1_features.json, ppo_CVX_window1_model.zip, ppo_CVX_window1_model_info.json, ppo_CVX_window1_probability_config.json, ppo_CVX_window1_vecnorm.pkl, ppo_GE_window1_features.json, ppo_GE_window1_model.zip, ppo_GE_window1_model_info.json, ppo_GE_window1_probability_config.json, ppo_GE_window1_vecnorm.pkl, ppo_UNH_window3_features.json, ppo_UNH_window3_model.zip, ppo_UNH_window3_model_info.json, ppo_UNH_window3_probability_config.json, ppo_UNH_window3_vecnorm.pkl\n",
            "2026-02-11 20:35:59,837 | INFO | DATA_TIMEFRAME=1H -> LIVE_TIMEFRAME=1Hour\n",
            "2026-02-11 20:36:00,158 | INFO | shorting_enabled=True\n",
            "2026-02-11 20:36:00,165 | INFO | Account status: ACTIVE | equity=99350.48 | cash=91747.96\n",
            "2026-02-11 20:36:00,295 | INFO | [UNH] model=ppo_UNH_window3_model.zip | window=3 | vecnorm=ppo_UNH_window3_vecnorm.pkl | features=ppo_UNH_window3_features.json\n",
            "2026-02-11 20:36:07,959 | INFO | [UNH] Artifacts loaded and ready.\n",
            "2026-02-11 20:36:07,968 | INFO | [GE] model=ppo_GE_window1_model.zip | window=1 | vecnorm=ppo_GE_window1_vecnorm.pkl | features=ppo_GE_window1_features.json\n",
            "2026-02-11 20:36:07,994 | INFO | [GE] Artifacts loaded and ready.\n",
            "2026-02-11 20:36:07,996 | INFO | Starting live execution for (loaded): ['UNH', 'GE']\n",
            "2026-02-11 20:36:08,000 | INFO | Starting live trading loop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-11 20:36:08,192 | INFO | Session open equity anchor set: 99350.61\n",
            "[HEARTBEAT] 2026-02-11T20:36:08.290127+00:00 cycle=0 equity=99,350.61\n",
            "2026-02-11 20:36:08,291 | INFO | [UNH] fetching 200 1Hour bars (feed='default')\n",
            "2026-02-11 20:36:08,688 | INFO | [UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=2168s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "2026-02-11 20:36:08,690 | INFO | [UNH] shape_check obs=(10, 2) expected=(10, 2)\n",
            "2026-02-11 20:36:08,692 | INFO | [UNH]  obs built. Calling model.predict()\n",
            "2026-02-11 20:36:08,789 | INFO | [UNH] obs stats raw: mean=275.6562 std=2.6921 | normed: mean=2.2573 std=7.7431\n",
            "2026-02-11 20:36:08,828 | INFO | [UNH] predict() ok → raw=-0.4568 target_w=-0.1827 conf=0.457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-11 20:36:08,926 | INFO | [UNH] raw=-0.4568 conf=0.457 → target_w=-0.1827 px=$278.00 eq=$99,350.61 have=0.017910469\n",
            "2026-02-11 20:36:09,331 | INFO | [UNH] Flip long→short requested. Flattening long first (have_qty=0.017910469).\n",
            "2026-02-11 20:36:09,450 | INFO | [UNH] close_position submitted.\n",
            "2026-02-11 20:36:11,476 | INFO | [TIMER] UNH symbol work: 3.185s\n",
            "2026-02-11 20:36:11,477 | INFO | [GE] fetching 200 1Hour bars (feed='default')\n",
            "2026-02-11 20:36:11,604 | INFO | [GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=2171s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "2026-02-11 20:36:11,606 | INFO | [GE] shape_check obs=(10, 2) expected=(10, 2)\n",
            "2026-02-11 20:36:11,607 | INFO | [GE]  obs built. Calling model.predict()\n",
            "2026-02-11 20:36:11,704 | INFO | [GE] obs stats raw: mean=315.8940 std=2.1398 | normed: mean=7.5455 std=2.4549\n",
            "2026-02-11 20:36:11,707 | INFO | [GE] predict() ok → raw=0.1911 target_w=0.0765 conf=0.191\n",
            "2026-02-11 20:36:11,803 | INFO | [GE] raw=0.1911 conf=0.191 → target_w=0.0765 px=$314.52 eq=$99,350.61 have=24.155996526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-11 20:36:12,876 | INFO | [TIMER] GE symbol work: 1.399s\n",
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2026-02-11/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n",
            "2026-02-11 20:36:16,146 | INFO | Perf: cum_return=0.01% | sharpe=42.62 | maxDD=-0.00%\n",
            "2026-02-11 20:36:16,149 | INFO | [TIMER] full-cycle active time: 7.955s (cooldown=10 min)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HEARTBEAT] 2026-02-11T20:40:00.393206+00:00 cycle=1 equity=99,346.38\n",
            "2026-02-11 20:40:00,394 | INFO | [UNH] fetching 200 1Hour bars (feed='default')\n",
            "2026-02-11 20:40:00,731 | INFO | [UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=2400s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "2026-02-11 20:40:00,733 | INFO | [UNH] shape_check obs=(10, 2) expected=(10, 2)\n",
            "2026-02-11 20:40:00,735 | INFO | [UNH]  obs built. Calling model.predict()\n",
            "2026-02-11 20:40:00,831 | INFO | [UNH] obs stats raw: mean=275.6814 std=2.7150 | normed: mean=2.2576 std=7.7428\n",
            "2026-02-11 20:40:00,834 | INFO | [UNH] predict() ok → raw=-0.4568 target_w=-0.1827 conf=0.457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-11 20:40:01,129 | INFO | [UNH] Submitted buy notional=$5.00\n",
            "2026-02-11 20:40:01,169 | INFO | [TIMER] UNH symbol work: 0.775s\n",
            "2026-02-11 20:40:01,172 | INFO | [GE] fetching 200 1Hour bars (feed='default')\n",
            "2026-02-11 20:40:01,299 | INFO | [GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=2401s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "2026-02-11 20:40:01,301 | INFO | [GE] shape_check obs=(10, 2) expected=(10, 2)\n",
            "2026-02-11 20:40:01,304 | INFO | [GE]  obs built. Calling model.predict()\n",
            "2026-02-11 20:40:01,401 | INFO | [GE] obs stats raw: mean=315.8770 std=2.1513 | normed: mean=7.5453 std=2.4551\n",
            "2026-02-11 20:40:01,403 | INFO | [GE] predict() ok → raw=0.1912 target_w=0.0765 conf=0.191\n",
            "2026-02-11 20:40:01,500 | INFO | [GE] raw=0.1912 conf=0.191 → target_w=0.0765 px=$314.35 eq=$99,346.38 have=24.155996526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-11 20:40:01,817 | INFO | [TIMER] GE symbol work: 0.646s\n",
            "2026-02-11 20:40:02,169 | INFO | [TIMER] full-cycle active time: 1.878s (cooldown=10 min)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HEARTBEAT] 2026-02-11T20:50:00.397539+00:00 cycle=2 equity=99,357.98\n",
            "2026-02-11 20:50:00,399 | INFO | [UNH] fetching 200 1Hour bars (feed='default')\n",
            "2026-02-11 20:50:00,724 | INFO | [UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=3000s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "2026-02-11 20:50:00,726 | INFO | [UNH] shape_check obs=(10, 2) expected=(10, 2)\n",
            "2026-02-11 20:50:00,727 | INFO | [UNH]  obs built. Calling model.predict()\n",
            "2026-02-11 20:50:00,823 | INFO | [UNH] obs stats raw: mean=275.7253 std=2.7595 | normed: mean=2.2581 std=7.7423\n",
            "2026-02-11 20:50:00,826 | INFO | [UNH] predict() ok → raw=-0.4567 target_w=-0.1827 conf=0.457\n",
            "2026-02-11 20:50:00,923 | INFO | [UNH] raw=-0.4567 conf=0.457 → target_w=-0.1827 px=$278.69 eq=$99,357.98 have=0.017934157\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-11 20:50:01,217 | INFO | [UNH] Flip long→short requested. Flattening long first (have_qty=0.017934157).\n",
            "2026-02-11 20:50:01,315 | INFO | [UNH] close_position submitted.\n",
            "2026-02-11 20:50:01,350 | INFO | [TIMER] UNH symbol work: 0.951s\n",
            "2026-02-11 20:50:01,351 | INFO | [GE] fetching 200 1Hour bars (feed='default')\n",
            "2026-02-11 20:50:01,489 | INFO | [GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=3001s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "2026-02-11 20:50:01,490 | INFO | [GE] shape_check obs=(10, 2) expected=(10, 2)\n",
            "2026-02-11 20:50:01,491 | INFO | [GE]  obs built. Calling model.predict()\n",
            "2026-02-11 20:50:01,592 | INFO | [GE] obs stats raw: mean=315.9250 std=2.1219 | normed: mean=7.5459 std=2.4545\n",
            "2026-02-11 20:50:01,595 | INFO | [GE] predict() ok → raw=0.1911 target_w=0.0764 conf=0.191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-11 20:50:01,694 | INFO | [GE] raw=0.1911 conf=0.191 → target_w=0.0764 px=$314.83 eq=$99,357.98 have=24.155996526\n",
            "2026-02-11 20:50:02,035 | INFO | [TIMER] GE symbol work: 0.684s\n",
            "2026-02-11 20:50:02,432 | INFO | [TIMER] full-cycle active time: 2.133s (cooldown=10 min)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-11 21:00:00,382 | INFO | Market closed. Sleeping 62999s until next open.\n",
            "2026-02-11 21:00:08,775 | INFO | KeyboardInterrupt: stopping live loop.\n",
            "2026-02-11 21:00:08,778 | INFO | Timeout executor reset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7ff8da831940>\n",
            "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2026-02-11/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n",
            "2026-02-11 21:00:09,368 | INFO | Live loop exited cleanly.\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "import csv\n",
        "import sys\n",
        "import gc\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "import shutil\n",
        "import time\n",
        "import warnings\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from decimal import Decimal, ROUND_DOWN, ROUND_HALF_UP\n",
        "from functools import lru_cache\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Mapping, Optional, Tuple, Union\n",
        "\n",
        "# Scientific / data stack\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "# Fix seed for reusability\n",
        "import random\n",
        "import torch\n",
        "\n",
        "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Alpaca trading API\n",
        "import alpaca_trade_api as tradeapi\n",
        "from alpaca_trade_api.rest import TimeFrame\n",
        "\n",
        "# Try to use TimeFrameUnit if your alpaca_trade_api version has it\n",
        "try:\n",
        "    from alpaca_trade_api.rest import TimeFrameUnit  # type: ignore\n",
        "    _TF_MAP = {\n",
        "        \"1min\": TimeFrame.Minute,\n",
        "        \"1minute\": TimeFrame.Minute,\n",
        "        \"5min\": TimeFrame(5, TimeFrameUnit.Minute),\n",
        "        \"15min\": TimeFrame(15, TimeFrameUnit.Minute),\n",
        "        \"1h\": TimeFrame.Hour,\n",
        "        \"1hour\": TimeFrame.Hour,\n",
        "        \"60min\": TimeFrame.Hour,\n",
        "        \"1d\": TimeFrame.Day,\n",
        "        \"day\": TimeFrame.Day,\n",
        "    }\n",
        "except Exception:\n",
        "    # alpaca_trade_api supports timeframe strings like \"5Min\", \"15Min\", etc.\n",
        "    _TF_MAP = {\n",
        "        \"1min\": \"1Min\",\n",
        "        \"1minute\": \"1Min\",\n",
        "        \"5min\": \"5Min\",\n",
        "        \"15min\": \"15Min\",\n",
        "        \"1h\": \"1Hour\",     # (if your env doesn't accept this, use \"60Min\")\n",
        "        \"1hour\": \"1Hour\",\n",
        "        \"60min\": \"60Min\",\n",
        "        \"1d\": \"1Day\",\n",
        "        \"day\": \"1Day\",\n",
        "    }\n",
        "\n",
        "LIVE_TIMEFRAME = \"1Hour\"  # placeholder; overwritten in __main__\n",
        "\n",
        "# RL models\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import VecNormalize\n",
        "\n",
        "# (Optional) Colab helpers\n",
        "IN_COLAB = False\n",
        "try:\n",
        "    import google.colab  # type: ignore\n",
        "    from google.colab import drive, files  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# ============================================================\n",
        "# Utils / Paths / Global cooldowns\n",
        "# ============================================================\n",
        "\n",
        "def round_to_cents(x: float) -> float:\n",
        "    return float(Decimal(str(x)).quantize(Decimal(\"0.01\"), rounding=ROUND_DOWN))\n",
        "\n",
        "if IN_COLAB:\n",
        "    try:\n",
        "        drive.mount(\"/content/drive\", force_remount=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# Project root (Drive in Colab; cwd locally)\n",
        "if IN_COLAB:\n",
        "    PROJECT_ROOT = Path(\"/content/drive/MyDrive/AlpacaPaper\")\n",
        "else:\n",
        "    PROJECT_ROOT = Path.cwd() / \"AlpacaPaper\"\n",
        "PROJECT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Order throttling timestamps (per symbol)\n",
        "_ORDER_EVENT_TS: Dict[str, float] = {}\n",
        "_LAST_ORDER_TS: Dict[str, float] = {}\n",
        "\n",
        "# One-time seed-buy flags (per symbol) for FORCE_FIRST_BUY\n",
        "_FORCED_FIRST_BUY_DONE: Dict[str, bool] = {}\n",
        "\n",
        "def begin_order_event(symbol: str, min_gap_sec: int) -> bool:\n",
        "    now = time.time()\n",
        "    last = _ORDER_EVENT_TS.get(symbol, 0.0)\n",
        "    if (now - last) < float(min_gap_sec):\n",
        "        return False\n",
        "    _ORDER_EVENT_TS[symbol] = now\n",
        "    return True\n",
        "\n",
        "def stamp_order_event(symbol: str) -> None:\n",
        "    ts = time.time()\n",
        "    _ORDER_EVENT_TS[symbol] = ts\n",
        "    _LAST_ORDER_TS[symbol] = ts\n",
        "\n",
        "SESSION_OPEN_EQUITY: Optional[float] = None\n",
        "_last_kill_ts: float = 0.0\n",
        "\n",
        "# Faster cooldown for the very first seed fill\n",
        "_SEED_COOLDOWN_SEC = 10\n",
        "\n",
        "_NO_POS_CYCLE_COUNT: Dict[str, int] = {}\n",
        "\n",
        "# Re-entry cooldown after forced flatten\n",
        "_REENTRY_BLOCK_UNTIL: Dict[str, float] = {}\n",
        "REENTRY_COOLDOWN_SEC = int(os.getenv(\"REENTRY_COOLDOWN_SEC\", \"300\"))\n",
        "\n",
        "def _too_soon(symbol: str, min_gap_sec: int = 30) -> bool:\n",
        "    now = time.time()\n",
        "    last = _LAST_ORDER_TS.get(symbol, 0.0)\n",
        "    if (now - last) < float(min_gap_sec):\n",
        "        return True\n",
        "    _LAST_ORDER_TS[symbol] = now\n",
        "    return False\n",
        "\n",
        "def _to_bool(x: str) -> bool:\n",
        "    return str(x).strip().lower() in (\"1\", \"true\", \"yes\", \"y\", \"on\")\n",
        "\n",
        "def env_bool(name: str, default: str = \"0\") -> bool:\n",
        "    return _to_bool(os.getenv(name, default))\n",
        "\n",
        "# ============================================================\n",
        "# Upload / Conversion Helpers (Colab)\n",
        "# ============================================================\n",
        "\n",
        "def can_short_symbol(api, symbol: str) -> bool:\n",
        "    if not bool(globals().get(\"ALLOW_SHORTS\", False)):\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        acct = api.get_account()\n",
        "        shorting_ok = bool(getattr(acct, \"shorting_enabled\", False))\n",
        "    except Exception:\n",
        "        shorting_ok = False\n",
        "\n",
        "    try:\n",
        "        asset = api.get_asset(symbol)\n",
        "        asset_ok = bool(getattr(asset, \"shortable\", False))\n",
        "    except Exception:\n",
        "        asset_ok = False\n",
        "\n",
        "    return shorting_ok and asset_ok\n",
        "\n",
        "def upload_env_and_artifacts_in_colab():\n",
        "    if not IN_COLAB:\n",
        "        return\n",
        "\n",
        "    target_dir = Path(os.getenv(\"ARTIFACTS_DIR\", str(PROJECT_ROOT / \"artifacts\")))\n",
        "    target_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(\"Upload your .env (or Alpaca_keys.env.txt). Cancel if already on Drive.\")\n",
        "    up = files.upload()\n",
        "    if up:\n",
        "        if \"Alpaca_keys.env.txt\" in up:\n",
        "            src = Path(\"Alpaca_keys.env.txt\")\n",
        "            dst = PROJECT_ROOT / \".env\"\n",
        "            shutil.move(str(src), str(dst))\n",
        "            print(f\"Saved env → {dst}\")\n",
        "        elif \".env\" in up:\n",
        "            src = Path(\".env\")\n",
        "            dst = PROJECT_ROOT / \".env\"\n",
        "            shutil.move(str(src), str(dst))\n",
        "            print(f\"Saved env → {dst}\")\n",
        "        else:\n",
        "            any_name = next(iter(up.keys()))\n",
        "            src = Path(any_name)\n",
        "            dst = PROJECT_ROOT / \".env\"\n",
        "            shutil.move(str(src), str(dst))\n",
        "            print(f\"Saved env (renamed {any_name}) → {dst}\")\n",
        "\n",
        "    print(\"Upload your artifacts (ppo_*_model.zip, *_vecnorm*.pkl, *_features*.json or .txt).\")\n",
        "    up2 = files.upload()\n",
        "    for name in up2.keys():\n",
        "        shutil.move(name, target_dir / name)\n",
        "    print(\"Artifacts now in:\", sorted(p.name for p in target_dir.iterdir()))\n",
        "\n",
        "def _maybe_convert_features_txt_to_json():\n",
        "    \"\"\"Convert any 'features_<TICKER>.txt' into 'ppo_<TICKER>_features.json' (simple list).\"\"\"\n",
        "    art_dir = Path(os.getenv(\"ARTIFACTS_DIR\", str(PROJECT_ROOT / \"artifacts\")))\n",
        "    art_dir.mkdir(parents=True, exist_ok=True)\n",
        "    for p in art_dir.glob(\"features_*.txt\"):\n",
        "        ticker = re.sub(r\"^features_|\\.txt$\", \"\", p.name, flags=re.IGNORECASE)\n",
        "        try:\n",
        "            raw = p.read_text().strip()\n",
        "            items = [x.strip() for x in raw.replace(\",\", \"\\n\").splitlines() if x.strip()]\n",
        "            out = {\"features\": items}\n",
        "            out_path = art_dir / f\"ppo_{ticker}_features.json\"\n",
        "            out_path.write_text(json.dumps(out, indent=2))\n",
        "            print(f\"Converted {p.name} → {out_path.name}  ({len(items)} features)\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not convert {p.name}: {e}\")\n",
        "\n",
        "def _maybe_rename_vecnorm_scaler():\n",
        "    \"\"\"Rename any 'scaler_<TICKER>.pkl' to 'ppo_<TICKER>_vecnorm.pkl'.\"\"\"\n",
        "    art_dir = Path(os.getenv(\"ARTIFACTS_DIR\", str(PROJECT_ROOT / \"artifacts\")))\n",
        "    art_dir.mkdir(parents=True, exist_ok=True)\n",
        "    for p in art_dir.glob(\"scaler_*.pkl\"):\n",
        "        ticker = re.sub(r\"^scaler_|\\.pkl$\", \"\", p.name, flags=re.IGNORECASE)\n",
        "        dst = art_dir / f\"ppo_{ticker}_vecnorm.pkl\"\n",
        "        if not dst.exists():\n",
        "            shutil.move(str(p), str(dst))\n",
        "            print(f\"Renamed {p.name} → {dst.name}\")\n",
        "\n",
        "def normalize_artifacts():\n",
        "    _maybe_convert_features_txt_to_json()\n",
        "    _maybe_rename_vecnorm_scaler()\n",
        "\n",
        "# ============================================================\n",
        "# Env & logging (initial) + .env loading\n",
        "# ============================================================\n",
        "\n",
        "warnings.filterwarnings(\"default\")\n",
        "\n",
        "# Load env (supports PROJECT_ROOT/.env)\n",
        "env_candidates = [PROJECT_ROOT / \".env\", Path(\".env\")]\n",
        "for env_path in env_candidates:\n",
        "    if env_path.exists():\n",
        "        load_dotenv(dotenv_path=env_path, override=True)\n",
        "        break\n",
        "else:\n",
        "    load_dotenv(override=True)\n",
        "\n",
        "# ---- Re-read env booleans AFTER .env is loaded ----\n",
        "# Keep your existing reentry env read\n",
        "REENTRY_COOLDOWN_SEC = int(os.getenv(\"REENTRY_COOLDOWN_SEC\", \"300\"))\n",
        "\n",
        "# Default timeouts / reporting cadence\n",
        "os.environ.setdefault(\"PH_TIMEOUT_SEC\", \"8\")\n",
        "os.environ.setdefault(\"EQUITY_TIMEFRAME\", \"5Min\")\n",
        "\n",
        "# Debug idle-seed knobs\n",
        "os.environ.setdefault(\"DEBUG_FORCE_SEED_IF_IDLE\", \"0\")\n",
        "os.environ.setdefault(\"DEBUG_SEED_IDLE_CYCLES\", \"10\")\n",
        "\n",
        "# Basic console logger (replaced with file+console after RESULTS_DIR exists)\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
        "logging.getLogger().setLevel(getattr(logging, os.getenv(\"LOG_LEVEL\", \"INFO\").upper(), logging.INFO))\n",
        "\n",
        "root = logging.getLogger()\n",
        "root.handlers.clear()\n",
        "\n",
        "handler = logging.StreamHandler(sys.stdout)\n",
        "handler.setLevel(getattr(logging, os.getenv(\"LOG_LEVEL\", \"INFO\").upper(), logging.INFO))\n",
        "handler.setFormatter(logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\"))\n",
        "root.addHandler(handler)\n",
        "root.setLevel(handler.level)\n",
        "\n",
        "try:\n",
        "    sys.stdout.reconfigure(line_buffering=True)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ============================================================\n",
        "# Config dataclass\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "def _to_list_csv(x: str) -> list:\n",
        "    return [s.strip().upper() for s in str(x).split(\",\") if s.strip()]\n",
        "\n",
        "@dataclass\n",
        "class Knobs:\n",
        "    # API / mode\n",
        "    APCA_API_BASE_URL: str = \"https://paper-api.alpaca.markets\"\n",
        "    DRY_RUN: bool = False\n",
        "    AUTO_RUN_LIVE: bool = True\n",
        "    INF_DETERMINISTIC: bool = True\n",
        "\n",
        "    FLATTEN_INTO_CLOSE: bool = False\n",
        "    FORCE_FIRST_BUY: bool = False\n",
        "    FORCE_FLATTEN_ON_EXIT: bool = False\n",
        "\n",
        "    # Equity logging controls\n",
        "    EQUITY_LOG_THROTTLE_SEC: int = 900\n",
        "    SKIP_EQUITY_WHEN_DRY_RUN: bool = True\n",
        "\n",
        "    # Universe / files\n",
        "    TICKERS: List[str] = field(default_factory=list)\n",
        "    ARTIFACTS_DIR: str = \"\"\n",
        "    RESULTS_ROOT: str = \"\"\n",
        "\n",
        "    # Data feed / cadence / staleness\n",
        "    BARS_FEED: str = \"iex\"\n",
        "    COOLDOWN_MIN: int = 10\n",
        "    STALE_MAX_SEC: int = 4200\n",
        "\n",
        "    # Sizing & entry/exit sensitivity\n",
        "    SIZING_MODE: str = \"threshold\"  # \"linear\" | \"threshold\"\n",
        "    WEIGHT_CAP: float = 0.35\n",
        "    CONF_FLOOR: float = 0.15\n",
        "    ENTER_CONF_MIN: float = 0.12\n",
        "    ENTER_WEIGHT_MIN: float = 0.02\n",
        "    EXIT_WEIGHT_MAX: float = 0.008\n",
        "    REBALANCE_MIN_NOTIONAL: float = 10.00\n",
        "    USE_FRACTIONALS: bool = True\n",
        "    SEED_FIRST_SHARE: bool = True\n",
        "    ALLOW_SHORTS: bool = False\n",
        "\n",
        "    # add-ons\n",
        "    DELTA_WEIGHT_MIN: float = 0.002\n",
        "    RAW_POS_MIN: float = 0.00\n",
        "    RAW_NEG_MAX: float = 0.00\n",
        "\n",
        "    # Risk\n",
        "    TAKE_PROFIT_PCT: float = 0.05\n",
        "    STOP_LOSS_PCT: float = 0.03\n",
        "\n",
        "    # Timeframe intent\n",
        "    TRAIN_TIMEFRAME: str = \"1H\"\n",
        "    DATA_TIMEFRAME: str = \"1H\"\n",
        "    EQUITY_TIMEFRAME: str = \"5Min\"\n",
        "\n",
        "    # Kill-switch\n",
        "    MAX_DAILY_DRAWDOWN_PCT: float = 0.05\n",
        "    KILL_SWITCH_COOLDOWN_MIN: int = 30\n",
        "\n",
        "    # Exit policy\n",
        "    EXIT_AFTER_CLOSE: bool = False\n",
        "\n",
        "    # Secrets\n",
        "    APCA_API_KEY_ID: str = \"\"\n",
        "    APCA_API_SECRET_KEY: str = \"\"\n",
        "\n",
        "    # Misc\n",
        "    STALE_BEST_WINDOW: str = \"\"\n",
        "\n",
        "    @classmethod\n",
        "    def from_env(\n",
        "        cls,\n",
        "        defaults: \"Knobs\",\n",
        "        project_root: Path,\n",
        "        env: Mapping[str, str],\n",
        "        overrides: Mapping[str, object] = None,\n",
        "    ):\n",
        "        kv = {**defaults.__dict__}\n",
        "        kv.update({\n",
        "          \"APCA_API_BASE_URL\": env.get(\"APCA_API_BASE_URL\", kv[\"APCA_API_BASE_URL\"]),\n",
        "          \"AUTO_RUN_LIVE\":     _to_bool(env.get(\"AUTO_RUN_LIVE\", str(kv[\"AUTO_RUN_LIVE\"]))),\n",
        "          \"DRY_RUN\":           _to_bool(env.get(\"DRY_RUN\", str(kv[\"DRY_RUN\"]))),\n",
        "          \"INF_DETERMINISTIC\": _to_bool(env.get(\"INF_DETERMINISTIC\", str(kv[\"INF_DETERMINISTIC\"]))),\n",
        "\n",
        "          # --- execution policy flags (only once) ---\n",
        "          \"FLATTEN_INTO_CLOSE\": _to_bool(env.get(\"FLATTEN_INTO_CLOSE\", str(kv.get(\"FLATTEN_INTO_CLOSE\", False)))),\n",
        "          \"FORCE_FIRST_BUY\": _to_bool(env.get(\"FORCE_FIRST_BUY\", str(kv.get(\"FORCE_FIRST_BUY\", False)))),\n",
        "          \"FORCE_FLATTEN_ON_EXIT\": _to_bool(env.get(\"FORCE_FLATTEN_ON_EXIT\", str(kv.get(\"FORCE_FLATTEN_ON_EXIT\", False)))),\n",
        "\n",
        "          \"EQUITY_LOG_THROTTLE_SEC\": int(env.get(\"EQUITY_LOG_THROTTLE_SEC\", str(kv[\"EQUITY_LOG_THROTTLE_SEC\"]))),\n",
        "          \"SKIP_EQUITY_WHEN_DRY_RUN\": _to_bool(env.get(\"SKIP_EQUITY_WHEN_DRY_RUN\", str(kv[\"SKIP_EQUITY_WHEN_DRY_RUN\"]))),\n",
        "\n",
        "          \"USE_FRACTIONALS\": _to_bool(env.get(\"USE_FRACTIONALS\", str(kv[\"USE_FRACTIONALS\"]))),\n",
        "          \"SEED_FIRST_SHARE\": _to_bool(env.get(\"SEED_FIRST_SHARE\", str(kv[\"SEED_FIRST_SHARE\"]))),\n",
        "          \"ALLOW_SHORTS\": _to_bool(env.get(\"ALLOW_SHORTS\", str(kv[\"ALLOW_SHORTS\"]))),\n",
        "\n",
        "          \"TICKERS\": _to_list_csv(env.get(\"TICKERS\", \",\".join(kv[\"TICKERS\"] or [\"UNH\", \"GE\"]))),\n",
        "          \"ARTIFACTS_DIR\": env.get(\"ARTIFACTS_DIR\", kv[\"ARTIFACTS_DIR\"] or str(project_root / \"artifacts\")),\n",
        "          \"RESULTS_ROOT\": env.get(\"RESULTS_ROOT\", kv[\"RESULTS_ROOT\"] or str(project_root / \"results\")),\n",
        "\n",
        "          \"BARS_FEED\": env.get(\"BARS_FEED\", kv[\"BARS_FEED\"]),\n",
        "          \"COOLDOWN_MIN\": int(env.get(\"COOLDOWN_MIN\", str(kv[\"COOLDOWN_MIN\"])) or kv[\"COOLDOWN_MIN\"]),\n",
        "          \"STALE_MAX_SEC\": int(env.get(\"STALE_MAX_SEC\", str(kv[\"STALE_MAX_SEC\"])) or kv[\"STALE_MAX_SEC\"]),\n",
        "\n",
        "          \"SIZING_MODE\": env.get(\"SIZING_MODE\", kv[\"SIZING_MODE\"]),\n",
        "          \"WEIGHT_CAP\": float(env.get(\"WEIGHT_CAP\", str(kv[\"WEIGHT_CAP\"]))),\n",
        "          \"CONF_FLOOR\": float(env.get(\"CONF_FLOOR\", str(kv[\"CONF_FLOOR\"]))),\n",
        "          \"ENTER_CONF_MIN\": float(env.get(\"ENTER_CONF_MIN\", str(kv[\"ENTER_CONF_MIN\"]))),\n",
        "          \"ENTER_WEIGHT_MIN\": float(env.get(\"ENTER_WEIGHT_MIN\", str(kv[\"ENTER_WEIGHT_MIN\"]))),\n",
        "          \"EXIT_WEIGHT_MAX\": float(env.get(\"EXIT_WEIGHT_MAX\", str(kv[\"EXIT_WEIGHT_MAX\"]))),\n",
        "          \"REBALANCE_MIN_NOTIONAL\": float(env.get(\"REBALANCE_MIN_NOTIONAL\", str(kv[\"REBALANCE_MIN_NOTIONAL\"]))),\n",
        "\n",
        "          \"TAKE_PROFIT_PCT\": float(env.get(\"TAKE_PROFIT_PCT\", str(kv[\"TAKE_PROFIT_PCT\"]))),\n",
        "          \"STOP_LOSS_PCT\": float(env.get(\"STOP_LOSS_PCT\", str(kv[\"STOP_LOSS_PCT\"]))),\n",
        "\n",
        "          \"DELTA_WEIGHT_MIN\": float(env.get(\"DELTA_WEIGHT_MIN\", str(kv.get(\"DELTA_WEIGHT_MIN\", 0.002)))),\n",
        "          \"RAW_POS_MIN\": float(env.get(\"RAW_POS_MIN\", str(kv.get(\"RAW_POS_MIN\", 0.0)))),\n",
        "          \"RAW_NEG_MAX\": float(env.get(\"RAW_NEG_MAX\", str(kv.get(\"RAW_NEG_MAX\", 0.0)))),\n",
        "\n",
        "          \"EXIT_AFTER_CLOSE\": _to_bool(env.get(\"EXIT_AFTER_CLOSE\", str(kv.get(\"EXIT_AFTER_CLOSE\", False)))),\n",
        "\n",
        "          \"STALE_BEST_WINDOW\": env.get(\"STALE_BEST_WINDOW\", kv.get(\"STALE_BEST_WINDOW\", \"\")),\n",
        "          \"DATA_TIMEFRAME\": env.get(\"DATA_TIMEFRAME\", kv.get(\"DATA_TIMEFRAME\", \"1H\")),\n",
        "          \"TRAIN_TIMEFRAME\": env.get(\"TRAIN_TIMEFRAME\", kv.get(\"TRAIN_TIMEFRAME\", \"1H\")),\n",
        "          \"EQUITY_TIMEFRAME\": env.get(\"EQUITY_TIMEFRAME\", kv.get(\"EQUITY_TIMEFRAME\", \"5Min\")),\n",
        "      })\n",
        "\n",
        "        kv[\"APCA_API_KEY_ID\"] = env.get(\"APCA_API_KEY_ID\") or env.get(\"ALPACA_API_KEY_ID\", \"\") or \"\"\n",
        "        kv[\"APCA_API_SECRET_KEY\"] = env.get(\"APCA_API_SECRET_KEY\") or env.get(\"ALPACA_API_SECRET_KEY\", \"\") or \"\"\n",
        "\n",
        "        if overrides:\n",
        "            for k, v in overrides.items():\n",
        "                key = str(k)\n",
        "                if key.upper() == \"TICKERS\" and isinstance(v, str):\n",
        "                    v = _to_list_csv(v)\n",
        "                kv[key] = v\n",
        "\n",
        "        return cls(**kv)\n",
        "\n",
        "    def apply_to_globals(self):\n",
        "        g = globals()\n",
        "        g[\"BASE_URL\"] = self.APCA_API_BASE_URL\n",
        "        g[\"DRY_RUN\"] = bool(self.DRY_RUN)\n",
        "        g[\"INF_DETERMINISTIC\"] = bool(self.INF_DETERMINISTIC)\n",
        "        g[\"TICKERS\"] = list(self.TICKERS or [\"UNH\", \"GE\"])\n",
        "\n",
        "        # Paths\n",
        "        g[\"ARTIFACTS_DIR\"] = Path(self.ARTIFACTS_DIR)\n",
        "        g[\"RESULTS_ROOT\"] = Path(self.RESULTS_ROOT)\n",
        "        g[\"RESULTS_DIR\"] = g[\"RESULTS_ROOT\"] / datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
        "        g[\"LATEST_DIR\"] = g[\"RESULTS_ROOT\"] / \"latest\"\n",
        "\n",
        "        for p in (g[\"ARTIFACTS_DIR\"], g[\"RESULTS_DIR\"], g[\"LATEST_DIR\"]):\n",
        "            p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        results_dir = g[\"RESULTS_DIR\"]\n",
        "        latest_dir = g[\"LATEST_DIR\"]\n",
        "\n",
        "        g[\"BARS_FEED\"] = str(self.BARS_FEED).strip()\n",
        "        g[\"COOLDOWN_MIN\"] = int(self.COOLDOWN_MIN)\n",
        "        g[\"STALE_MAX_SEC\"] = int(self.STALE_MAX_SEC)\n",
        "        g[\"SIZING_MODE\"] = self.SIZING_MODE\n",
        "        g[\"WEIGHT_CAP\"] = float(self.WEIGHT_CAP)\n",
        "        g[\"ENTER_CONF_MIN\"] = float(self.ENTER_CONF_MIN)\n",
        "        g[\"ENTER_WEIGHT_MIN\"] = float(self.ENTER_WEIGHT_MIN)\n",
        "        g[\"EXIT_WEIGHT_MAX\"] = float(self.EXIT_WEIGHT_MAX)\n",
        "        g[\"REBALANCE_MIN_NOTIONAL\"] = float(self.REBALANCE_MIN_NOTIONAL)\n",
        "        g[\"USE_FRACTIONALS\"] = bool(self.USE_FRACTIONALS)\n",
        "        g[\"SEED_FIRST_SHARE\"] = bool(self.SEED_FIRST_SHARE)\n",
        "        g[\"ALLOW_SHORTS\"] = bool(self.ALLOW_SHORTS)\n",
        "        g[\"CONF_FLOOR\"] = float(self.CONF_FLOOR)\n",
        "        g[\"TAKE_PROFIT_PCT\"] = float(self.TAKE_PROFIT_PCT)\n",
        "        g[\"STOP_LOSS_PCT\"] = float(self.STOP_LOSS_PCT)\n",
        "        g[\"BEST_WINDOW_ENV\"] = (self.STALE_BEST_WINDOW or None)\n",
        "        g[\"API_KEY\"] = self.APCA_API_KEY_ID or \"\"\n",
        "        g[\"API_SECRET\"] = self.APCA_API_SECRET_KEY or \"\"\n",
        "\n",
        "        g[\"DELTA_WEIGHT_MIN\"] = float(self.DELTA_WEIGHT_MIN)\n",
        "        g[\"RAW_POS_MIN\"] = float(self.RAW_POS_MIN)\n",
        "        g[\"RAW_NEG_MAX\"] = float(self.RAW_NEG_MAX)\n",
        "\n",
        "        g[\"TRADE_LOG_CSV\"] = results_dir / \"trade_log_master.csv\"\n",
        "        g[\"EQUITY_LOG_CSV\"] = results_dir / \"equity_log.csv\"\n",
        "        g[\"PLOT_PATH\"] = results_dir / \"equity_curve.png\"\n",
        "        g[\"PLOT_PATH_LATEST\"] = latest_dir / \"equity_curve.png\"\n",
        "        g[\"EQUITY_LOG_LATEST\"] = latest_dir / \"equity_log.csv\"\n",
        "        g[\"TRADE_LOG_LATEST\"] = latest_dir / \"trade_log_master.csv\"\n",
        "\n",
        "        g[\"EQUITY_LOG_THROTTLE_SEC\"] = int(self.EQUITY_LOG_THROTTLE_SEC)\n",
        "        g[\"SKIP_EQUITY_WHEN_DRY_RUN\"] = bool(self.SKIP_EQUITY_WHEN_DRY_RUN)\n",
        "        g[\"_LAST_EQUITY_LOG_TS\"] = 0\n",
        "        g[\"_TRADE_EVENT_FLAG\"] = False\n",
        "\n",
        "        g[\"MAX_DAILY_DRAWDOWN_PCT\"] = float(self.MAX_DAILY_DRAWDOWN_PCT)\n",
        "        g[\"KILL_SWITCH_COOLDOWN_MIN\"] = int(self.KILL_SWITCH_COOLDOWN_MIN)\n",
        "        g[\"EXIT_AFTER_CLOSE\"] = bool(self.EXIT_AFTER_CLOSE)\n",
        "\n",
        "        g[\"FLATTEN_INTO_CLOSE\"] = bool(self.FLATTEN_INTO_CLOSE)\n",
        "        g[\"FORCE_FIRST_BUY\"] = bool(self.FORCE_FIRST_BUY)\n",
        "        g[\"FORCE_FLATTEN_ON_EXIT\"] = bool(self.FORCE_FLATTEN_ON_EXIT)\n",
        "\n",
        "        g[\"DATA_TIMEFRAME\"] = str(self.DATA_TIMEFRAME)\n",
        "        os.environ[\"EXIT_AFTER_CLOSE\"] = \"1\" if self.EXIT_AFTER_CLOSE else \"0\"\n",
        "\n",
        "        os.environ[\"APCA_API_BASE_URL\"] = self.APCA_API_BASE_URL\n",
        "        os.environ[\"DRY_RUN\"] = \"1\" if self.DRY_RUN else \"0\"\n",
        "        os.environ[\"AUTO_RUN_LIVE\"] = \"1\" if self.AUTO_RUN_LIVE else \"0\"\n",
        "        os.environ[\"BARS_FEED\"] = self.BARS_FEED\n",
        "\n",
        "def configure_knobs(overrides: Mapping[str, object] = None) -> Knobs:\n",
        "    defaults = Knobs(\n",
        "        TICKERS=_to_list_csv(os.getenv(\"TICKERS\", \"UNH,GE\")),\n",
        "        ARTIFACTS_DIR=os.getenv(\"ARTIFACTS_DIR\", str(PROJECT_ROOT / \"artifacts\")),\n",
        "        RESULTS_ROOT=os.getenv(\"RESULTS_ROOT\", str(PROJECT_ROOT / \"results\")),\n",
        "        DATA_TIMEFRAME=os.getenv(\"DATA_TIMEFRAME\", \"1H\"),\n",
        "    )\n",
        "    cfg = Knobs.from_env(defaults, PROJECT_ROOT, os.environ, overrides=overrides)\n",
        "    cfg.apply_to_globals()\n",
        "    return cfg\n",
        "\n",
        "# ============================================================\n",
        "# Time helpers\n",
        "# ============================================================\n",
        "\n",
        "def ensure_utc(ts_like) -> pd.Timestamp:\n",
        "    \"\"\"Return a timezone-aware UTC Timestamp from any datetime-like input.\"\"\"\n",
        "    ts = pd.Timestamp(ts_like)\n",
        "    if ts.tzinfo is None:\n",
        "        return ts.tz_localize(\"UTC\")\n",
        "    return ts.tz_convert(\"UTC\")\n",
        "\n",
        "def now_utc() -> datetime:\n",
        "    return datetime.now(timezone.utc)\n",
        "\n",
        "def utc_ts(dt_like) -> int:\n",
        "    if isinstance(dt_like, (int, np.integer)):\n",
        "        return int(dt_like)\n",
        "    if isinstance(dt_like, (float, np.floating)):\n",
        "        return int(dt_like)\n",
        "    ts = ensure_utc(dt_like)\n",
        "    return int(ts.value // 10**9)\n",
        "\n",
        "\n",
        "def utcnow_iso() -> str:\n",
        "    return datetime.now(timezone.utc).isoformat()\n",
        "\n",
        "def _sleep_to_next_minute_block(n: int):\n",
        "    n = max(1, int(n))\n",
        "    now = now_utc()\n",
        "    base = now.replace(second=0, microsecond=0)\n",
        "    remainder = base.minute % n\n",
        "    add = (n - remainder) % n\n",
        "    if add == 0:\n",
        "        add = n\n",
        "    next_slot = base + timedelta(minutes=add)\n",
        "    time.sleep(max(0.0, (next_slot - now).total_seconds()))\n",
        "\n",
        "# ============================================================\n",
        "# CSV logging (master) + equity snapshots\n",
        "# ============================================================\n",
        "\n",
        "TRADE_FIELDS = [\"datetime_utc\", \"ticker\", \"signal\", \"action\", \"price\", \"equity\", \"qty\", \"comment\"]\n",
        "\n",
        "def ensure_trade_log_header():\n",
        "    if (not TRADE_LOG_CSV.exists()) or (TRADE_LOG_CSV.stat().st_size == 0):\n",
        "        pd.DataFrame(columns=TRADE_FIELDS).to_csv(TRADE_LOG_CSV, index=False)\n",
        "\n",
        "def log_trade(\n",
        "    ticker: str,\n",
        "    signal: float,\n",
        "    action: str,\n",
        "    price: float,\n",
        "    equity: float,\n",
        "    qty: float = None,\n",
        "    comment: str = \"\",\n",
        "):\n",
        "    ensure_trade_log_header()\n",
        "    row = {\n",
        "        \"datetime_utc\": utcnow_iso(),\n",
        "        \"ticker\": ticker,\n",
        "        \"signal\": int(signal) if signal is not None else \"\",\n",
        "        \"action\": action,\n",
        "        \"price\": (float(price) if price is not None and np.isfinite(price) else \"\"),\n",
        "        \"equity\": (float(equity) if equity is not None and np.isfinite(equity) else \"\"),\n",
        "        \"qty\": (float(qty) if qty is not None and np.isfinite(qty) else \"\"),\n",
        "        \"comment\": (str(comment) if comment else \"\"),\n",
        "    }\n",
        "    with TRADE_LOG_CSV.open(\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        csv.DictWriter(f, fieldnames=TRADE_FIELDS).writerow(row)\n",
        "    try:\n",
        "        shutil.copy2(TRADE_LOG_CSV, TRADE_LOG_LATEST)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def init_alpaca() -> \"tradeapi.REST\":\n",
        "    if not (globals().get(\"API_KEY\") and globals().get(\"API_SECRET\")):\n",
        "        raise RuntimeError(\"Missing Alpaca API keys (check your .env).\")\n",
        "    return tradeapi.REST(API_KEY, API_SECRET, base_url=BASE_URL)\n",
        "\n",
        "# Timeout-safe Alpaca calls for portfolio history\n",
        "from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError\n",
        "_TIMEOUT_EXEC = ThreadPoolExecutor(max_workers=8)\n",
        "\n",
        "def _call_with_timeout(func, timeout_sec: int, *args, **kwargs):\n",
        "    fut = _TIMEOUT_EXEC.submit(func, *args, **kwargs)\n",
        "    try:\n",
        "        return fut.result(timeout=timeout_sec)\n",
        "    except FuturesTimeoutError:\n",
        "        raise TimeoutError(f\"Timed out after {timeout_sec}s\")\n",
        "\n",
        "def get_portfolio_history_safe(api, period=\"1M\", timeframe=None, timeout_sec: int = 8, retries: int = 1):\n",
        "    timeframe = timeframe or os.getenv(\"EQUITY_TIMEFRAME\", os.getenv(\"DATA_TIMEFRAME\", \"1H\"))\n",
        "    last_exc = None\n",
        "    for _ in range(max(1, retries + 1)):\n",
        "        try:\n",
        "            return _call_with_timeout(api.get_portfolio_history, timeout_sec, period=period, timeframe=timeframe)\n",
        "        except Exception as e:\n",
        "            last_exc = e\n",
        "            time.sleep(0.5)\n",
        "    logging.warning(f\"get_portfolio_history_safe failed: {last_exc}\")\n",
        "    return None\n",
        "\n",
        "def fetch_portfolio_history(period=\"1M\", timeframe=None, api_in=None):\n",
        "    timeframe = timeframe or os.getenv(\"EQUITY_TIMEFRAME\", os.getenv(\"DATA_TIMEFRAME\", \"1H\"))\n",
        "    a = api_in if api_in is not None else globals().get(\"api\", None)\n",
        "    if a is None:\n",
        "        return pd.DataFrame(columns=[\"timestamp_utc\", \"equity\"])\n",
        "\n",
        "    hist = get_portfolio_history_safe(\n",
        "        a, period=period, timeframe=timeframe,\n",
        "        timeout_sec=int(os.getenv(\"PH_TIMEOUT_SEC\", \"8\")), retries=1\n",
        "    )\n",
        "\n",
        "    if (not hist) or (not getattr(hist, \"timestamp\", None)) or (not getattr(hist, \"equity\", None)):\n",
        "        if EQUITY_LOG_CSV.exists():\n",
        "            try:\n",
        "                df = pd.read_csv(EQUITY_LOG_CSV, parse_dates=[\"datetime_utc\"])\n",
        "                return df.rename(columns={\"datetime_utc\": \"timestamp_utc\"})[[\"timestamp_utc\", \"equity\"]]\n",
        "            except Exception:\n",
        "                pass\n",
        "        return pd.DataFrame(columns=[\"timestamp_utc\", \"equity\"])\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        \"timestamp_utc\": pd.to_datetime(hist.timestamp, unit=\"s\", utc=True),\n",
        "        \"equity\": pd.Series(hist.equity, dtype=\"float64\"),\n",
        "    }).dropna()\n",
        "\n",
        "def log_equity_snapshot(api_in=None):\n",
        "    snap = fetch_portfolio_history(period=\"1D\", timeframe=os.getenv(\"EQUITY_TIMEFRAME\", \"5Min\"), api_in=api_in)\n",
        "    if snap.empty:\n",
        "        return\n",
        "    latest = snap.iloc[-1:].copy().rename(columns={\"timestamp_utc\": \"datetime_utc\"})\n",
        "\n",
        "    if EQUITY_LOG_CSV.exists():\n",
        "        df_old = pd.read_csv(EQUITY_LOG_CSV, parse_dates=[\"datetime_utc\"])\n",
        "        if not df_old.empty and pd.to_datetime(df_old[\"datetime_utc\"].iloc[-1]) == latest[\"datetime_utc\"].iloc[0]:\n",
        "            return\n",
        "        pd.concat([df_old, latest], ignore_index=True)\\\n",
        "            .drop_duplicates(subset=[\"datetime_utc\"], keep=\"last\")\\\n",
        "            .to_csv(EQUITY_LOG_CSV, index=False)\n",
        "    else:\n",
        "        latest.to_csv(EQUITY_LOG_CSV, index=False)\n",
        "\n",
        "    try:\n",
        "        shutil.copy2(EQUITY_LOG_CSV, EQUITY_LOG_LATEST)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def maybe_log_equity_snapshot(api_in=None, reason: str = \"cycle\"):\n",
        "    global _LAST_EQUITY_LOG_TS, _TRADE_EVENT_FLAG\n",
        "    if bool(globals().get(\"DRY_RUN\", False)) and bool(globals().get(\"SKIP_EQUITY_WHEN_DRY_RUN\", True)):\n",
        "        return\n",
        "\n",
        "    now_ts = time.time()\n",
        "    force = reason in {\"trade\", \"finalize\", \"close\"}\n",
        "    if force or (now_ts - float(_LAST_EQUITY_LOG_TS)) >= int(globals().get(\"EQUITY_LOG_THROTTLE_SEC\", 900)):\n",
        "        try:\n",
        "            log_equity_snapshot(api_in=api_in)\n",
        "            _LAST_EQUITY_LOG_TS = now_ts\n",
        "        except Exception as e:\n",
        "            logging.debug(f\"maybe_log_equity_snapshot failed: {e}\")\n",
        "\n",
        "    if reason == \"trade\":\n",
        "        _TRADE_EVENT_FLAG = False\n",
        "\n",
        "def plot_equity_curve(from_equity_csv: bool = True):\n",
        "    with plt.ioff():\n",
        "        if from_equity_csv and EQUITY_LOG_CSV.exists():\n",
        "            df = pd.read_csv(EQUITY_LOG_CSV, parse_dates=[\"datetime_utc\"]).sort_values(\"datetime_utc\")\n",
        "        else:\n",
        "            df = fetch_portfolio_history(period=\"3M\", timeframe=os.getenv(\"EQUITY_TIMEFRAME\", \"5Min\"))\\\n",
        "                .rename(columns={\"timestamp_utc\": \"datetime_utc\"})\n",
        "\n",
        "        if df.empty:\n",
        "            print(\"No equity data to plot yet.\")\n",
        "            return\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 4))\n",
        "        ax.plot(df[\"datetime_utc\"], df[\"equity\"])\n",
        "        ax.set_title(\"Portfolio Value Over Time (Paper)\")\n",
        "        ax.set_xlabel(\"Time (UTC)\")\n",
        "        ax.set_ylabel(\"Equity ($)\")\n",
        "        fig.tight_layout()\n",
        "        fig.savefig(PLOT_PATH, bbox_inches=\"tight\")\n",
        "        fig.savefig(PLOT_PATH_LATEST, bbox_inches=\"tight\")\n",
        "        plt.close(fig)\n",
        "        print(f\"Saved equity curve → {PLOT_PATH}\")\n",
        "        print(f\"Updated latest copy → {PLOT_PATH_LATEST}\")\n",
        "\n",
        "def compute_performance_metrics(df_equity: pd.DataFrame):\n",
        "    if df_equity.empty or df_equity[\"equity\"].isna().all():\n",
        "        return {\"cum_return\": np.nan, \"sharpe\": np.nan, \"max_drawdown\": np.nan}\n",
        "\n",
        "    df = df_equity.sort_values(\"datetime_utc\")\n",
        "    e = df[\"equity\"].astype(float)\n",
        "    r = e.pct_change().dropna()\n",
        "    if r.empty:\n",
        "        return {\"cum_return\": 0.0, \"sharpe\": np.nan, \"max_drawdown\": np.nan}\n",
        "\n",
        "    dt_sec = df[\"datetime_utc\"].diff().dt.total_seconds().dropna().median()\n",
        "    if not (isinstance(dt_sec, (int, float)) and dt_sec > 0):\n",
        "        periods_per_year = 252 * 78  # fallback for ~5-min bars\n",
        "    else:\n",
        "        periods_per_day = (6.5 * 3600) / dt_sec\n",
        "        periods_per_year = 252 * periods_per_day\n",
        "\n",
        "    sharpe = (r.mean() / (r.std() + 1e-12)) * math.sqrt(periods_per_year)\n",
        "    cum = (1 + r).cumprod()\n",
        "    peak = cum.cummax()\n",
        "    dd = (cum / peak - 1.0).min()\n",
        "    cum_return = e.iloc[-1] / e.iloc[0] - 1.0\n",
        "    return {\"cum_return\": float(cum_return), \"sharpe\": float(sharpe), \"max_drawdown\": float(dd)}\n",
        "\n",
        "# ============================================================\n",
        "# Per-ticker CSV logging\n",
        "# ============================================================\n",
        "\n",
        "def _append_csv_row(path: Path, row: dict):\n",
        "    if path.name in (\"trade_log_master.csv\", \"equity_log.csv\"):\n",
        "        raise RuntimeError(f\"_append_csv_row must not write to master file: {path}\")\n",
        "\n",
        "    fieldnames = list(row.keys())\n",
        "\n",
        "    if not path.exists():\n",
        "        with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            w = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "            w.writeheader()\n",
        "            w.writerow(row)\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        with path.open(\"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            r = csv.reader(f)\n",
        "            old_header = next(r)\n",
        "    except Exception:\n",
        "        old_header = []\n",
        "\n",
        "    if old_header != fieldnames:\n",
        "        tmp = path.with_suffix(\".tmp\")\n",
        "        with tmp.open(\"w\", newline=\"\", encoding=\"utf-8\") as wf, path.open(\"r\", newline=\"\", encoding=\"utf-8\") as rf:\n",
        "            r = csv.DictReader(rf) if old_header else None\n",
        "            w = csv.DictWriter(wf, fieldnames=fieldnames)\n",
        "            w.writeheader()\n",
        "            if r:\n",
        "                for old_row in r:\n",
        "                    merged = {k: old_row.get(k, \"\") for k in fieldnames}\n",
        "                    w.writerow(merged)\n",
        "        tmp.replace(path)\n",
        "\n",
        "    with path.open(\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "        w.writerow(row)\n",
        "\n",
        "def log_trade_symbol(\n",
        "    symbol: str,\n",
        "    bar_time,\n",
        "    signal: int,\n",
        "    raw_action: float,\n",
        "    weight: float,\n",
        "    confidence: float,\n",
        "    price: float,\n",
        "    equity: float,\n",
        "    dry_run: bool,\n",
        "    note: str = \"\",\n",
        "    order_submitted: int = 0,\n",
        "    order_id: str = \"\",\n",
        "    order_status: str = \"\",\n",
        "    filled_qty: str = \"\",\n",
        "):\n",
        "    try:\n",
        "        bt = pd.Timestamp(bar_time)\n",
        "        if bt.tzinfo is None:\n",
        "            bt = bt.tz_localize(\"UTC\")\n",
        "        else:\n",
        "            bt = bt.tz_convert(\"UTC\")\n",
        "        bt_iso = bt.isoformat()\n",
        "        age_sec = int((now_utc() - bt.to_pydatetime()).total_seconds())\n",
        "    except Exception:\n",
        "        bt_iso = \"\"\n",
        "        age_sec = \"\"\n",
        "\n",
        "    resolved_feed = (os.getenv(\"BARS_FEED\", \"\").strip() or \"default\")\n",
        "\n",
        "    if note:\n",
        "        decision = note\n",
        "    else:\n",
        "        if abs(float(weight)) <= float(globals().get(\"EXIT_WEIGHT_MAX\", 0.0)):\n",
        "            decision = \"hold_or_flat\"\n",
        "        else:\n",
        "            decision = \"rebalance_long\" if float(weight) > 0 else \"rebalance_short\"\n",
        "\n",
        "    try:\n",
        "        w = float(weight) if weight is not None else 0.0\n",
        "    except Exception:\n",
        "        w = 0.0\n",
        "\n",
        "    if abs(w) <= float(globals().get(\"EXIT_WEIGHT_MAX\", 0.0)):\n",
        "        sig = \"FLAT\"\n",
        "    elif w > 0:\n",
        "        sig = \"LONG\"\n",
        "    else:\n",
        "        sig = \"SHORT\"\n",
        "\n",
        "    row = {\n",
        "        \"log_time\": now_utc().isoformat(),\n",
        "        \"symbol\": symbol,\n",
        "        \"bar_time\": bt_iso,\n",
        "        \"bar_age_sec\": age_sec,\n",
        "        \"feed\": resolved_feed,\n",
        "        \"signal\": sig,\n",
        "        \"raw_action\": float(raw_action) if raw_action is not None and np.isfinite(raw_action) else \"\",\n",
        "        \"weight\": float(weight) if weight is not None and np.isfinite(weight) else \"\",\n",
        "        \"confidence\": float(confidence) if confidence is not None and np.isfinite(confidence) else \"\",\n",
        "        \"price\": float(price) if price is not None and np.isfinite(price) else \"\",\n",
        "        \"equity\": float(equity) if equity is not None and np.isfinite(equity) else \"\",\n",
        "        \"dry_run\": int(bool(dry_run)),\n",
        "        \"decision\": decision,\n",
        "        \"note\": note,\n",
        "        \"order_submitted\": int(order_submitted),\n",
        "        \"order_id\": str(order_id or \"\"),\n",
        "        \"order_status\": str(order_status or \"\"),\n",
        "        \"filled_qty\": str(filled_qty or \"\"),\n",
        "    }\n",
        "\n",
        "    _append_csv_row(RESULTS_DIR / f\"trade_log_{symbol}.csv\", row)\n",
        "\n",
        "    try:\n",
        "        action = str(decision)[:64]\n",
        "        comment = str(note or decision)[:200]\n",
        "        log_trade(\n",
        "            ticker=symbol,\n",
        "            signal=1 if int(signal) == 1 else 0,\n",
        "            action=action,\n",
        "            price=float(price) if (price is not None and np.isfinite(price)) else None,\n",
        "            equity=float(equity) if (equity is not None and np.isfinite(equity)) else None,\n",
        "            qty=None,\n",
        "            comment=comment,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logging.debug(\"master trade log write failed: %s\", e)\n",
        "\n",
        "# ============================================================\n",
        "# Artifacts: picker & loaders\n",
        "# ============================================================\n",
        "\n",
        "def _extract_window_idx(path: Path) -> Optional[int]:\n",
        "    m = re.search(r\"_window(\\d+)\", path.stem)\n",
        "    return int(m.group(1)) if m else None\n",
        "\n",
        "def _prefer_same_window(cands, w: Optional[int]):\n",
        "    cands = list(cands)\n",
        "    if not cands:\n",
        "        return []\n",
        "    if w is None:\n",
        "        return sorted(cands)\n",
        "    same = [p for p in cands if _extract_window_idx(p) == w]\n",
        "    return sorted(same or cands)\n",
        "\n",
        "def pick_artifacts_for_ticker(ticker: str, artifacts_dir: str, best_window: Optional[str] = None) -> Dict[str, Optional[Path]]:\n",
        "    p = Path(artifacts_dir)\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f\"Artifacts directory not found: {p.resolve()}\")\n",
        "\n",
        "    models = sorted(p.glob(f\"ppo_{ticker}_window*_model*.zip\"))\n",
        "    if not models:\n",
        "        models = (\n",
        "            sorted(p.glob(f\"ppo_{ticker}_model*.zip\")) or\n",
        "            sorted(p.glob(f\"*{ticker}*model*.zip\"))\n",
        "        )\n",
        "    if not models:\n",
        "        raise FileNotFoundError(f\"No PPO model zip found for {ticker} in {p}\")\n",
        "\n",
        "    def _model_sort_key(path: Path):\n",
        "        w = _extract_window_idx(path)\n",
        "        return (w if w is not None else -1, \" (1)\" in path.stem)\n",
        "\n",
        "    models = sorted(models, key=_model_sort_key)\n",
        "\n",
        "    chosen: Optional[Path] = None\n",
        "    if best_window:\n",
        "        chosen = next((m for m in models if f\"_window{best_window}_\" in m.stem), None)\n",
        "        if chosen is None:\n",
        "            logging.warning(\"[%s] BEST_WINDOW=%s not found; falling back.\", ticker, best_window)\n",
        "\n",
        "    if chosen is None:\n",
        "        with_idx = [(m, _extract_window_idx(m)) for m in models]\n",
        "        with_idx = [(m, w) for (m, w) in with_idx if w is not None]\n",
        "        chosen = max(with_idx, key=lambda t: t[1])[0] if with_idx else models[-1]\n",
        "\n",
        "    chosen_w = _extract_window_idx(chosen)\n",
        "\n",
        "    vec_candidates = list(p.glob(f\"ppo_{ticker}_window*_vecnorm*.pkl\"))\n",
        "    feat_candidates = list(p.glob(f\"ppo_{ticker}_window*_features*.json\"))\n",
        "\n",
        "    vecnorm = (_prefer_same_window(vec_candidates, chosen_w)[0] if vec_candidates else None)\n",
        "    feats = (_prefer_same_window(feat_candidates, chosen_w)[0] if feat_candidates else None)\n",
        "\n",
        "    logging.info(\"[%s] model=%s | window=%s | vecnorm=%s | features=%s\",\n",
        "                 ticker,\n",
        "                 chosen.name,\n",
        "                 chosen_w,\n",
        "                 vecnorm.name if vecnorm else \"None\",\n",
        "                 feats.name if feats else \"None\")\n",
        "\n",
        "    return {\"model\": chosen, \"vecnorm\": vecnorm, \"features\": feats}\n",
        "\n",
        "def load_vecnormalize(path: Optional[Path]):\n",
        "    if path is None:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        with open(path, \"rb\") as f:\n",
        "            return pickle.load(f)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        return VecNormalize.load(str(path), venv=None)\n",
        "    except Exception as e:\n",
        "        logging.warning(\"VecNormalize load failed (%s). Proceeding without normalization.\", e)\n",
        "        return None\n",
        "\n",
        "def load_features(path: Optional[Path]):\n",
        "    if path is None:\n",
        "        return None\n",
        "    with open(path, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def _const_schedule(val: float):\n",
        "    # SB3 schedules are callables: f(progress_remaining) -> float\n",
        "    return lambda _progress_remaining: float(val)\n",
        "\n",
        "def load_ppo_model(model_path: Path):\n",
        "    # Pick safe defaults; these only matter if the model tries to use them during training.\n",
        "    # For inference, they’re irrelevant—but this prevents the unpickle warning.\n",
        "    custom_objects = {\n",
        "        \"lr_schedule\": _const_schedule(5e-5),\n",
        "        \"clip_range\":  _const_schedule(0.2),\n",
        "        # Sometimes also present depending on SB3 version / PPO config:\n",
        "        \"clip_range_vf\": _const_schedule(0.2),\n",
        "    }\n",
        "    return PPO.load(str(model_path), custom_objects=custom_objects)\n",
        "\n",
        "# Cached asset flags\n",
        "@lru_cache(maxsize=256)\n",
        "def _asset_flags(symbol: str) -> Tuple[bool, bool, bool]:\n",
        "    try:\n",
        "        _api = globals().get(\"api\") or init_alpaca()\n",
        "        a = _api.get_asset(symbol)\n",
        "        return (\n",
        "            bool(getattr(a, \"tradable\", True)),\n",
        "            bool(getattr(a, \"fractionable\", False)),\n",
        "            bool(getattr(a, \"shortable\", False)),\n",
        "        )\n",
        "    except Exception:\n",
        "        return True, False, False\n",
        "\n",
        "def _can_seed_short(api, symbol: str) -> Tuple[bool, str]:\n",
        "    if not globals().get(\"ALLOW_SHORTS\", False):\n",
        "        return False, \"shorts_disabled_seed\"\n",
        "    try:\n",
        "        a = api.get_asset(symbol)\n",
        "        if not getattr(a, \"shortable\", False):\n",
        "            return False, \"not_shortable_seed\"\n",
        "        return True, \"\"\n",
        "    except Exception as e:\n",
        "        logging.info(f\"[{symbol}] get_asset shortable check failed: {e}\")\n",
        "        return False, \"shortable_check_error\"\n",
        "\n",
        "# ============================================================\n",
        "# Market data + account helpers\n",
        "# ============================================================\n",
        "\n",
        "def get_recent_bars(api, symbol: str, limit: int = 200, timeframe=LIVE_TIMEFRAME) -> pd.DataFrame:\n",
        "    def _as_df(bars):\n",
        "        if hasattr(bars, \"df\"):\n",
        "            df = bars.df.copy()\n",
        "            if not df.empty:\n",
        "                if isinstance(df.index, pd.MultiIndex):\n",
        "                    try:\n",
        "                        df = df.xs(symbol, level=0)\n",
        "                    except KeyError:\n",
        "                        df = df.reset_index(level=0, drop=True)\n",
        "                df.index = pd.to_datetime(df.index, utc=True, errors=\"coerce\")\n",
        "                df = df.rename(columns={\"open\": \"Open\", \"high\": \"High\", \"low\": \"Low\", \"close\": \"Close\", \"volume\": \"Volume\"})\n",
        "                cols = [c for c in [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"] if c in df.columns]\n",
        "                return df[cols].sort_index()\n",
        "            return pd.DataFrame(columns=[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"])\n",
        "\n",
        "        rows = []\n",
        "        for b in bars:\n",
        "            ts = getattr(b, \"t\", None)\n",
        "            ts = pd.to_datetime(ts, utc=True) if ts is not None else pd.NaT\n",
        "            rows.append({\n",
        "                \"timestamp\": ts,\n",
        "                \"Open\": float(getattr(b, \"o\", getattr(b, \"open\", np.nan))),\n",
        "                \"High\": float(getattr(b, \"h\", getattr(b, \"high\", np.nan))),\n",
        "                \"Low\": float(getattr(b, \"l\", getattr(b, \"low\", np.nan))),\n",
        "                \"Close\": float(getattr(b, \"c\", getattr(b, \"close\", np.nan))),\n",
        "                \"Volume\": float(getattr(b, \"v\", getattr(b, \"volume\", np.nan))),\n",
        "            })\n",
        "        df = pd.DataFrame(rows)\n",
        "        if df.empty:\n",
        "            return pd.DataFrame(columns=[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"])\n",
        "        return df.set_index(pd.to_datetime(df[\"timestamp\"], utc=True)).drop(columns=[\"timestamp\"]).sort_index()\n",
        "\n",
        "    feed = os.getenv(\"BARS_FEED\", \"\").strip()\n",
        "    try:\n",
        "        logging.info(f\"[{symbol}] fetching {limit} {timeframe} bars (feed='{feed or 'default'}')\")\n",
        "        bars = api.get_bars(symbol, timeframe, limit=limit, feed=feed) if feed else api.get_bars(symbol, timeframe, limit=limit)\n",
        "        df = _as_df(bars)\n",
        "        if not df.empty:\n",
        "            return df\n",
        "        if feed:\n",
        "            logging.info(f\"[{symbol}] explicit feed empty; retrying with default feed\")\n",
        "            df2 = _as_df(api.get_bars(symbol, timeframe, limit=limit))\n",
        "            if not df2.empty:\n",
        "                return df2\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[{symbol}] get_bars(limit) failed: {e}\")\n",
        "\n",
        "    try:\n",
        "        end_dt = datetime.now(timezone.utc).replace(microsecond=0)\n",
        "        start_dt = end_dt - timedelta(days=5)\n",
        "        end = end_dt.isoformat().replace(\"+00:00\", \"Z\")\n",
        "        start = start_dt.isoformat().replace(\"+00:00\", \"Z\")\n",
        "        logging.info(f\"[{symbol}] retry window start={start} end={end} (feed='{feed or 'default'}')\")\n",
        "        bars = api.get_bars(symbol, timeframe, start=start, end=end, feed=feed) if feed else api.get_bars(symbol, timeframe, start=start, end=end)\n",
        "        return _as_df(bars)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[{symbol}] get_bars(start/end) failed: {e}\")\n",
        "        return pd.DataFrame(columns=[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"])\n",
        "\n",
        "def get_account_equity(api) -> float:\n",
        "    return float(api.get_account().equity)\n",
        "\n",
        "def get_position(api, symbol: str):\n",
        "    try:\n",
        "        return api.get_position(symbol)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def get_position_qty(api, symbol: str):\n",
        "    try:\n",
        "        pos = api.get_position(symbol)\n",
        "    except Exception:\n",
        "        pos = None\n",
        "    if not pos:\n",
        "        return 0.0 if USE_FRACTIONALS else 0\n",
        "    try:\n",
        "        q = float(pos.qty)\n",
        "        return q if USE_FRACTIONALS else int(round(q))\n",
        "    except Exception:\n",
        "        return 0.0 if USE_FRACTIONALS else 0\n",
        "\n",
        "def get_last_price(api, symbol: str) -> float:\n",
        "    try:\n",
        "        tr = api.get_latest_trade(symbol)\n",
        "        price = getattr(tr, \"price\", None)\n",
        "        if price is None:\n",
        "            price = getattr(tr, \"p\", None)\n",
        "        if price is not None and np.isfinite(price):\n",
        "            return float(price)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        feed = os.getenv(\"BARS_FEED\", \"\").strip() or None\n",
        "        bars = api.get_bars(symbol, LIVE_TIMEFRAME, limit=1, feed=feed) if feed else api.get_bars(symbol, LIVE_TIMEFRAME, limit=1)\n",
        "        if hasattr(bars, \"df\"):\n",
        "            df = bars.df.copy()\n",
        "            if isinstance(df.index, pd.MultiIndex):\n",
        "                try:\n",
        "                    df = df.xs(symbol, level=0)\n",
        "                except Exception:\n",
        "                    df = df.reset_index(level=0, drop=True)\n",
        "            if not df.empty:\n",
        "                if \"close\" in df.columns:\n",
        "                    return float(df[\"close\"].iloc[-1])\n",
        "                if \"Close\" in df.columns:\n",
        "                    return float(df[\"Close\"].iloc[-1])\n",
        "        elif bars:\n",
        "            b = bars[0]\n",
        "            close = getattr(b, \"c\", getattr(b, \"close\", None))\n",
        "            if close is not None:\n",
        "                return float(close)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[{symbol}] get_last_price via bars failed: {e}\")\n",
        "\n",
        "    try:\n",
        "        qt = api.get_latest_quote(symbol)\n",
        "        ap = getattr(qt, \"ap\", None) or getattr(qt, \"ask_price\", None)\n",
        "        bp = getattr(qt, \"bp\", None) or getattr(qt, \"bid_price\", None)\n",
        "        if ap and bp:\n",
        "            return float((float(ap) + float(bp)) / 2.0)\n",
        "        if ap:\n",
        "            return float(ap)\n",
        "        if bp:\n",
        "            return float(bp)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        pos = api.get_position(symbol)\n",
        "        return float(pos.avg_entry_price)\n",
        "    except Exception:\n",
        "        return float(\"nan\")\n",
        "\n",
        "def flatten_symbol(api, symbol: str):\n",
        "    try:\n",
        "        api.close_position(symbol)\n",
        "        logging.info(f\"[{symbol}] close_position submitted.\")\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[{symbol}] close_position failed: {e}\")\n",
        "\n",
        "def flatten_all_positions(api):\n",
        "    try:\n",
        "        api.close_all_positions()\n",
        "        logging.info(\"close_all_positions submitted.\")\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"close_all_positions failed: {e}\")\n",
        "\n",
        "def to_2dp_str(x) -> str:\n",
        "    return format(Decimal(str(x)).quantize(Decimal(\"0.01\"), rounding=ROUND_HALF_UP), \"f\")\n",
        "\n",
        "def to_6dp_str(x) -> str:\n",
        "    return format(Decimal(str(x)).quantize(Decimal(\"0.000001\"), rounding=ROUND_DOWN), \"f\")\n",
        "\n",
        "def market_order(api, symbol: str, side: str, qty=None, notional: float = None):\n",
        "    if qty is not None and notional is not None:\n",
        "        logging.warning(f\"[{symbol}] Both qty and notional provided; preferring notional.\")\n",
        "        qty = None\n",
        "\n",
        "    if qty is None and notional is None:\n",
        "        logging.warning(f\"[{symbol}] No order size provided; skipping.\")\n",
        "        return None\n",
        "\n",
        "    if qty is not None:\n",
        "        try:\n",
        "            if float(qty) <= 0:\n",
        "                logging.warning(f\"[{symbol}] Non-positive qty ({qty}); skipping.\")\n",
        "                return None\n",
        "        except Exception:\n",
        "            pass\n",
        "    if notional is not None and notional <= 0:\n",
        "        logging.warning(f\"[{symbol}] Non-positive notional (${notional}); skipping.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        if (notional is not None and (not np.isfinite(float(notional)) or float(notional) < 0.01)) or \\\n",
        "           (qty is not None and (not np.isfinite(float(qty)) or float(qty) == 0.0)):\n",
        "            logging.info(f\"[{symbol}] Order size ~0; skipping.\")\n",
        "            return None\n",
        "    except Exception:\n",
        "        logging.info(f\"[{symbol}] Order size parse issue; skipping.\")\n",
        "        return None\n",
        "\n",
        "    if DRY_RUN:\n",
        "        notional_str = to_2dp_str(notional) if notional is not None else None\n",
        "        logging.info(\n",
        "            f\"[DRY_RUN] Would submit {side} \"\n",
        "            f\"{('notional=$' + str(notional_str)) if notional_str is not None else ('qty=' + str(qty))} \"\n",
        "            f\"{symbol} (market, day)\"\n",
        "        )\n",
        "        globals()[\"_TRADE_EVENT_FLAG\"] = True\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        qty_arg = None\n",
        "        if qty is not None:\n",
        "            qty_arg = to_6dp_str(float(qty)) if USE_FRACTIONALS else int(qty)\n",
        "            if (not USE_FRACTIONALS) and int(qty_arg) <= 0:\n",
        "                logging.info(f\"[{symbol}] qty rounds to 0 shares; skipping.\")\n",
        "                return None\n",
        "        notional_arg = to_2dp_str(float(notional)) if notional is not None else None\n",
        "\n",
        "        o = api.submit_order(\n",
        "            symbol=symbol,\n",
        "            side=side,\n",
        "            type=\"market\",\n",
        "            time_in_force=\"day\",\n",
        "            qty=qty_arg,\n",
        "            notional=notional_arg,\n",
        "        )\n",
        "\n",
        "        logging.info(\n",
        "            f\"[{symbol}] Submitted {side} \"\n",
        "            f\"{('notional=$' + str(notional_arg)) if notional_arg is not None else ('qty=' + str(qty_arg))}\"\n",
        "        )\n",
        "        globals()[\"_TRADE_EVENT_FLAG\"] = True\n",
        "        return o\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"[{symbol}] submit_order failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def market_order_to_qty(api, symbol: str, side: str, qty):\n",
        "    if qty is None:\n",
        "        logging.warning(f\"[{symbol}] qty is None; skipping.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        q = float(qty)\n",
        "    except Exception:\n",
        "        logging.warning(f\"[{symbol}] qty not numeric ({qty}); skipping.\")\n",
        "        return None\n",
        "\n",
        "    if not np.isfinite(q) or q <= 0:\n",
        "        logging.info(f\"[{symbol}] Non-positive qty ({qty}); skipping.\")\n",
        "        return None\n",
        "\n",
        "    if not bool(globals().get(\"USE_FRACTIONALS\", True)):\n",
        "        q_int = int(math.floor(q))\n",
        "        if q_int <= 0:\n",
        "            logging.info(f\"[{symbol}] qty rounds to 0 shares; skipping.\")\n",
        "            return None\n",
        "        q = q_int\n",
        "\n",
        "    if bool(globals().get(\"DRY_RUN\", False)):\n",
        "        logging.info(f\"[DRY_RUN] Would submit {side} qty={q} {symbol} (market, day)\")\n",
        "        globals()[\"_TRADE_EVENT_FLAG\"] = True\n",
        "        return None\n",
        "\n",
        "    if side == \"sell\" and (not can_short_symbol(api, symbol)):\n",
        "        have_qty = get_position_qty(api, symbol)\n",
        "        if have_qty <= 0:\n",
        "            logging.info(f\"[{symbol}] Sell skipped (no shares and shorting not allowed).\")\n",
        "            return None\n",
        "        q = min(float(q), float(have_qty))\n",
        "        if q <= 0:\n",
        "            logging.info(f\"[{symbol}] Sell qty clamped to 0; skipping.\")\n",
        "            return None\n",
        "\n",
        "    try:\n",
        "        qty_arg = to_6dp_str(float(q)) if bool(globals().get(\"USE_FRACTIONALS\", True)) else int(q)\n",
        "        o = api.submit_order(\n",
        "            symbol=symbol,\n",
        "            side=side,\n",
        "            type=\"market\",\n",
        "            time_in_force=\"day\",\n",
        "            qty=qty_arg,\n",
        "        )\n",
        "        logging.info(f\"[{symbol}] Submitted {side} qty={qty_arg}\")\n",
        "        globals()[\"_TRADE_EVENT_FLAG\"] = True\n",
        "        return o\n",
        "    except Exception as e:\n",
        "        logging.error(f\"[{symbol}] submit_order(qty) failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def submit_fractional_rebalance(api, symbol: str, delta_notional: float, price: float):\n",
        "    dn = round_to_cents(abs(delta_notional))\n",
        "    if dn < float(globals().get(\"REBALANCE_MIN_NOTIONAL\", 0.0)):\n",
        "        return None\n",
        "\n",
        "    if delta_notional > 0:\n",
        "        return market_order(api, symbol, side=\"buy\", notional=dn)\n",
        "    qty = dn / max(float(price), 1e-9)\n",
        "\n",
        "    if not can_short_symbol(api, symbol):\n",
        "        have_qty = get_position_qty(api, symbol)\n",
        "        if have_qty <= 0:\n",
        "            logging.info(f\"[{symbol}] Fractional sell skipped (no shares, no shorting).\")\n",
        "            return None\n",
        "        qty = min(float(qty), float(have_qty))\n",
        "        if qty <= 0:\n",
        "            return None\n",
        "    return market_order_to_qty(api, symbol, side=\"sell\", qty=qty)\n",
        "\n",
        "NO_ORDER = {\"order_submitted\": 0, \"order_id\": \"\", \"order_status\": \"\", \"filled_qty\": \"\"}\n",
        "\n",
        "def _order_info(order_obj) -> dict:\n",
        "    if order_obj is None:\n",
        "        return dict(NO_ORDER)\n",
        "    return {\n",
        "        \"order_submitted\": 1,\n",
        "        \"order_id\": str(getattr(order_obj, \"id\", \"\") or \"\"),\n",
        "        \"order_status\": str(getattr(order_obj, \"status\", \"\") or \"\"),\n",
        "        \"filled_qty\": str(getattr(order_obj, \"filled_qty\", \"\") or \"\"),\n",
        "    }\n",
        "\n",
        "def compute_target_qty_by_cash(equity: float, price: float, target_weight: float, api=None) -> int:\n",
        "    if not np.isfinite(equity) or equity <= 0:\n",
        "        return 0\n",
        "    if not np.isfinite(price) or price <= 0:\n",
        "        return 0\n",
        "\n",
        "    w = float(target_weight)\n",
        "    cap = float(globals().get(\"WEIGHT_CAP\", 1.0))\n",
        "    if cap > 0:\n",
        "        w = max(-cap, min(cap, w))\n",
        "    if not bool(globals().get(\"ALLOW_SHORTS\", False)):\n",
        "        w = max(0.0, w)\n",
        "\n",
        "    target_notional = equity * w\n",
        "\n",
        "    if target_notional >= 0:\n",
        "        qty = int(math.floor(target_notional / price))\n",
        "    else:\n",
        "        qty = int(math.ceil(target_notional / price))\n",
        "    return qty\n",
        "\n",
        "def rebalance_to_weight(api, symbol: str, equity: float, target_weight: float) -> dict:\n",
        "    price = get_last_price(api, symbol)\n",
        "    if not np.isfinite(price) or price <= 0:\n",
        "        logging.warning(f\"[{symbol}] Price unavailable; skipping rebalance.\")\n",
        "        return dict(NO_ORDER)\n",
        "\n",
        "    tradable, fractionable, shortable = _asset_flags(symbol)\n",
        "    if not tradable:\n",
        "        logging.info(f\"[{symbol}] Not tradable; skipping.\")\n",
        "        return dict(NO_ORDER)\n",
        "\n",
        "    use_fractionals = bool(USE_FRACTIONALS and fractionable)\n",
        "\n",
        "    have_qty = get_position_qty(api, symbol)\n",
        "    have_notional = have_qty * price\n",
        "    target_notional = equity * float(target_weight)\n",
        "    delta_notional = target_notional - have_notional\n",
        "\n",
        "    if have_qty > 0 and target_notional < 0:\n",
        "      logging.info(f\"[{symbol}] Flip long→short requested. Flattening long first (have_qty={have_qty}).\")\n",
        "      flatten_symbol(api, symbol)\n",
        "      return dict(NO_ORDER)  # next cycle will open the short cleanly\n",
        "\n",
        "    if abs(delta_notional) < 1e-9:\n",
        "        return dict(NO_ORDER)\n",
        "\n",
        "    delta_weight = abs(delta_notional) / max(float(equity), 1e-9)\n",
        "    if delta_weight < float(globals().get(\"DELTA_WEIGHT_MIN\", 0.0)):\n",
        "        return dict(NO_ORDER)\n",
        "\n",
        "    if use_fractionals:\n",
        "        dn = round_to_cents(abs(delta_notional))\n",
        "        if dn < float(globals().get(\"REBALANCE_MIN_NOTIONAL\", 0.0)):\n",
        "            return dict(NO_ORDER)\n",
        "\n",
        "        side = \"buy\" if delta_notional > 0 else \"sell\"\n",
        "        shorting = (target_notional < 0) and (side == \"sell\")\n",
        "        covering = (have_qty < 0) and (side == \"buy\")\n",
        "\n",
        "        if shorting:\n",
        "            if not shortable:\n",
        "                logging.info(f\"[{symbol}] Not shortable; skipping short rebalance.\")\n",
        "                return dict(NO_ORDER)\n",
        "            qty = max(1, int(math.floor(dn / price))) if price > 0 else 1\n",
        "            o = market_order_to_qty(api, symbol, side=\"sell\", qty=qty)\n",
        "            return _order_info(o)\n",
        "\n",
        "        if covering:\n",
        "            qty = max(1, int(math.ceil(dn / price))) if price > 0 else 1\n",
        "            qty = min(int(abs(have_qty)), qty) if have_qty < 0 else qty\n",
        "            o = market_order_to_qty(api, symbol, side=\"buy\", qty=qty)\n",
        "            return _order_info(o)\n",
        "\n",
        "        o = submit_fractional_rebalance(api, symbol, delta_notional=delta_notional, price=price)\n",
        "        return _order_info(o)\n",
        "\n",
        "    want_qty = compute_target_qty_by_cash(equity, price, target_weight, api)\n",
        "    delta_qty = want_qty - have_qty\n",
        "    if delta_qty == 0:\n",
        "        return dict(NO_ORDER)\n",
        "\n",
        "    approx_delta_notional = abs(delta_qty) * price\n",
        "    if equity > 0 and approx_delta_notional / equity < float(globals().get(\"DELTA_WEIGHT_MIN\", 0.0)):\n",
        "        return dict(NO_ORDER)\n",
        "    if approx_delta_notional < float(globals().get(\"REBALANCE_MIN_NOTIONAL\", 0.0)):\n",
        "        return dict(NO_ORDER)\n",
        "\n",
        "    side = \"buy\" if delta_qty > 0 else \"sell\"\n",
        "    shorting = (target_notional < 0) and (side == \"sell\")\n",
        "    if shorting and not shortable:\n",
        "        logging.info(f\"[{symbol}] Not shortable; skipping short rebalance.\")\n",
        "        return dict(NO_ORDER)\n",
        "\n",
        "    o = market_order_to_qty(api, symbol, side=side, qty=int(abs(delta_qty)))\n",
        "    return _order_info(o)\n",
        "\n",
        "def check_tp_sl_and_maybe_flatten(api, symbol: str) -> bool:\n",
        "    if TAKE_PROFIT_PCT <= 0 and STOP_LOSS_PCT <= 0:\n",
        "        return False\n",
        "    pos = get_position(api, symbol)\n",
        "    if not pos:\n",
        "        return False\n",
        "    try:\n",
        "        plpc = float(pos.unrealized_plpc)\n",
        "    except Exception:\n",
        "        return False\n",
        "    if TAKE_PROFIT_PCT > 0 and plpc >= TAKE_PROFIT_PCT:\n",
        "        logging.info(f\"[{symbol}] TP hit ({plpc:.4f} >= {TAKE_PROFIT_PCT:.4f}). Flattening.\")\n",
        "        flatten_symbol(api, symbol)\n",
        "        return True\n",
        "    if STOP_LOSS_PCT > 0 and plpc <= -abs(STOP_LOSS_PCT):\n",
        "        logging.info(f\"[{symbol}] SL hit ({plpc:.4f} <= {-abs(STOP_LOSS_PCT):.4f}). Flattening.\")\n",
        "        flatten_symbol(api, symbol)\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# ============================================================\n",
        "# Inference / obs building\n",
        "# ============================================================\n",
        "\n",
        "def expected_obs_shape(model, vecnorm) -> Optional[tuple]:\n",
        "    for src in (vecnorm, model):\n",
        "        try:\n",
        "            shp = tuple(src.observation_space.shape)\n",
        "            if shp:\n",
        "                return shp\n",
        "        except Exception:\n",
        "            pass\n",
        "    return None\n",
        "\n",
        "\n",
        "# ---- Feature alias resolution (training ↔ live parity) ----\n",
        "FEATURE_ALIASES = {\n",
        "    \"SMA_50\": \"Rolling_Mean_50\",\n",
        "    \"Rolling_Mean_50\": \"SMA_50\",\n",
        "}\n",
        "\n",
        "def resolve_feature_alias(name: str, df: pd.DataFrame) -> Optional[str]:\n",
        "    if name in df.columns:\n",
        "        return name\n",
        "    alt = FEATURE_ALIASES.get(name)\n",
        "    if alt and alt in df.columns:\n",
        "        return alt\n",
        "    return None\n",
        "\n",
        "\n",
        "def compute_art_feat_order(features_hint: Any, df: pd.DataFrame) -> List[str]:\n",
        "    if features_hint is None:\n",
        "        return [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
        "\n",
        "    feats = features_hint.get(\"features\", features_hint) if isinstance(features_hint, dict) else list(features_hint)\n",
        "    drop = {\"datetime\", \"symbol\", \"target\", \"return\"}\n",
        "\n",
        "    resolved = []\n",
        "    for f in feats:\n",
        "        if f in drop:\n",
        "            continue\n",
        "        col = resolve_feature_alias(f, df)\n",
        "        if col and pd.api.types.is_numeric_dtype(df[col]):\n",
        "            resolved.append(col)\n",
        "\n",
        "    return resolved\n",
        "\n",
        "def build_obs_from_row(row: pd.Series, order: List[str]) -> np.ndarray:\n",
        "    vals = []\n",
        "    for c in order:\n",
        "        v = row.get(c, np.nan)\n",
        "        vals.append(0.0 if (pd.isna(v) or v is None or v is False) else float(v))\n",
        "    return np.array(vals, dtype=np.float32)\n",
        "\n",
        "def _pick_columns_for_channels(features_hint: Any, df: pd.DataFrame, channels: int) -> List[str]:\n",
        "    numeric = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
        "    cols: List[str] = []\n",
        "    if isinstance(features_hint, dict) and \"features\" in features_hint:\n",
        "        cand = [c for c in features_hint[\"features\"] if c in df.columns and pd.api.types.is_numeric_dtype(df[c])]\n",
        "        if len(cand) >= channels:\n",
        "            cols = cand[:channels]\n",
        "    if not cols:\n",
        "        pref = [\"Close\", \"Volume\", \"Adj Close\", \"Open\", \"High\", \"Low\"]\n",
        "        cols = [c for c in pref if c in numeric]\n",
        "        cols += [c for c in numeric if c not in cols]\n",
        "        cols = cols[:channels]\n",
        "    if len(cols) < channels and cols:\n",
        "        while len(cols) < channels:\n",
        "            cols.append(cols[-1])\n",
        "    return cols[:channels]\n",
        "\n",
        "def add_regime(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df[\"Vol20\"] = df[\"Close\"].pct_change().rolling(20).std()\n",
        "    df[\"Ret20\"] = df[\"Close\"].pct_change(20)\n",
        "    vol_hi = (df[\"Vol20\"] > df[\"Vol20\"].median()).astype(int)\n",
        "    trend_hi = (df[\"Ret20\"].abs() > df[\"Ret20\"].abs().median()).astype(int)\n",
        "    df[\"Regime4\"] = vol_hi * 2 + trend_hi\n",
        "    return df\n",
        "\n",
        "def denoise_wavelet(series: pd.Series, wavelet: str = \"db1\", level: int = 2) -> pd.Series:\n",
        "    try:\n",
        "        import pywt\n",
        "    except Exception:\n",
        "        return pd.Series(series).astype(float).ffill().bfill().ewm(span=5, adjust=False).mean()\n",
        "\n",
        "    s = pd.Series(series).astype(float).ffill().bfill()\n",
        "    arr = s.to_numpy()\n",
        "    try:\n",
        "        w = pywt.Wavelet(wavelet)\n",
        "        maxlvl = pywt.dwt_max_level(len(arr), w.dec_len)\n",
        "        lvl = int(max(0, min(level, maxlvl)))\n",
        "        if lvl < 1:\n",
        "            return s\n",
        "        coeffs = pywt.wavedec(arr, w, mode=\"symmetric\", level=lvl)\n",
        "        for i in range(1, len(coeffs)):\n",
        "            coeffs[i] = np.zeros_like(coeffs[i])\n",
        "        rec = pywt.waverec(coeffs, w, mode=\"symmetric\")\n",
        "        return pd.Series(rec[:len(arr)], index=s.index)\n",
        "    except Exception:\n",
        "        return s.ewm(span=5, adjust=False).mean()\n",
        "\n",
        "def add_features_live(\n",
        "    df: pd.DataFrame,\n",
        "    use_sentiment: bool = False,\n",
        "    rsi_wilder: bool = True,\n",
        "    atr_wilder: bool = True,\n",
        ") -> pd.DataFrame:\n",
        "    df = df.copy().sort_index()\n",
        "    cols_ci = {c.lower(): c for c in df.columns}\n",
        "    rename = {}\n",
        "    for final, alts in {\n",
        "        \"Open\": [\"open\"],\n",
        "        \"High\": [\"high\"],\n",
        "        \"Low\": [\"low\"],\n",
        "        \"Close\": [\"close\", \"close*\", \"last\"],\n",
        "        \"Adj Close\": [\"adj close\", \"adj_close\", \"adjclose\", \"adjusted close\"],\n",
        "        \"Volume\": [\"volume\", \"vol\"],\n",
        "    }.items():\n",
        "        for a in [final.lower()] + alts:\n",
        "            if a in cols_ci:\n",
        "                rename[cols_ci[a]] = final\n",
        "                break\n",
        "    df = df.rename(columns=rename)\n",
        "    if \"Adj Close\" not in df.columns and \"Close\" in df.columns:\n",
        "        df[\"Adj Close\"] = df[\"Close\"]\n",
        "\n",
        "    df[\"SMA_20\"] = df[\"Close\"].rolling(20).mean()\n",
        "    df[\"STD_20\"] = df[\"Close\"].rolling(20).std()\n",
        "    df[\"Upper_Band\"] = df[\"SMA_20\"] + 2 * df[\"STD_20\"]\n",
        "    df[\"Lower_Band\"] = df[\"SMA_20\"] - 2 * df[\"STD_20\"]\n",
        "\n",
        "    df[\"Lowest_Low\"] = df[\"Low\"].rolling(14).min()\n",
        "    df[\"Highest_High\"] = df[\"High\"].rolling(14).max()\n",
        "    denom = (df[\"Highest_High\"] - df[\"Lowest_Low\"]).replace(0, np.nan)\n",
        "    df[\"Stoch\"] = ((df[\"Close\"] - df[\"Lowest_Low\"]) / denom) * 100\n",
        "\n",
        "    df[\"ROC\"] = df[\"Close\"].pct_change(10)\n",
        "    sign = np.sign(df[\"Close\"].diff().fillna(0))\n",
        "    df[\"OBV\"] = (sign * df[\"Volume\"].fillna(0)).cumsum()\n",
        "\n",
        "    tp = (df[\"High\"] + df[\"Low\"] + df[\"Close\"]) / 3.0\n",
        "    sma_tp = tp.rolling(20).mean()\n",
        "    md = (tp - sma_tp).abs().rolling(20).mean().replace(0, np.nan)\n",
        "    df[\"CCI\"] = (tp - sma_tp) / (0.015 * md)\n",
        "\n",
        "    df[\"EMA_10\"] = df[\"Close\"].ewm(span=10, adjust=False).mean()\n",
        "    df[\"EMA_50\"] = df[\"Close\"].ewm(span=50, adjust=False).mean()\n",
        "    ema12 = df[\"Close\"].ewm(span=12, adjust=False).mean()\n",
        "    ema26 = df[\"Close\"].ewm(span=26, adjust=False).mean()\n",
        "    df[\"MACD_Line\"] = ema12 - ema26\n",
        "    df[\"MACD_Signal\"] = df[\"MACD_Line\"].ewm(span=9, adjust=False).mean()\n",
        "\n",
        "    d = df[\"Close\"].diff()\n",
        "    gain = d.clip(lower=0)\n",
        "    loss = (-d.clip(upper=0))\n",
        "    if rsi_wilder:\n",
        "        avg_gain = gain.ewm(alpha=1 / 14, adjust=False).mean()\n",
        "        avg_loss = loss.ewm(alpha=1 / 14, adjust=False).mean()\n",
        "    else:\n",
        "        avg_gain = gain.rolling(14).mean()\n",
        "        avg_loss = loss.rolling(14).mean()\n",
        "    rs = avg_gain / avg_loss.replace(0, np.nan)\n",
        "    df[\"RSI\"] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    tr = pd.concat([\n",
        "        (df[\"High\"] - df[\"Low\"]),\n",
        "        (df[\"High\"] - df[\"Close\"].shift()).abs(),\n",
        "        (df[\"Low\"] - df[\"Close\"].shift()).abs(),\n",
        "    ], axis=1).max(axis=1)\n",
        "    df[\"ATR\"] = tr.ewm(alpha=1 / 14, adjust=False).mean() if atr_wilder else tr.rolling(14).mean()\n",
        "\n",
        "    df[\"Volatility\"] = df[\"Close\"].pct_change().rolling(20).std()\n",
        "    df[\"Denoised_Close\"] = denoise_wavelet(df[\"Close\"])\n",
        "\n",
        "    df = add_regime(df)\n",
        "    df[\"SentimentScore\"] = (df.get(\"SentimentScore\", 0.0) if use_sentiment else 0.0)\n",
        "    df[\"Delta\"] = df[\"Close\"].pct_change(1).fillna(0.0)\n",
        "    df[\"Gamma\"] = df[\"Delta\"].diff().fillna(0.0)\n",
        "\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    return df\n",
        "\n",
        "def prepare_observation_from_bars(\n",
        "    bars_df: pd.DataFrame,\n",
        "    features_hint: Any = None,\n",
        "    min_required_rows: int = 60,\n",
        "    expected_shape: Optional[tuple] = None,\n",
        "    symbol: str = \"\",\n",
        ") -> Tuple[np.ndarray, int]:\n",
        "\n",
        "    feats_df = add_features_live(bars_df).replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    ts = ensure_utc(pd.Timestamp.utcnow())\n",
        "\n",
        "    if not feats_df.empty:\n",
        "        try:\n",
        "            ts = ensure_utc(feats_df.index[-1])\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    obs_ts = int(ts.timestamp())\n",
        "\n",
        "    if expected_shape is not None:\n",
        "        if len(expected_shape) == 2:\n",
        "            lookback, channels = int(expected_shape[0]), int(expected_shape[1])\n",
        "            cols = _pick_columns_for_channels(features_hint, feats_df, channels)\n",
        "            window_df = feats_df[cols].tail(lookback).fillna(0.0)\n",
        "            arr = window_df.to_numpy(dtype=np.float32)\n",
        "            if arr.shape[0] < lookback:\n",
        "                pad_rows = lookback - arr.shape[0]\n",
        "                arr = np.vstack([np.zeros((pad_rows, channels), dtype=np.float32), arr])\n",
        "            arr = arr[-lookback:, :channels]\n",
        "            return arr.reshape(lookback, channels), obs_ts\n",
        "\n",
        "        elif len(expected_shape) == 1:\n",
        "            n = int(expected_shape[0])\n",
        "            cand = compute_art_feat_order(features_hint, feats_df)\n",
        "            if len(feats_df) < max(20, min_required_rows):\n",
        "                raise ValueError(f\"Not enough bars to compute features robustly (have {len(feats_df)}).\")\n",
        "            last = feats_df.iloc[-1]\n",
        "            vals = []\n",
        "            for c in cand[:n]:\n",
        "                v = last.get(c, np.nan)\n",
        "                vals.append(0.0 if (pd.isna(v) or v is None) else float(v))\n",
        "            if len(vals) < n:\n",
        "                vals += [0.0] * (n - len(vals))\n",
        "            return np.asarray(vals, dtype=np.float32), obs_ts\n",
        "\n",
        "    order = compute_art_feat_order(features_hint, feats_df)\n",
        "    missing = []\n",
        "    if isinstance(features_hint, dict) and \"features\" in features_hint:\n",
        "        missing = [c for c in features_hint[\"features\"] if c not in feats_df.columns]\n",
        "\n",
        "    logging.info(\"[%s] features_used=%d missing_from_live=%d\", symbol, len(order), len(missing))\n",
        "    if missing:\n",
        "        logging.debug(\"[%s] missing examples: %s\", symbol, missing[:20])\n",
        "\n",
        "    if not order:\n",
        "        raise ValueError(\"No usable features after resolving artifact order.\")\n",
        "    feats_df = feats_df.dropna(subset=order)\n",
        "    if len(feats_df) < max(20, min_required_rows):\n",
        "        raise ValueError(f\"Not enough bars to compute features robustly (have {len(feats_df)}).\")\n",
        "\n",
        "    last = feats_df.iloc[-1]\n",
        "    obs = build_obs_from_row(last, order)\n",
        "    return obs.astype(np.float32), obs_ts\n",
        "\n",
        "# ============================================================\n",
        "# Live loop helpers\n",
        "# ============================================================\n",
        "\n",
        "def ensure_market_open(api) -> bool:\n",
        "    try:\n",
        "        return bool(api.get_clock().is_open)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def _sleep_until_open(api):\n",
        "    try:\n",
        "        clock = api.get_clock()\n",
        "        if getattr(clock, \"is_open\", False):\n",
        "            return\n",
        "        nxt = pd.to_datetime(getattr(clock, \"next_open\"), utc=True, errors=\"coerce\")\n",
        "        if pd.isna(nxt):\n",
        "            time.sleep(60)\n",
        "            return\n",
        "        wait = max(1, int((nxt - now_utc()).total_seconds()))\n",
        "        logging.info(\"Market closed. Sleeping %ds until next open.\", wait)\n",
        "        time.sleep(wait)\n",
        "    except Exception:\n",
        "        time.sleep(60)\n",
        "\n",
        "def write_account_info_to_run_config(api) -> None:\n",
        "    try:\n",
        "        acct = api.get_account()\n",
        "        acct_info = {\n",
        "            \"account_id\": getattr(acct, \"id\", \"\"),\n",
        "            \"status\": getattr(acct, \"status\", \"\"),\n",
        "            \"equity\": getattr(acct, \"equity\", \"\"),\n",
        "            \"cash\": getattr(acct, \"cash\", \"\"),\n",
        "            \"pattern_day_trader\": getattr(acct, \"pattern_day_trader\", \"\"),\n",
        "        }\n",
        "\n",
        "        cfg_path = RESULTS_DIR / \"run_config.json\"\n",
        "        try:\n",
        "            meta = json.loads(cfg_path.read_text()) if cfg_path.exists() else {}\n",
        "        except Exception:\n",
        "            meta = {}\n",
        "\n",
        "        meta[\"alpaca_account\"] = acct_info\n",
        "        tmp = cfg_path.with_suffix(\".tmp\")\n",
        "        tmp.write_text(json.dumps(meta, indent=2))\n",
        "        tmp.replace(cfg_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.warning(\"Could not augment run_config.json with account info: %s\", e)\n",
        "\n",
        "def action_to_weight(action) -> Tuple[float, float, float]:\n",
        "    a = float(np.asarray(action).reshape(-1)[0])\n",
        "    raw = a\n",
        "\n",
        "    cap = float(globals().get(\"WEIGHT_CAP\", 0.35))\n",
        "    target_w = float(np.clip(a, -1, 1)) * cap\n",
        "    conf = float(min(1.0, abs(a)))\n",
        "\n",
        "    if not bool(globals().get(\"ALLOW_SHORTS\", False)):\n",
        "        target_w = max(0.0, target_w)\n",
        "\n",
        "    if str(globals().get(\"SIZING_MODE\", \"linear\")).lower() == \"threshold\":\n",
        "        floor = float(globals().get(\"CONF_FLOOR\", 0.15))\n",
        "        if conf < floor:\n",
        "            target_w = 0.0\n",
        "        else:\n",
        "            scale = (conf - floor) / max(1e-9, (1.0 - floor))\n",
        "            target_w = np.sign(target_w) * cap * float(np.clip(scale, 0, 1))\n",
        "\n",
        "    return float(target_w), float(conf), float(raw)\n",
        "\n",
        "def infer_target_weight(model: PPO, vecnorm: Optional[VecNormalize], obs: np.ndarray) -> Tuple[float, float, float]:\n",
        "    x = np.asarray(obs, dtype=np.float32)\n",
        "\n",
        "    if vecnorm is not None and hasattr(vecnorm, \"normalize_obs\") and getattr(vecnorm, \"obs_rms\", None) is not None:\n",
        "        try:\n",
        "            x = vecnorm.normalize_obs(x)\n",
        "        except Exception:\n",
        "            try:\n",
        "                x = vecnorm.normalize_obs(np.expand_dims(x, axis=0))[0]\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    try:\n",
        "        action, _ = model.predict(x, deterministic=INF_DETERMINISTIC)\n",
        "    except Exception:\n",
        "        action, _ = model.predict(np.expand_dims(x, axis=0), deterministic=INF_DETERMINISTIC)\n",
        "        if isinstance(action, (list, np.ndarray)):\n",
        "            action = np.asarray(action)\n",
        "            if action.ndim > 0:\n",
        "                action = action[0]\n",
        "\n",
        "    return action_to_weight(action)\n",
        "\n",
        "def maybe_patch_stale_with_latest_trade(api, symbol: str, bars_df: pd.DataFrame, max_age_sec: int = None) -> pd.DataFrame:\n",
        "    if bars_df.empty:\n",
        "        return bars_df\n",
        "    max_age_sec = max_age_sec or int(globals().get(\"STALE_MAX_SEC\", 600))\n",
        "    try:\n",
        "        last_ts = pd.Timestamp(bars_df.index[-1])\n",
        "        last_ts = last_ts.tz_convert(\"UTC\") if last_ts.tzinfo else last_ts.tz_localize(\"UTC\")\n",
        "        age_sec = int((now_utc() - last_ts).total_seconds())\n",
        "        if age_sec <= max_age_sec:\n",
        "            return bars_df\n",
        "\n",
        "        lt = api.get_latest_trade(symbol)\n",
        "        price = float(getattr(lt, \"price\", getattr(lt, \"p\", float(\"nan\"))))\n",
        "        ts = pd.to_datetime(getattr(lt, \"timestamp\", getattr(lt, \"t\", None)), utc=True)\n",
        "        if not (pd.notna(ts) and np.isfinite(price)):\n",
        "            return bars_df\n",
        "\n",
        "        lt_age = int((now_utc() - ts).total_seconds())\n",
        "        if lt_age > max_age_sec:\n",
        "            return bars_df\n",
        "\n",
        "        synth_time = max(last_ts + pd.Timedelta(minutes=1), ts.floor(\"min\"))\n",
        "        row = pd.DataFrame(\n",
        "            {\"Open\": [price], \"High\": [price], \"Low\": [price], \"Close\": [price], \"Volume\": [0.0]},\n",
        "            index=pd.DatetimeIndex([synth_time], tz=\"UTC\")\n",
        "        )\n",
        "        patched = pd.concat([bars_df, row]).sort_index()\n",
        "        patched = patched[~patched.index.duplicated(keep=\"last\")]\n",
        "        logging.info(f\"[{symbol}] Patched stale bars with synthetic trade bar @ {synth_time.isoformat()} px={price:.2f}\")\n",
        "        return patched\n",
        "    except Exception as e:\n",
        "        logging.debug(f\"[{symbol}] maybe_patch_stale_with_latest_trade failed: {e}\")\n",
        "        return bars_df\n",
        "\n",
        "def run_live_once_for_symbol(\n",
        "    api,\n",
        "    symbol: str,\n",
        "    model: PPO,\n",
        "    vecnorm: Optional[VecNormalize],\n",
        "    features_hint: Optional[dict] = None,\n",
        "    cycle_equity: Optional[float] = None,\n",
        "):\n",
        "    shape = expected_obs_shape(model, vecnorm)\n",
        "    lookback = int(shape[0]) if (shape and len(shape) == 2) else None\n",
        "    bars_need = max(200, (lookback or 0) * 3)\n",
        "\n",
        "    bars_df = get_recent_bars(api, symbol, limit=bars_need, timeframe=LIVE_TIMEFRAME)\n",
        "    if bars_df is None or bars_df.empty:\n",
        "        logging.warning(\"[%s] No recent bars; skipping.\", symbol)\n",
        "        return\n",
        "\n",
        "    bars_df = maybe_patch_stale_with_latest_trade(api, symbol, bars_df)\n",
        "\n",
        "    block_until = _REENTRY_BLOCK_UNTIL.get(symbol, 0.0)\n",
        "    if time.time() < block_until:\n",
        "        remaining = int(max(0, block_until - time.time()))\n",
        "        logging.info(f\"[{symbol}] Re-entry cooldown active ({remaining}s left); skipping.\")\n",
        "        try:\n",
        "            eq = float(cycle_equity) if cycle_equity is not None else float(get_account_equity(api))\n",
        "        except Exception:\n",
        "            eq = float(\"nan\")\n",
        "        try:\n",
        "            px = float(bars_df[\"Close\"].iloc[-1]) if not bars_df.empty else float(get_last_price(api, symbol))\n",
        "        except Exception:\n",
        "            px = float(\"nan\")\n",
        "\n",
        "        _NO_POS_CYCLE_COUNT[symbol] = 0\n",
        "        log_trade_symbol(\n",
        "            symbol,\n",
        "            bars_df.index[-1] if not bars_df.empty else pd.NaT,\n",
        "            signal=0,\n",
        "            raw_action=0.0,\n",
        "            weight=0.0,\n",
        "            confidence=0.0,\n",
        "            price=px,\n",
        "            equity=eq,\n",
        "            dry_run=DRY_RUN,\n",
        "            note=\"reentry_cooldown\",\n",
        "        )\n",
        "        return\n",
        "\n",
        "    min_rows_needed = max(20, int(shape[0]) if (shape and len(shape) == 2) else 60)\n",
        "    try:\n",
        "        obs, obs_ts = prepare_observation_from_bars(\n",
        "            bars_df,\n",
        "            features_hint=features_hint,\n",
        "            min_required_rows=min_rows_needed,\n",
        "            expected_shape=shape,\n",
        "            symbol=symbol,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logging.info(\"[%s] Could not prepare observation (%s); skipping.\", symbol, e)\n",
        "        try:\n",
        "            eq = float(cycle_equity) if cycle_equity is not None else float(get_account_equity(api))\n",
        "        except Exception:\n",
        "            eq = float(\"nan\")\n",
        "        try:\n",
        "            px = float(bars_df[\"Close\"].iloc[-1]) if not bars_df.empty else float(get_last_price(api, symbol))\n",
        "        except Exception:\n",
        "            px = float(\"nan\")\n",
        "        log_trade_symbol(\n",
        "            symbol,\n",
        "            bars_df.index[-1] if not bars_df.empty else pd.NaT,\n",
        "            signal=0,\n",
        "            raw_action=0.0,\n",
        "            weight=0.0,\n",
        "            confidence=0.0,\n",
        "            price=px,\n",
        "            equity=eq,\n",
        "            dry_run=DRY_RUN,\n",
        "            note=\"obs_build_failed\",\n",
        "        )\n",
        "        return\n",
        "\n",
        "    _obs_shape = getattr(obs, \"shape\", None)\n",
        "    _vecnorm_str = (\n",
        "        f\"{type(vecnorm).__name__}(training={getattr(vecnorm,'training',None)}, norm_reward={getattr(vecnorm,'norm_reward',None)})\"\n",
        "    ) if vecnorm is not None else \"None\"\n",
        "    _now_ts = utc_ts(now_utc())\n",
        "    _age = _now_ts - int(obs_ts)\n",
        "    logging.info(\"[%s] obs_shape=%s | exp_shape=%s | age=%ss | vecnorm=%s\",\n",
        "                 symbol, _obs_shape, shape, _age, _vecnorm_str)\n",
        "    #shape check + predict heartbeat\n",
        "    exp = expected_obs_shape(model, vecnorm)\n",
        "    if exp is not None and hasattr(obs, \"shape\"):\n",
        "        logging.info(\"[%s] shape_check obs=%s expected=%s\", symbol, getattr(obs, \"shape\", None), exp)\n",
        "    logging.info(\"[%s]  obs built. Calling model.predict()\", symbol)\n",
        "\n",
        "    if _now_ts - obs_ts >= STALE_MAX_SEC:\n",
        "        logging.info(f\"[{symbol}] Observation stale (age={_now_ts-obs_ts}s ≥ {STALE_MAX_SEC}s); skipping.\")\n",
        "        try:\n",
        "            eq = get_account_equity(api)\n",
        "            px = float(bars_df[\"Close\"].iloc[-1]) if not bars_df.empty else get_last_price(api, symbol)\n",
        "        except Exception:\n",
        "            eq, px = float(\"nan\"), float(\"nan\")\n",
        "        log_trade_symbol(symbol, bars_df.index[-1] if not bars_df.empty else pd.NaT,\n",
        "                         0, 0.0, 0.0, 0.0, px, eq, DRY_RUN, note=\"skip_stale\")\n",
        "        return\n",
        "\n",
        "    if check_tp_sl_and_maybe_flatten(api, symbol):\n",
        "        return\n",
        "\n",
        "    # ---- DIAGNOSTIC: obs stats (raw vs normalized) ----\n",
        "    x = np.asarray(obs, dtype=np.float32)\n",
        "    x2 = x.copy()\n",
        "    if vecnorm is not None and getattr(vecnorm, \"obs_rms\", None) is not None:\n",
        "        try:\n",
        "            x2 = vecnorm.normalize_obs(x2)\n",
        "        except Exception:\n",
        "            x2 = vecnorm.normalize_obs(np.expand_dims(x2, axis=0))[0]\n",
        "\n",
        "    logging.info(\n",
        "        \"[%s] obs stats raw: mean=%.4f std=%.4f | normed: mean=%.4f std=%.4f\",\n",
        "        symbol, float(x.mean()), float(x.std()),\n",
        "        float(np.asarray(x2).mean()), float(np.asarray(x2).std())\n",
        "    )\n",
        "\n",
        "    # IMPORTANT: infer_target_weight() already applies vecnorm normalization internally\n",
        "    target_w, conf, raw = infer_target_weight(model, vecnorm, obs)\n",
        "    logging.info(\"[%s] predict() ok → raw=%.4f target_w=%.4f conf=%.3f\", symbol, raw, target_w, conf)\n",
        "    eq = float(cycle_equity) if cycle_equity is not None else get_account_equity(api)\n",
        "    px = float(bars_df[\"Close\"].iloc[-1]) if not bars_df.empty else get_last_price(api, symbol)\n",
        "    have = get_position_qty(api, symbol)\n",
        "\n",
        "    #FORCE_FIRST_BUY: one-time seed entry if no position exists\n",
        "    if FORCE_FIRST_BUY and (have == 0) and (not _FORCED_FIRST_BUY_DONE.get(symbol, False)):\n",
        "        if ensure_market_open(api):\n",
        "            if begin_order_event(symbol, _SEED_COOLDOWN_SEC):\n",
        "                tradable, fractionable, shortable = _asset_flags(symbol)\n",
        "                seed_notional = round_to_cents(float(globals().get(\"REBALANCE_MIN_NOTIONAL\", 5.00)))\n",
        "\n",
        "                # seed direction = model direction (not always BUY)\n",
        "                if target_w < 0:\n",
        "                    # seed short only if allowed & shortable\n",
        "                    if not ALLOW_SHORTS or not shortable:\n",
        "                        log_trade_symbol(\n",
        "                            symbol, bars_df.index[-1], 0, raw, target_w, conf, px, eq, DRY_RUN,\n",
        "                            note=\"force_first_seed_short_blocked\"\n",
        "                        )\n",
        "                        _FORCED_FIRST_BUY_DONE[symbol] = True\n",
        "                        return\n",
        "\n",
        "                    # short seed uses 1 share (fractional short not supported here safely)\n",
        "                    o = market_order_to_qty(api, symbol, side=\"sell\", qty=1)\n",
        "\n",
        "                else:\n",
        "                    # long seed\n",
        "                    if USE_FRACTIONALS and fractionable:\n",
        "                        o = market_order(api, symbol, side=\"buy\", notional=seed_notional)\n",
        "                    else:\n",
        "                        o = market_order_to_qty(api, symbol, side=\"buy\", qty=1)\n",
        "\n",
        "                stamp_order_event(symbol)\n",
        "                _FORCED_FIRST_BUY_DONE[symbol] = True\n",
        "\n",
        "                log_trade_symbol(\n",
        "                    symbol=symbol,\n",
        "                    bar_time=bars_df.index[-1],\n",
        "                    signal=1,\n",
        "                    raw_action=raw,\n",
        "                    weight=target_w,\n",
        "                    confidence=conf,\n",
        "                    price=px,\n",
        "                    equity=eq,\n",
        "                    dry_run=DRY_RUN,\n",
        "                    note=\"force_first_seed_directional\",\n",
        "                    **_order_info(o),\n",
        "                )\n",
        "                return\n",
        "\n",
        "        logging.info(f\"[{symbol}] raw={raw:.4f} conf={conf:.3f} → target_w={target_w:.4f} px=${px:.2f} eq=${eq:,.2f} have={have}\")\n",
        "\n",
        "        if os.getenv(\"DEBUG_FORCE_SEED_IF_IDLE\", \"0\").lower() in (\"1\", \"true\", \"yes\"):\n",
        "            if have != 0:\n",
        "                _NO_POS_CYCLE_COUNT[symbol] = 0\n",
        "            else:\n",
        "                _NO_POS_CYCLE_COUNT[symbol] = _NO_POS_CYCLE_COUNT.get(symbol, 0) + 1\n",
        "\n",
        "            idle_cycles = int(os.getenv(\"DEBUG_SEED_IDLE_CYCLES\", \"10\"))\n",
        "            if have == 0 and _NO_POS_CYCLE_COUNT[symbol] >= idle_cycles and ensure_market_open(api):\n",
        "                tradable, fractionable, _ = _asset_flags(symbol)\n",
        "                if not tradable:\n",
        "                    log_trade_symbol(symbol, bars_df.index[-1], 0, raw, target_w, conf, px, eq, DRY_RUN, note=\"not_tradable_seed\")\n",
        "                    return\n",
        "                seed_amt = round_to_cents(REBALANCE_MIN_NOTIONAL)\n",
        "                if USE_FRACTIONALS and fractionable:\n",
        "                    market_order(api, symbol, side=\"buy\", notional=seed_amt)\n",
        "                else:\n",
        "                    market_order_to_qty(api, symbol, side=\"buy\", qty=1)\n",
        "                log_trade_symbol(symbol, bars_df.index[-1], 1, raw, target_w, conf, px, eq, DRY_RUN, note=\"debug_force_seed\")\n",
        "                return\n",
        "\n",
        "    RAW_POS_MIN_LOCAL = float(globals().get(\"RAW_POS_MIN\", 0.0))\n",
        "    if target_w > 0 and raw < RAW_POS_MIN_LOCAL:\n",
        "        logging.info(f\"[{symbol}] Raw {raw:.4f} < RAW_POS_MIN {RAW_POS_MIN_LOCAL:.4f}; no action.\")\n",
        "        log_trade_symbol(symbol, bars_df.index[-1], 0, raw, target_w, conf, px, eq, DRY_RUN, note=\"raw_gate_long\")\n",
        "        return\n",
        "\n",
        "    RAW_NEG_GATE = float(globals().get(\"RAW_NEG_MAX\", 0.0))\n",
        "    if target_w < 0 and abs(raw) < RAW_NEG_GATE:\n",
        "        logging.info(f\"[{symbol}] |raw| {abs(raw):.4f} < RAW_NEG_GATE {RAW_NEG_GATE:.4f}; no action.\")\n",
        "        log_trade_symbol(symbol, bars_df.index[-1], 0, raw, target_w, conf, px, eq, DRY_RUN, note=\"raw_gate_short\")\n",
        "        return\n",
        "\n",
        "    pos = get_position(api, symbol)\n",
        "    if abs(target_w) <= EXIT_WEIGHT_MAX and pos:\n",
        "        logging.info(f\"[{symbol}] Model near-flat (≤{EXIT_WEIGHT_MAX:.3f}); flattening.\")\n",
        "        flatten_symbol(api, symbol)\n",
        "        log_trade_symbol(symbol, bars_df.index[-1], int(target_w > 0), raw, target_w, conf, px, eq, DRY_RUN, note=\"flatten\")\n",
        "        return\n",
        "\n",
        "    if conf < ENTER_CONF_MIN and abs(target_w) <= EXIT_WEIGHT_MAX:\n",
        "        logging.info(f\"[{symbol}] Below conf/near-flat gates; no action.\")\n",
        "        log_trade_symbol(symbol, bars_df.index[-1], int(target_w > 0), raw, target_w, conf, px, eq, DRY_RUN, note=\"no_action\")\n",
        "        return\n",
        "\n",
        "    wants_trade = (abs(target_w) >= ENTER_WEIGHT_MIN and conf >= ENTER_CONF_MIN)\n",
        "    has_pos = (have != 0)\n",
        "\n",
        "    if wants_trade:\n",
        "        event_gap = _SEED_COOLDOWN_SEC if (SEED_FIRST_SHARE and not has_pos) else 30\n",
        "        if not begin_order_event(symbol, event_gap):\n",
        "            note = \"order_event_cooldown_seed\" if (SEED_FIRST_SHARE and not has_pos) else \"order_event_cooldown_rebalance\"\n",
        "            log_trade_symbol(symbol, bars_df.index[-1], 0, raw, target_w, conf, px, eq, DRY_RUN, note=note)\n",
        "            return\n",
        "\n",
        "        tradable, fractionable, _shortable = _asset_flags(symbol)\n",
        "        if not tradable:\n",
        "            log_trade_symbol(symbol, bars_df.index[-1], 0, raw, target_w, conf, px, eq, DRY_RUN, note=\"not_tradable\")\n",
        "            return\n",
        "\n",
        "        seeded = False\n",
        "        if SEED_FIRST_SHARE and not has_pos:\n",
        "            seed_notional = round_to_cents(REBALANCE_MIN_NOTIONAL)\n",
        "            side = \"buy\" if target_w > 0 else \"sell\"\n",
        "\n",
        "            if side == \"sell\":\n",
        "                ok, why = _can_seed_short(api, symbol)\n",
        "                if not ok:\n",
        "                    log_trade_symbol(symbol, bars_df.index[-1], 0, raw, target_w, conf, px, eq, DRY_RUN, note=why)\n",
        "                    return\n",
        "                _ = market_order_to_qty(api, symbol, side=\"sell\", qty=1)\n",
        "                seeded = True\n",
        "            else:\n",
        "                if USE_FRACTIONALS and fractionable:\n",
        "                    _ = market_order(api, symbol, side=\"buy\", notional=seed_notional)\n",
        "                else:\n",
        "                    _ = market_order_to_qty(api, symbol, side=\"buy\", qty=1)\n",
        "                seeded = True\n",
        "\n",
        "        order_info = rebalance_to_weight(api, symbol, eq, target_w) or dict(NO_ORDER)\n",
        "\n",
        "        if seeded or int(order_info.get(\"order_submitted\", 0)) == 1 or DRY_RUN:\n",
        "            stamp_order_event(symbol)\n",
        "\n",
        "        note = \"seed+rebalance\" if seeded else \"rebalance_only\"\n",
        "        log_trade_symbol(\n",
        "            symbol, bars_df.index[-1],\n",
        "            int(target_w > 0),\n",
        "            raw, target_w, conf, px, eq, DRY_RUN,\n",
        "            note=note,\n",
        "            order_submitted=order_info.get(\"order_submitted\", 0),\n",
        "            order_id=order_info.get(\"order_id\", \"\"),\n",
        "            order_status=order_info.get(\"order_status\", \"\"),\n",
        "            filled_qty=order_info.get(\"filled_qty\", \"\"),\n",
        "        )\n",
        "        return\n",
        "\n",
        "# ============================================================\n",
        "# Live runner\n",
        "# ============================================================\n",
        "\n",
        "def run_live(tickers: List[str], api: tradeapi.REST):\n",
        "    def minutes_to_close(api: tradeapi.REST) -> Optional[int]:\n",
        "        clk = api.get_clock()\n",
        "        if getattr(clk, \"is_open\", False):\n",
        "            close = pd.to_datetime(clk.next_close, utc=True)\n",
        "            return int(max(0, (close - now_utc()).total_seconds() // 60))\n",
        "        return None\n",
        "\n",
        "    api_local = api\n",
        "    per_ticker: Dict[str, Tuple[PPO, Optional[VecNormalize], Optional[dict]]] = {}\n",
        "    best = (globals().get(\"BEST_WINDOW_ENV\") or None)\n",
        "\n",
        "    for t in tickers:\n",
        "        try:\n",
        "            picks = pick_artifacts_for_ticker(t, os.getenv(\"ARTIFACTS_DIR\", str(ARTIFACTS_DIR)), best_window=best)\n",
        "            model = load_ppo_model(picks[\"model\"])\n",
        "            vecnorm = load_vecnormalize(picks.get(\"vecnorm\"))\n",
        "            if vecnorm is not None and hasattr(vecnorm, \"training\"):\n",
        "                vecnorm.training = False\n",
        "            if vecnorm is not None and hasattr(vecnorm, \"norm_reward\"):\n",
        "                vecnorm.norm_reward = False\n",
        "            feats = load_features(picks.get(\"features\"))\n",
        "            per_ticker[t] = (model, vecnorm, feats)\n",
        "            logging.info(\"[%s] Artifacts loaded and ready.\", t)\n",
        "        except Exception as e:\n",
        "            logging.exception(\"[%s] Failed to load artifacts: %s\", t, e)\n",
        "\n",
        "    if not per_ticker:\n",
        "        raise RuntimeError(\"No models loaded for any ticker. Check artifacts directory and names.\")\n",
        "\n",
        "    loaded_syms = list(per_ticker.keys())\n",
        "    logging.info(\"Starting live execution for (loaded): %s\", loaded_syms)\n",
        "\n",
        "    global _last_kill_ts\n",
        "\n",
        "    def run_symbol_step_safe(\n",
        "        symbol: str,\n",
        "        model: PPO,\n",
        "        vecnorm: Optional[VecNormalize],\n",
        "        feat_hint: Optional[dict],\n",
        "        cycle_equity: float,\n",
        "        timeout_sec: int = 15,\n",
        "    ):\n",
        "        return _call_with_timeout(\n",
        "            run_live_once_for_symbol,\n",
        "            timeout_sec,\n",
        "            api_local,\n",
        "            symbol,\n",
        "            model,\n",
        "            vecnorm,\n",
        "            feat_hint,\n",
        "            cycle_equity,\n",
        "        )\n",
        "\n",
        "    cycle = 0\n",
        "    last_plot_ts = 0\n",
        "    flattened_today = False\n",
        "\n",
        "    logging.info(\"Starting live trading loop\")\n",
        "    try:\n",
        "        while True:\n",
        "            if not ensure_market_open(api_local):\n",
        "                flattened_today = False\n",
        "                globals()[\"SESSION_OPEN_EQUITY\"] = None\n",
        "                _sleep_until_open(api_local)\n",
        "                continue\n",
        "\n",
        "            if globals().get(\"SESSION_OPEN_EQUITY\") is None:\n",
        "                try:\n",
        "                    globals()[\"SESSION_OPEN_EQUITY\"] = float(api_local.get_account().equity)\n",
        "                    logging.info(\n",
        "                        \"Session open equity anchor set: %.2f\",\n",
        "                        globals()[\"SESSION_OPEN_EQUITY\"],\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    logging.debug(\"Could not set SESSION_OPEN_EQUITY: %s\", e)\n",
        "\n",
        "            t_cycle_start = time.perf_counter()\n",
        "\n",
        "            try:\n",
        "                cycle_equity = float(api_local.get_account().equity)\n",
        "            except Exception as e:\n",
        "                logging.warning(\"Could not fetch equity: %s\", e)\n",
        "                cycle_equity = float(\"nan\")\n",
        "\n",
        "            print(\n",
        "                f\"[HEARTBEAT] {utcnow_iso()} cycle={cycle} equity={cycle_equity:,.2f}\",\n",
        "                flush=True,\n",
        "            )\n",
        "\n",
        "            for sym, (model, vecnorm, feat_hint) in per_ticker.items():\n",
        "                t_sym_start = time.perf_counter()\n",
        "                try:\n",
        "                    run_symbol_step_safe(\n",
        "                        sym,\n",
        "                        model,\n",
        "                        vecnorm,\n",
        "                        feat_hint,\n",
        "                        cycle_equity,\n",
        "                        timeout_sec=15,\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    logging.warning(\"[%s] symbol step timeout/fail: %s\", sym, e)\n",
        "                finally:\n",
        "                    logging.info(\n",
        "                        \"[TIMER] %s symbol work: %.3fs\",\n",
        "                        sym,\n",
        "                        time.perf_counter() - t_sym_start,\n",
        "                    )\n",
        "\n",
        "            maybe_log_equity_snapshot(\n",
        "                api_in=api_local,\n",
        "                reason=(\"trade\" if globals().get(\"_TRADE_EVENT_FLAG\", False) else \"cycle\"),\n",
        "            )\n",
        "\n",
        "            # Kill-switch\n",
        "            try:\n",
        "                anchor = globals().get(\"SESSION_OPEN_EQUITY\", None)\n",
        "                if anchor is not None:\n",
        "                    eq_now = float(api_local.get_account().equity)\n",
        "                    dd = (eq_now / max(1e-9, float(anchor))) - 1.0\n",
        "                    max_dd = float(\n",
        "                        os.getenv(\n",
        "                            \"MAX_DAILY_DRAWDOWN_PCT\",\n",
        "                            globals().get(\"MAX_DAILY_DRAWDOWN_PCT\", 0.05),\n",
        "                        )\n",
        "                    )\n",
        "                    if dd <= -abs(max_dd):\n",
        "                        if time.time() - _last_kill_ts > 60:\n",
        "                            for s in per_ticker.keys():\n",
        "                                flatten_symbol(api_local, s)\n",
        "                            logging.warning(\n",
        "                                \"KILL-SWITCH: daily drawdown %.2f%% reached. Flattening & pausing.\",\n",
        "                                100.0 * dd,\n",
        "                            )\n",
        "                            _last_kill_ts = time.time()\n",
        "\n",
        "                            cooldown_min = int(\n",
        "                                os.getenv(\n",
        "                                    \"KILL_SWITCH_COOLDOWN_MIN\",\n",
        "                                    str(globals().get(\"KILL_SWITCH_COOLDOWN_MIN\", 30)),\n",
        "                                )\n",
        "                            )\n",
        "                            if not DRY_RUN:\n",
        "                                time.sleep(60 * cooldown_min)\n",
        "                            continue\n",
        "            except Exception as e:\n",
        "                logging.debug(\"kill-switch check failed: %s\", e)\n",
        "\n",
        "            # Flatten into close\n",
        "            m2c = minutes_to_close(api_local)\n",
        "            if FLATTEN_INTO_CLOSE and not flattened_today and m2c is not None and m2c <= 5:\n",
        "                for s in per_ticker.keys():\n",
        "                    flatten_symbol(api_local, s)\n",
        "                    _REENTRY_BLOCK_UNTIL[s] = time.time() + REENTRY_COOLDOWN_SEC\n",
        "                logging.info(\"Flattened all positions into the close.\")\n",
        "                maybe_log_equity_snapshot(api_in=api_local, reason=\"close\")\n",
        "                flattened_today = True\n",
        "\n",
        "                if bool(globals().get(\"EXIT_AFTER_CLOSE\", False)):\n",
        "                    logging.info(\"EXIT_AFTER_CLOSE=True — exiting live loop after close flatten.\")\n",
        "                    break\n",
        "\n",
        "            cycle += 1\n",
        "\n",
        "            now_ts = time.time()\n",
        "            if now_ts - last_plot_ts >= 900:\n",
        "                try:\n",
        "                    plot_equity_curve(from_equity_csv=True)\n",
        "                    df = pd.read_csv(EQUITY_LOG_CSV, parse_dates=[\"datetime_utc\"])\n",
        "                    m = compute_performance_metrics(df)\n",
        "                    logging.info(\n",
        "                        \"Perf: cum_return=%.2f%% | sharpe=%.2f | maxDD=%.2f%%\",\n",
        "                        100 * m[\"cum_return\"],\n",
        "                        m[\"sharpe\"],\n",
        "                        100 * m[\"max_drawdown\"],\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    logging.warning(\"Plot/metrics failed: %s\", e)\n",
        "                last_plot_ts = now_ts\n",
        "\n",
        "            logging.info(\n",
        "                \"[TIMER] full-cycle active time: %.3fs (cooldown=%d min)\",\n",
        "                time.perf_counter() - t_cycle_start,\n",
        "                COOLDOWN_MIN,\n",
        "            )\n",
        "\n",
        "            if (cycle % 12) == 0:\n",
        "                gc.collect()\n",
        "\n",
        "            _sleep_to_next_minute_block(COOLDOWN_MIN)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        logging.info(\"KeyboardInterrupt: stopping live loop.\")\n",
        "    except Exception as e:\n",
        "        logging.exception(\"Live loop exception: %s\", e)\n",
        "        try:\n",
        "            log_equity_snapshot(api_in=api_local)\n",
        "        except Exception:\n",
        "            pass\n",
        "    finally:\n",
        "        global _TIMEOUT_EXEC\n",
        "        try:\n",
        "            _TIMEOUT_EXEC.shutdown(wait=False, cancel_futures=True)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        _TIMEOUT_EXEC = ThreadPoolExecutor(max_workers=8)\n",
        "        logging.info(\"Timeout executor reset.\")\n",
        "\n",
        "        try:\n",
        "            if FORCE_FLATTEN_ON_EXIT:\n",
        "                flatten_all_positions(api_local)\n",
        "        except Exception as e:\n",
        "            logging.warning(\"Flatten-on-exit skipped: %s\", e)\n",
        "\n",
        "        try:\n",
        "            maybe_log_equity_snapshot(api_in=api_local, reason=\"finalize\")\n",
        "            plot_equity_curve(from_equity_csv=True)\n",
        "        except Exception as e:\n",
        "            logging.warning(\"Finalization failed: %s\", e)\n",
        "\n",
        "        logging.info(\"Live loop exited cleanly.\")\n",
        "\n",
        "# ============================================================\n",
        "# Diagnostic runner (optional)\n",
        "# ============================================================\n",
        "\n",
        "def ticker_diagnostic(ticker: str, artifacts_dir: str, lookback_bars: int = 200):\n",
        "    logging.info(\"[diagnostic] starting for %s\", ticker)\n",
        "    api_local = init_alpaca()\n",
        "\n",
        "    art = pick_artifacts_for_ticker(ticker, artifacts_dir)\n",
        "    model_path = art.get(\"model\")\n",
        "    vec_path = art.get(\"vecnorm\")\n",
        "    feats_path = art.get(\"features\")\n",
        "\n",
        "    if model_path is None:\n",
        "        logging.error(\"[%s] no model found; aborting diagnostic\", ticker)\n",
        "        return None\n",
        "\n",
        "    model = load_ppo_model(model_path)\n",
        "    vecnorm = load_vecnormalize(vec_path)\n",
        "    if vecnorm is not None and hasattr(vecnorm, \"training\"):\n",
        "        vecnorm.training = False\n",
        "    if vecnorm is not None and hasattr(vecnorm, \"norm_reward\"):\n",
        "        vecnorm.norm_reward = False\n",
        "\n",
        "    feats = load_features(feats_path) if feats_path else None\n",
        "\n",
        "    bars_df = get_recent_bars(api_local, ticker, limit=lookback_bars, timeframe=LIVE_TIMEFRAME)\n",
        "    if bars_df is None or bars_df.empty:\n",
        "        logging.error(\"[%s] no bars returned\", ticker)\n",
        "        return None\n",
        "\n",
        "    shape = expected_obs_shape(model, vecnorm)\n",
        "    obs, obs_ts = prepare_observation_from_bars(bars_df, features_hint=feats, expected_shape=shape, symbol=ticker)\n",
        "\n",
        "    if vecnorm is not None and getattr(vecnorm, \"obs_rms\", None) is not None:\n",
        "        try:\n",
        "            obs = vecnorm.normalize_obs(obs)\n",
        "        except Exception:\n",
        "            obs = vecnorm.normalize_obs(np.expand_dims(obs, axis=0))[0]\n",
        "\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    logging.info(\"[diagnostic %s] action=%s | obs_ts=%s | bars=%d | timeframe=%s\",\n",
        "                 ticker, action, obs_ts, len(bars_df), LIVE_TIMEFRAME)\n",
        "    return {\"ticker\": ticker, \"action\": action, \"obs_ts\": obs_ts, \"bars\": len(bars_df), \"timeframe\": str(LIVE_TIMEFRAME)}\n",
        "\n",
        "# ============================================================\n",
        "# Config banner + logging setup after paths\n",
        "# ============================================================\n",
        "\n",
        "def log_config_banner():\n",
        "    try:\n",
        "        artifacts_list = sorted(p.name for p in ARTIFACTS_DIR.iterdir()) if ARTIFACTS_DIR.exists() else []\n",
        "    except Exception:\n",
        "        artifacts_list = []\n",
        "\n",
        "    logging.info(\"EXIT_AFTER_CLOSE      : %s\", os.getenv(\"EXIT_AFTER_CLOSE\", \"0\"))\n",
        "    logging.info(\"FORCE_FIRST_BUY       : %s\", FORCE_FIRST_BUY)\n",
        "    logging.info(\"FORCE_FLATTEN_ON_EXIT : %s\", FORCE_FLATTEN_ON_EXIT)\n",
        "    logging.info(\"CONFIG\")\n",
        "    logging.info(\"Project root          : %s\", PROJECT_ROOT)\n",
        "    logging.info(\"ARTIFACTS_DIR         : %s\", ARTIFACTS_DIR)\n",
        "    logging.info(\"RESULTS_DIR           : %s\", RESULTS_DIR)\n",
        "    logging.info(\"Tickers               : %s\", TICKERS)\n",
        "    logging.info(\"API base              : %s\", BASE_URL)\n",
        "    logging.info(\"AUTO_RUN_LIVE         : %s\", os.getenv(\"AUTO_RUN_LIVE\", \"\"))\n",
        "    logging.info(\"INF_DETERMINISTIC     : %s\", INF_DETERMINISTIC)\n",
        "    logging.info(\"ALLOW_SHORTS          : %s\", ALLOW_SHORTS)\n",
        "    logging.info(\"FLATTEN_INTO_CLOSE    : %s\", FLATTEN_INTO_CLOSE)\n",
        "    logging.info(\"REENTRY_COOLDOWN_SEC  : %s\", os.getenv(\"REENTRY_COOLDOWN_SEC\", str(REENTRY_COOLDOWN_SEC)))\n",
        "    logging.info(\"DRY_RUN=%s | BARS_FEED=%s | USE_FRACTIONALS=%s | COOLDOWN_MIN=%s | STALE_MAX_SEC=%s\",\n",
        "                 DRY_RUN, BARS_FEED, USE_FRACTIONALS, COOLDOWN_MIN, STALE_MAX_SEC)\n",
        "\n",
        "    logging.info(\"DEBUG_FORCE_SEED_IF_IDLE=%s | DEBUG_SEED_IDLE_CYCLES=%s\",\n",
        "                 os.getenv(\"DEBUG_FORCE_SEED_IF_IDLE\", \"0\"), os.getenv(\"DEBUG_SEED_IDLE_CYCLES\", \"10\"))\n",
        "\n",
        "    logging.info(\"PH_TIMEOUT_SEC        : %s\", os.getenv(\"PH_TIMEOUT_SEC\", \"8\"))\n",
        "    logging.info(\"DATA_TIMEFRAME        : %s (model bars)\", os.getenv(\"DATA_TIMEFRAME\", \"1H\"))\n",
        "    logging.info(\"EQUITY_TIMEFRAME      : %s (equity reporting)\", os.getenv(\"EQUITY_TIMEFRAME\", \"5Min\"))\n",
        "\n",
        "    logging.info(\"MAX_DD_PCT: %.3f | KILL_SWITCH_COOLDOWN_MIN: %s\",\n",
        "                 float(globals().get(\"MAX_DAILY_DRAWDOWN_PCT\", 0.05)),\n",
        "                 os.getenv(\"KILL_SWITCH_COOLDOWN_MIN\", str(globals().get(\"KILL_SWITCH_COOLDOWN_MIN\", 30))))\n",
        "\n",
        "    logging.info(\"WEIGHT_CAP: %.3f | SIZING_MODE: %s | ENTER_CONF_MIN: %.3f | ENTER_WEIGHT_MIN: %.3f | EXIT_WEIGHT_MAX: %.3f | REBALANCE_MIN_NOTIONAL: %.2f\",\n",
        "                 WEIGHT_CAP, SIZING_MODE, ENTER_CONF_MIN, ENTER_WEIGHT_MIN, EXIT_WEIGHT_MAX, REBALANCE_MIN_NOTIONAL)\n",
        "\n",
        "    logging.info(\"TAKE_PROFIT_PCT: %.3f | STOP_LOSS_PCT: %.3f | BEST_WINDOW_ENV: %s\",\n",
        "                 TAKE_PROFIT_PCT, STOP_LOSS_PCT, (BEST_WINDOW_ENV or \"\"))\n",
        "\n",
        "    logging.info(\"DELTA_WEIGHT_MIN: %.3f | RAW_POS_MIN: %.3f | RAW_NEG_MAX: %.3f\",\n",
        "                 float(globals().get(\"DELTA_WEIGHT_MIN\", 0.0)),\n",
        "                 float(globals().get(\"RAW_POS_MIN\", 0.0)),\n",
        "                 float(globals().get(\"RAW_NEG_MAX\", 0.0)))\n",
        "\n",
        "    if artifacts_list:\n",
        "        logging.info(\"Artifacts present (%d): %s\", len(artifacts_list), \", \".join(artifacts_list))\n",
        "\n",
        "def setup_logging_after_paths():\n",
        "    warnings.filterwarnings(\"default\")\n",
        "    level = getattr(logging, os.getenv(\"LOG_LEVEL\", \"INFO\").upper(), logging.INFO)\n",
        "\n",
        "    root = logging.getLogger()\n",
        "    root.handlers.clear()\n",
        "    root.setLevel(level)\n",
        "\n",
        "    fmt = logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\")\n",
        "\n",
        "    sh = logging.StreamHandler(sys.stdout)\n",
        "    sh.setLevel(level)\n",
        "    sh.setFormatter(fmt)\n",
        "    root.addHandler(sh)\n",
        "\n",
        "    log_path = RESULTS_DIR / \"live_loop.log\"\n",
        "    fh = logging.FileHandler(log_path)\n",
        "    fh.setLevel(level)\n",
        "    fh.setFormatter(fmt)\n",
        "    root.addHandler(fh)\n",
        "\n",
        "    try:\n",
        "        sys.stdout.reconfigure(line_buffering=True)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# ============================================================\n",
        "# Main\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if IN_COLAB:\n",
        "        upload_env_and_artifacts_in_colab()\n",
        "        normalize_artifacts()\n",
        "        load_dotenv(dotenv_path=PROJECT_ROOT / \".env\", override=True)\n",
        "        REENTRY_COOLDOWN_SEC = int(os.getenv(\"REENTRY_COOLDOWN_SEC\", \"300\"))\n",
        "\n",
        "    cfg = configure_knobs(overrides={\n",
        "        # data freshness\n",
        "        \"BARS_FEED\": \"\",\n",
        "        \"STALE_MAX_SEC\": 4200,\n",
        "\n",
        "        # sizing & threshold shaping\n",
        "        \"SIZING_MODE\": \"linear\",\n",
        "        \"CONF_FLOOR\": 0.00,\n",
        "        \"WEIGHT_CAP\": 0.40,\n",
        "\n",
        "        # entry/exit sensitivity\n",
        "        \"ENTER_CONF_MIN\": 0.02,\n",
        "        \"ENTER_WEIGHT_MIN\": 0.002,\n",
        "        \"EXIT_WEIGHT_MAX\": 0.001,\n",
        "        \"DELTA_WEIGHT_MIN\": 0.0005,\n",
        "        \"REBALANCE_MIN_NOTIONAL\": 5.00,\n",
        "        # --- execution ---\n",
        "        \"FORCE_FIRST_BUY\": True,\n",
        "\n",
        "        # posture\n",
        "        \"ALLOW_SHORTS\": True,\n",
        "        \"COOLDOWN_MIN\": 10,\n",
        "\n",
        "        # raw-action gates\n",
        "        \"RAW_POS_MIN\": 0.00,\n",
        "        \"RAW_NEG_MAX\": 0.00,\n",
        "\n",
        "        # risk\n",
        "        \"TAKE_PROFIT_PCT\": 0.05,\n",
        "        \"STOP_LOSS_PCT\": 0.02,\n",
        "\n",
        "        # logging cadence\n",
        "        \"EQUITY_LOG_THROTTLE_SEC\": 300,\n",
        "        \"SKIP_EQUITY_WHEN_DRY_RUN\": False,\n",
        "\n",
        "        # kill-switch\n",
        "        \"MAX_DAILY_DRAWDOWN_PCT\": 0.05,\n",
        "    })\n",
        "    globals()[\"cfg\"] = cfg\n",
        "\n",
        "    setup_logging_after_paths()\n",
        "\n",
        "    # Live data timeframe (match PPO training granularity)\n",
        "    LIVE_TIMEFRAME = _TF_MAP.get(cfg.DATA_TIMEFRAME.strip().lower(), TimeFrame.Hour)\n",
        "\n",
        "    TRAIN_TIMEFRAME = os.getenv(\"TRAIN_TIMEFRAME\", \"1H\").strip().lower()\n",
        "    if cfg.DATA_TIMEFRAME.strip().lower() != TRAIN_TIMEFRAME:\n",
        "        logging.warning(\n",
        "            \"⚠️ Timeframe mismatch: trained=%s live=%s. Only change DATA_TIMEFRAME if you retrained the model.\",\n",
        "            TRAIN_TIMEFRAME, cfg.DATA_TIMEFRAME\n",
        "        )\n",
        "\n",
        "    if cfg.AUTO_RUN_LIVE:\n",
        "        assert \"paper-api\" in BASE_URL.lower()\n",
        "\n",
        "    log_config_banner()\n",
        "    logging.info(\"DATA_TIMEFRAME=%s -> LIVE_TIMEFRAME=%s\", cfg.DATA_TIMEFRAME, LIVE_TIMEFRAME)\n",
        "\n",
        "    # Save run config snapshot\n",
        "    try:\n",
        "        cfg_path = RESULTS_DIR / \"run_config.json\"\n",
        "        payload = {\n",
        "            \"time\": utcnow_iso(),\n",
        "            \"tickers\": TICKERS,\n",
        "            \"dry_run\": DRY_RUN,\n",
        "            \"bars_feed\": BARS_FEED,\n",
        "            \"weight_cap\": WEIGHT_CAP,\n",
        "            \"enter_conf_min\": ENTER_CONF_MIN,\n",
        "            \"enter_weight_min\": ENTER_WEIGHT_MIN,\n",
        "            \"exit_weight_max\": EXIT_WEIGHT_MAX,\n",
        "            \"rebalance_min_notional\": REBALANCE_MIN_NOTIONAL,\n",
        "            \"delta_weight_min\": DELTA_WEIGHT_MIN,\n",
        "            \"tp\": TAKE_PROFIT_PCT,\n",
        "            \"sl\": STOP_LOSS_PCT,\n",
        "            \"allow_shorts\": ALLOW_SHORTS,\n",
        "        }\n",
        "        tmp = cfg_path.with_suffix(\".tmp\")\n",
        "        tmp.write_text(json.dumps(payload, indent=2))\n",
        "        tmp.replace(cfg_path)\n",
        "    except Exception as e:\n",
        "        logging.warning(\"Could not write run_config.json: %s\", e)\n",
        "\n",
        "    # Paper safety\n",
        "    assert \"paper-api\" in BASE_URL.lower(), f\"Refusing to trade: BASE_URL is not paper ({BASE_URL})\"\n",
        "\n",
        "    # Single init\n",
        "    api = init_alpaca()\n",
        "    acct = api.get_account()\n",
        "\n",
        "    logging.info(\"shorting_enabled=%s\", getattr(acct, \"shorting_enabled\", None))\n",
        "\n",
        "    # Optional sanity\n",
        "    assert not bool(getattr(acct, \"trading_blocked\", False)), f\"Trading is blocked on this account: {getattr(acct,'status','')}\"\n",
        "\n",
        "    logging.info(\"Account status: %s | equity=%s | cash=%s\", acct.status, acct.equity, acct.cash)\n",
        "    write_account_info_to_run_config(api)\n",
        "\n",
        "    if cfg.AUTO_RUN_LIVE:\n",
        "        run_live(TICKERS, api)\n",
        "    else:\n",
        "        logging.info(\"AUTO_RUN_LIVE disabled; live loop not started.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5X68r85om00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2656caf-93f4-4330-f972-62a878b6ac36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[equity source] /content/drive/MyDrive/AlpacaPaper/results/2026-02-11/equity_log.csv\n",
            "\n",
            "Equity summary — last: $99,331.40 | n=8 pts | Sharpe(h): -2.87 | src=/content/drive/MyDrive/AlpacaPaper/results/2026-02-11/equity_log.csv\n",
            "Tickers to report: ['GE', 'MASTER', 'UNH']\n",
            "\n",
            "Trade Summary:\n",
            "GE: {'LONG': 7} | src=trade_log_GE.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GE weight: mean=0.086, std=0.012, min=0.076, max=0.099\n",
            "GE raw_action: mean=0.215, std=0.030, min=0.191, max=0.247\n",
            "MASTER: no trades logged yet.\n",
            "UNH: {'SHORT': 6, 'FLAT': 1} | src=trade_log_UNH.csv\n",
            "UNH weight: mean=-0.157, std=0.069, min=-0.183, max=0.000\n",
            "UNH raw_action: mean=-0.392, std=0.173, min=-0.458, max=0.000\n",
            "\n",
            "Position Summary:\n",
            "  GE: 24.155996526 shares @ $313.73 | Value: $7,578.46\n",
            "\n",
            "Total Market Value: $7,578.46\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GE: 11 filled trades in last 14 days\n",
            "MASTER: 0 filled trades in last 14 days\n",
            "UNH: 6 filled trades in last 14 days\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from alpaca_trade_api.rest import TimeFrame\n",
        "\n",
        "\n",
        "RESULTS_DIR = Path(globals().get(\"RESULTS_DIR\", os.getenv(\"RESULTS_DIR\", \".\")))\n",
        "LATEST_DIR  = Path(globals().get(\"LATEST_DIR\",  os.getenv(\"LATEST_DIR\",  str(RESULTS_DIR))))\n",
        "\n",
        "eq_candidates = [\n",
        "    globals().get(\"EQUITY_LOG_CSV\"),\n",
        "    globals().get(\"EQUITY_LOG_LATEST\"),\n",
        "    RESULTS_DIR / \"equity_log.csv\",\n",
        "    LATEST_DIR / \"equity_log.csv\",\n",
        "]\n",
        "\n",
        "def _first_existing(paths):\n",
        "    for p in paths:\n",
        "        if p:\n",
        "            p = Path(p)\n",
        "            if p.exists() and p.is_file():\n",
        "                return p\n",
        "    return None\n",
        "\n",
        "eq_path = _first_existing(eq_candidates)\n",
        "if eq_path is None:\n",
        "    all_eq = list(RESULTS_DIR.glob(\"equity_log*.csv\")) + list(LATEST_DIR.glob(\"equity_log*.csv\"))\n",
        "    eq_path = max(all_eq, key=lambda p: p.stat().st_mtime, default=None)\n",
        "\n",
        "if eq_path and eq_path.exists():\n",
        "    print(f\"[equity source] {eq_path}\")\n",
        "    try:\n",
        "        eq = pd.read_csv(eq_path, parse_dates=[\"datetime_utc\"]).sort_values(\"datetime_utc\")\n",
        "        if not eq.empty:\n",
        "            r = eq[\"equity\"].pct_change().dropna()\n",
        "            sharpe_h = (r.mean() / (r.std() + 1e-12)) * np.sqrt(252 * 6.5) if len(r) else float(\"nan\")\n",
        "            print(\n",
        "                f\"\\nEquity summary — last: ${eq['equity'].iloc[-1]:,.2f} | \"\n",
        "                f\"n={len(eq)} pts | Sharpe(h): {sharpe_h:.2f} | src={eq_path}\"\n",
        "            )\n",
        "        else:\n",
        "            print(f\"No rows in equity log: {eq_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not summarize equity ({eq_path}): {e}\")\n",
        "else:\n",
        "    print(\"No equity_log*.csv found in RESULTS_DIR/LATEST_DIR.\")\n",
        "\n",
        "def _resolve_tickers():\n",
        "    g = globals().get(\"TICKERS\", None)\n",
        "\n",
        "    # Base tickers from globals or env\n",
        "    if isinstance(g, (list, tuple, set)):\n",
        "        base = [str(x).upper() for x in g]\n",
        "    else:\n",
        "        env_val = os.getenv(\"TICKERS\", (g if isinstance(g, str) else \"\"))\n",
        "        base = [t.strip().upper() for t in str(env_val).split(\",\") if t.strip()]\n",
        "\n",
        "    # Also include symbols with existing logs on disk\n",
        "    discovered = [\n",
        "        p.stem.replace(\"trade_log_\", \"\").upper()\n",
        "        for p in list(RESULTS_DIR.glob(\"trade_log_*.csv\")) + list(LATEST_DIR.glob(\"trade_log_*.csv\"))\n",
        "    ]\n",
        "\n",
        "    ticks = sorted(set(base) | set(discovered))\n",
        "    return ticks if ticks else [\"UNH\", \"GE\"]\n",
        "\n",
        "tickers_to_report = _resolve_tickers()\n",
        "print(\"Tickers to report:\", tickers_to_report)\n",
        "\n",
        "print(\"\\nTrade Summary:\")\n",
        "for ticker in tickers_to_report:\n",
        "    trade_candidates = [\n",
        "        RESULTS_DIR / f\"trade_log_{ticker}.csv\",\n",
        "        LATEST_DIR / f\"trade_log_{ticker}.csv\",\n",
        "    ]\n",
        "    log_path = _first_existing(trade_candidates)\n",
        "    if not log_path:\n",
        "        #Tolerate Drive duplicates like \"trade_log_XYZ (1).csv\"\n",
        "        any_logs = list(RESULTS_DIR.glob(f\"trade_log_{ticker}*.csv\")) + \\\n",
        "                   list(LATEST_DIR.glob(f\"trade_log_{ticker}*.csv\"))\n",
        "        log_path = max(any_logs, key=lambda p: p.stat().st_mtime, default=None)\n",
        "\n",
        "    if not log_path or not log_path.exists():\n",
        "        print(f\"{ticker}: no trades logged yet.\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(\n",
        "            log_path,\n",
        "            on_bad_lines=\"skip\",\n",
        "            engine=\"python\",\n",
        "            parse_dates=[\"log_time\", \"bar_time\"],\n",
        "        )\n",
        "        key = \"signal\" if \"signal\" in df.columns else (\"action\" if \"action\" in df.columns else None)\n",
        "        if key:\n",
        "            counts = df[key].value_counts(dropna=False).to_dict()\n",
        "            print(f\"{ticker}: {counts} | src={log_path.name}\")\n",
        "        else:\n",
        "            print(f\"{ticker}: log present but missing 'signal'/'action' columns. src={log_path.name}\")\n",
        "\n",
        "        if \"confidence\" in df.columns and df[\"confidence\"].notna().any():\n",
        "            plt.figure(figsize=(8, 3.5))\n",
        "            df[\"confidence\"].dropna().plot(kind=\"hist\", bins=10, edgecolor=\"black\")\n",
        "            plt.title(f\"{ticker} - Confidence Distribution\")\n",
        "            plt.xlabel(\"confidence\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        for col in [\"weight\", \"raw_action\"]:\n",
        "            if col in df.columns and df[col].notna().any():\n",
        "                s = df[col].dropna()\n",
        "                print(\n",
        "                    f\"{ticker} {col}: mean={s.mean():.3f}, std={s.std():.3f}, \"\n",
        "                    f\"min={s.min():.3f}, max={s.max():.3f}\"\n",
        "                )\n",
        "    except Exception as e:\n",
        "        print(f\"{ticker}: could not summarize trades ({log_path}): {e}\")\n",
        "\n",
        "try:\n",
        "    if \"api\" not in globals():\n",
        "        api = init_alpaca()\n",
        "    positions = api.list_positions()\n",
        "    total_market_value = 0.0\n",
        "    print(\"\\nPosition Summary:\")\n",
        "    for p in positions:\n",
        "        mv = float(p.market_value)\n",
        "        total_market_value += mv\n",
        "        print(f\"  {p.symbol}: {p.qty} shares @ ${float(p.current_price):.2f} | Value: ${mv:,.2f}\")\n",
        "    print(f\"\\nTotal Market Value: ${total_market_value:,.2f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not summarize positions: {e}\")\n",
        "\n",
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "def count_filled_orders_since(api, symbol: str, days: int = 14) -> int:\n",
        "    after = (datetime.now(timezone.utc) - timedelta(days=days)).isoformat()\n",
        "    orders = api.list_orders(status=\"all\", after=after, nested=True)\n",
        "    return sum(1 for o in orders if o.symbol == symbol and o.status in (\"filled\", \"partially_filled\"))\n",
        "\n",
        "try:\n",
        "    api_chk = api if \"api\" in globals() else init_alpaca()\n",
        "    for sym in tickers_to_report:\n",
        "        n = count_filled_orders_since(api_chk, sym, days=14)\n",
        "        print(f\"{sym}: {n} filled trades in last 14 days\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not fetch filled orders: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaKYNmKdOwOt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "97645246-7e16-4873-cbb2-a947b26cdfd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied: live_loop.log from 2026-02-11\n",
            "Copied: run_config.json from 2026-02-11\n",
            "Copied: trade_log_master.csv from 2026-02-11\n",
            "Copied: trade_log_UNH.csv from 2026-02-11\n",
            "Copied: equity_log.csv from 2026-02-11\n",
            "Copied: trade_log_GE.csv from 2026-02-11\n",
            "Copied: equity_curve.png from 2026-02-11\n",
            "Missing source: /content/drive/MyDrive/AlpacaPaper/results_export/2026-02-11\n",
            "Wrote: /content/exports/2026-02-11_export/trade_log_master.csv\n",
            "ZIP -> /content/results_2026-02-11_1770843610.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_84f2a79b-5fdf-404b-875f-99c4ef52775c\", \"results_2026-02-11_1770843610.zip\", 44387)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Local export now contains:\n",
            " - equity_curve.png\n",
            " - equity_log.csv\n",
            " - live_loop.log\n",
            " - run_config.json\n",
            " - trade_log_GE.csv\n",
            " - trade_log_UNH.csv\n",
            " - trade_log_master.csv\n"
          ]
        }
      ],
      "source": [
        "#--- Export locally & download to your computer (Colab) ---\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "from google.colab import files   #<-- NEW: for browser download\n",
        "import shutil, time, pandas as pd\n",
        "\n",
        "#Drive root (same as before, to read your results)\n",
        "ROOT = Path(\"/content/drive/MyDrive/AlpacaPaper\")\n",
        "TODAY = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "#Original sources in Drive (unchanged)\n",
        "SRC_RESULTS = ROOT / \"results\" / TODAY         #e.g., /.../results/2025-10-13\n",
        "SRC_EXPORT  = ROOT / \"results_export\" / TODAY  #rescue export folder (if used)\n",
        "\n",
        "#=== CHANGE: write/export to LOCAL staging (in Colab VM), not Drive ===\n",
        "DEST = Path(\"/content\") / \"exports\" / f\"{TODAY}_export\"\n",
        "DEST.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def copy_all(src_dir, dest_dir):\n",
        "    if src_dir.exists():\n",
        "        for p in src_dir.glob(\"*\"):\n",
        "            if p.is_file():\n",
        "                shutil.copy2(p, dest_dir / p.name)\n",
        "                print(\"Copied:\", p.name, \"from\", src_dir.name)\n",
        "    else:\n",
        "        print(\"Missing source:\", src_dir)\n",
        "\n",
        "#Copy from both possible sources into local /content/exports/<today>_export\n",
        "copy_all(SRC_RESULTS, DEST)\n",
        "copy_all(SRC_EXPORT, DEST)\n",
        "\n",
        "#Build/refresh trade_log_master.csv from per-symbol logs (in LOCAL DEST)\n",
        "sym_logs = list(DEST.glob(\"trade_log_*.csv\"))\n",
        "if sym_logs:\n",
        "    frames = []\n",
        "    for p in sym_logs:\n",
        "        try:\n",
        "            df = pd.read_csv(p)\n",
        "            df[\"symbol_file\"] = p.stem.replace(\"trade_log_\", \"\")\n",
        "            frames.append(df)\n",
        "        except Exception as e:\n",
        "            print(\"Skip\", p.name, \"->\", e)\n",
        "    if frames:\n",
        "        master = pd.concat(frames, ignore_index=True, sort=False)\n",
        "        master_path = DEST / \"trade_log_master.csv\"\n",
        "        master.to_csv(master_path, index=False)\n",
        "        print(\"Wrote:\", master_path)\n",
        "\n",
        "#Zip LOCALLY under /content and trigger a browser download\n",
        "zip_base = Path(\"/content\") / f\"results_{TODAY}_{int(time.time())}\"\n",
        "archive_path = shutil.make_archive(str(zip_base), \"zip\", DEST)\n",
        "archive_path = str(Path(archive_path))  #ensure string for files.download\n",
        "\n",
        "print(\"ZIP ->\", archive_path)\n",
        "\n",
        "#OPTIONAL: also keep a copy in Drive (uncomment if wanted)\n",
        "#shutil.copy2(archive_path, ROOT / \"results\" / Path(archive_path).name)\n",
        "\n",
        "#Prompt download to your computer\n",
        "files.download(archive_path)\n",
        "\n",
        "#Show what's in the LOCAL export folder\n",
        "print(\"\\nLocal export now contains:\")\n",
        "for p in sorted(DEST.iterdir()):\n",
        "    print(\" -\", p.name)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}