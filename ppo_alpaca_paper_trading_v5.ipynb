{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/racoope70/exploratory-daytrading/blob/main/ppo_alpaca_paper_trading_v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-DEy5gEqqEi",
        "outputId": "3a0ea24f-3fbd-4640-a611-add24856cea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping stable-baselines3 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping shimmy as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: gymnasium 1.2.2\n",
            "Uninstalling gymnasium-1.2.2:\n",
            "  Successfully uninstalled gymnasium-1.2.2\n",
            "Found existing installation: gym 0.25.2\n",
            "Uninstalling gym-0.25.2:\n",
            "  Successfully uninstalled gym-0.25.2\n",
            "\u001b[33mWARNING: Skipping autorom as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping AutoROM.accept-rom-license as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: ale-py 0.11.2\n",
            "Uninstalling ale-py-0.11.2:\n",
            "  Successfully uninstalled ale-py-0.11.2\n",
            "Collecting gymnasium==0.29.1\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting shimmy==1.3.0\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting stable-baselines3==2.3.0\n",
            "  Downloading stable_baselines3-2.3.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (0.0.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.3.0) (2.9.0+cu126)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.3.0) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.3.0) (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3==2.3.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3==2.3.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==2.3.0) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13->stable-baselines3==2.3.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13->stable-baselines3==2.3.0) (3.0.3)\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'stable-baselines3' candidate (version 2.3.0 at https://files.pythonhosted.org/packages/51/0b/6539076ed58343f1404dea0462167b079b5264508b8e5bbed01cea9f66b8/stable_baselines3-2.3.0-py3-none-any.whl (from https://pypi.org/simple/stable-baselines3/) (requires-python:>=3.8))\n",
            "Reason for being yanked: Loading broken with PyTorch 1.13\u001b[0m\u001b[33m\n",
            "\u001b[0mDownloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Downloading stable_baselines3-2.3.0-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.1/182.1 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gymnasium, shimmy, stable-baselines3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires ale-py>=0.10.1, which is not installed.\n",
            "dopamine-rl 4.1.2 requires gym<=0.25.2, which is not installed.\n",
            "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gymnasium-0.29.1 shimmy-1.3.0 stable-baselines3-2.3.0\n",
            "Collecting alpaca-trade-api\n",
            "  Downloading alpaca_trade_api-3.2.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Collecting gym-anytrading\n",
            "  Downloading gym_anytrading-2.0.0-py3-none-any.whl.metadata (292 bytes)\n",
            "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.0.2)\n",
            "Requirement already satisfied: requests<3,>2 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.32.4)\n",
            "Collecting urllib3<2,>1.24 (from alpaca-trade-api)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: websocket-client<2,>=0.56.0 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (1.9.0)\n",
            "Collecting websockets<11,>=9.0 (from alpaca-trade-api)\n",
            "  Downloading websockets-10.4.tar.gz (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting msgpack==1.0.3 (from alpaca-trade-api)\n",
            "  Downloading msgpack-1.0.3.tar.gz (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (3.13.2)\n",
            "Collecting PyYAML==6.0.1 (from alpaca-trade-api)\n",
            "  Downloading PyYAML-6.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: deprecation==2.1.0 in /usr/local/lib/python3.12/dist-packages (from alpaca-trade-api) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from deprecation==2.1.0->alpaca-trade-api) (25.0)\n",
            "Requirement already satisfied: gymnasium>=0.29.1 in /usr/local/lib/python3.12/dist-packages (from gym-anytrading) (0.29.1)\n",
            "Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.12/dist-packages (from gym-anytrading) (3.10.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.22.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=0.29.1->gym-anytrading) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=0.29.1->gym-anytrading) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=0.29.1->gym-anytrading) (0.0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.1->gym-anytrading) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>2->alpaca-trade-api) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>2->alpaca-trade-api) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>2->alpaca-trade-api) (2025.11.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.1->gym-anytrading) (1.17.0)\n",
            "Downloading alpaca_trade_api-3.2.0-py3-none-any.whl (34 kB)\n",
            "Downloading PyYAML-6.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (724 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m725.0/725.0 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gym_anytrading-2.0.0-py3-none-any.whl (172 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.2/172.2 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: msgpack, ta, websockets\n",
            "  Building wheel for msgpack (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for msgpack: filename=msgpack-1.0.3-cp312-cp312-linux_x86_64.whl size=15688 sha256=2cf29031f4fdbba07c9ff87dda9de6095368d15fd9531f4b1af7579c476a6310\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/bd/3f/f043e8f634db9c90ae128d631f43ae9990eef01274a63291f9\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=4fcb0c3b403f62448053383c8c9de0ac2670785fe26893434378203c9cabc77b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/a1/5f/c6b85a7d9452057be4ce68a8e45d77ba34234a6d46581777c6\n",
            "  Building wheel for websockets (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for websockets: filename=websockets-10.4-cp312-cp312-linux_x86_64.whl size=107330 sha256=57f28845a3edf2cbb197434d2e2e0f6a289417f4875597fa7b65da6305e515c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/cf/6d/5d7e4c920cb41925a178b2d2621889c520d648bab487b1d7fd\n",
            "Successfully built msgpack ta websockets\n",
            "Installing collected packages: msgpack, websockets, urllib3, PyYAML, ta, gym-anytrading, alpaca-trade-api\n",
            "  Attempting uninstall: msgpack\n",
            "    Found existing installation: msgpack 1.1.2\n",
            "    Uninstalling msgpack-1.1.2:\n",
            "      Successfully uninstalled msgpack-1.1.2\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0.3\n",
            "    Uninstalling PyYAML-6.0.3:\n",
            "      Successfully uninstalled PyYAML-6.0.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires ale-py>=0.10.1, which is not installed.\n",
            "dopamine-rl 4.1.2 requires gym<=0.25.2, which is not installed.\n",
            "google-genai 1.53.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 10.4 which is incompatible.\n",
            "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.1 which is incompatible.\n",
            "google-adk 1.20.0 requires PyYAML<7.0.0,>=6.0.2, but you have pyyaml 6.0.1 which is incompatible.\n",
            "google-adk 1.20.0 requires websockets<16.0.0,>=15.0.1, but you have websockets 10.4 which is incompatible.\n",
            "dataproc-spark-connect 1.0.1 requires websockets>=14.0, but you have websockets 10.4 which is incompatible.\n",
            "gradio-client 1.14.0 requires websockets<16.0,>=13.0, but you have websockets 10.4 which is incompatible.\n",
            "yfinance 0.2.66 requires websockets>=13.0, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyYAML-6.0.1 alpaca-trade-api-3.2.0 gym-anytrading-2.0.0 msgpack-1.0.3 ta-0.11.0 urllib3-1.26.20 websockets-10.4\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#Clean any partials\n",
        "!pip uninstall -y stable-baselines3 shimmy gymnasium gym autorom AutoROM.accept-rom-license ale-py\n",
        "\n",
        "#Install the compatible trio (no [extra] to avoid Atari deps)\n",
        "!pip install \"gymnasium==0.29.1\" \"shimmy==1.3.0\" \"stable-baselines3==2.3.0\"\n",
        "\n",
        "#Your other libs (safe to keep separate)\n",
        "!pip install alpaca-trade-api ta python-dotenv gym-anytrading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wyc0Dr0D86cA",
        "outputId": "116edb61-2d3d-469c-dfa4-5e1dcad4493d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch: 2.9.0+cu126\n",
            "gymnasium: 0.29.1\n",
            "shimmy: 1.3.0\n",
            "stable-baselines3: 2.3.0\n",
            "alpaca-trade-api: 3.2.0\n",
            "websockets: 10.4\n",
            "pywavelets: 1.8.0\n"
          ]
        }
      ],
      "source": [
        "import torch, gymnasium, shimmy, stable_baselines3 as sb3\n",
        "import alpaca_trade_api, websockets, pywt\n",
        "\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"gymnasium:\", gymnasium.__version__)\n",
        "print(\"shimmy:\", shimmy.__version__)\n",
        "print(\"stable-baselines3:\", sb3.__version__)\n",
        "print(\"alpaca-trade-api:\", alpaca_trade_api.__version__)\n",
        "print(\"websockets:\", websockets.__version__)\n",
        "print(\"pywavelets:\", pywt.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cM2YQVmcZjEu",
        "outputId": "c7757fd4-be1c-42d4-c7b8-581fecf9708c"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Upload your .env (or Alpaca_keys.env.txt). Cancel if already on Drive.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f7646aa3-7dbd-4169-a463-6b035e4c464c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f7646aa3-7dbd-4169-a463-6b035e4c464c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving Alpaca_keys.env.txt to Alpaca_keys.env.txt\n",
            "Saved env → /content/drive/MyDrive/AlpacaPaper/.env\n",
            "Upload your artifacts (ppo_*_model.zip, *_vecnorm*.pkl, *_features*.json or .txt).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cafa0151-d1fd-4aa7-9ae2-a22c7a4d80aa\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cafa0151-d1fd-4aa7-9ae2-a22c7a4d80aa\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving ppo_GE_window1_features.json to ppo_GE_window1_features.json\n",
            "Saving ppo_GE_window1_model_info.json to ppo_GE_window1_model_info.json\n",
            "Saving ppo_GE_window1_model.zip to ppo_GE_window1_model.zip\n",
            "Saving ppo_GE_window1_probability_config.json to ppo_GE_window1_probability_config.json\n",
            "Saving ppo_GE_window1_vecnorm.pkl to ppo_GE_window1_vecnorm.pkl\n",
            "Saving ppo_UNH_window3_features.json to ppo_UNH_window3_features.json\n",
            "Saving ppo_UNH_window3_model_info.json to ppo_UNH_window3_model_info.json\n",
            "Saving ppo_UNH_window3_model.zip to ppo_UNH_window3_model.zip\n",
            "Saving ppo_UNH_window3_probability_config.json to ppo_UNH_window3_probability_config.json\n",
            "Saving ppo_UNH_window3_vecnorm.pkl to ppo_UNH_window3_vecnorm.pkl\n",
            "Artifacts now in: ['ppo_GE_window1_features.json', 'ppo_GE_window1_model.zip', 'ppo_GE_window1_model_info.json', 'ppo_GE_window1_probability_config.json', 'ppo_GE_window1_vecnorm.pkl', 'ppo_UNH_window3_features.json', 'ppo_UNH_window3_model.zip', 'ppo_UNH_window3_model_info.json', 'ppo_UNH_window3_probability_config.json', 'ppo_UNH_window3_vecnorm.pkl']\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:EXIT_AFTER_CLOSE     : 0\n",
            "INFO:root:=== CONFIG ===\n",
            "INFO:root:Project root        : /content/drive/MyDrive/AlpacaPaper\n",
            "INFO:root:ARTIFACTS_DIR       : /content/drive/MyDrive/AlpacaPaper/artifacts\n",
            "INFO:root:RESULTS_DIR         : /content/drive/MyDrive/AlpacaPaper/results/2025-12-10\n",
            "INFO:root:Tickers             : ['UNH', 'GE']\n",
            "INFO:root:API base            : https://paper-api.alpaca.markets\n",
            "INFO:root:AUTO_RUN_LIVE       : 1\n",
            "INFO:root:INF_DETERMINISTIC   : True\n",
            "INFO:root:ALLOW_SHORTS        : False\n",
            "INFO:root:DRY_RUN: False | BARS_FEED: iex | USE_FRACTIONALS: True | COOLDOWN_MIN: 1 | STALE_MAX_SEC: 600\n",
            "INFO:root:DEBUG_FORCE_SEED_IF_IDLE: 0 | DEBUG_SEED_IDLE_CYCLES: 10\n",
            "INFO:root:PH_TIMEOUT_SEC       : 8\n",
            "INFO:root:MAX_DD_PCT: 0.050 | KILL_SWITCH_COOLDOWN_MIN: 30\n",
            "INFO:root:WEIGHT_CAP: 0.250 | SIZING_MODE: linear | ENTER_CONF_MIN: 0.020 | ENTER_WEIGHT_MIN: 0.010 | EXIT_WEIGHT_MAX: 0.007 | REBALANCE_MIN_NOTIONAL: 7.50\n",
            "INFO:root:TAKE_PROFIT_PCT: 0.020 | STOP_LOSS_PCT: 0.010 | BEST_WINDOW_ENV: \n",
            "INFO:root:DELTA_WEIGHT_MIN: 0.003 | RAW_POS_MIN: 0.000 | RAW_NEG_MAX: 0.000\n",
            "INFO:root:Artifacts present (10): ppo_GE_window1_features.json, ppo_GE_window1_model.zip, ppo_GE_window1_model_info.json, ppo_GE_window1_probability_config.json, ppo_GE_window1_vecnorm.pkl, ppo_UNH_window3_features.json, ppo_UNH_window3_model.zip, ppo_UNH_window3_model_info.json, ppo_UNH_window3_probability_config.json, ppo_UNH_window3_vecnorm.pkl\n",
            "INFO:root:Account status: ACTIVE | equity=99205.34 | cash=94754.66\n",
            "INFO:root:[UNH] model=ppo_UNH_window3_model.zip | vecnorm=True | features=True\n",
            "INFO:root:[UNH] Artifacts loaded and ready.\n",
            "INFO:root:[GE] model=ppo_GE_window1_model.zip | vecnorm=True | features=True\n",
            "INFO:root:[GE] Artifacts loaded and ready.\n",
            "INFO:root:Starting live execution for (loaded): ['UNH', 'GE']\n",
            "INFO:root:Session open equity anchor set: 99206.05\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=98s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3828 conf=0.365 → target_w=0.0000 px=$325.27 eq=$99,206.05 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.499s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:20:00+00:00 px=282.23\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=38s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] SL hit (-0.0142 <= -0.0100). Flattening.\n",
            "INFO:root:[GE] close_position submitted\n",
            "INFO:root:[TIMER] GE symbol work: 0.472s\n",
            "INFO:root:Perf: cum_return=0.00% | sharpe=nan | maxDD=nan%\n",
            "INFO:root:[TIMER] full-cycle active time: 2.783s (cooldown=1 min)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2025-12-10/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3828 conf=0.365 → target_w=0.0000 px=$325.12 eq=$99,205.42 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.307s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:20:00+00:00 px=282.24\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.24 eq=$99,205.42 have=0.0\n",
            "INFO:root:[GE] Submitted buy notional=$7.50\n",
            "INFO:root:[GE] Submitted buy notional=$4485.19\n",
            "INFO:root:[TIMER] GE symbol work: 1.434s\n",
            "INFO:root:[TIMER] full-cycle active time: 2.024s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=120s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3828 conf=0.365 → target_w=0.0000 px=$325.12 eq=$99,204.39 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.305s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:21:00+00:00 px=282.20\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.20 eq=$99,204.39 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.583s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.086s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=180s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3828 conf=0.365 → target_w=0.0000 px=$325.12 eq=$99,207.09 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.293s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:22:00+00:00 px=282.25\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.25 eq=$99,207.09 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.530s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.018s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=120s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3829 conf=0.365 → target_w=0.0000 px=$324.97 eq=$99,206.60 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.296s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:22:00+00:00 px=282.25\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=120s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.25 eq=$99,206.60 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.548s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.043s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=120s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$324.98 eq=$99,207.81 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.313s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:24:00+00:00 px=282.33\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.33 eq=$99,207.81 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.529s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.037s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=120s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3831 conf=0.365 → target_w=0.0000 px=$324.92 eq=$99,206.18 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.291s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:25:00+00:00 px=282.27\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.27 eq=$99,206.18 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.541s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.033s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=120s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3831 conf=0.365 → target_w=0.0000 px=$324.98 eq=$99,206.51 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.293s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:26:00+00:00 px=282.29\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.29 eq=$99,206.51 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.562s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.202s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3831 conf=0.365 → target_w=0.0000 px=$324.85 eq=$99,205.98 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.292s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:27:00+00:00 px=282.31\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.31 eq=$99,205.98 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.519s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.006s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=120s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3831 conf=0.365 → target_w=0.0000 px=$324.85 eq=$99,203.19 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.298s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:28:00+00:00 px=282.15\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.15 eq=$99,203.19 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.539s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.034s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=120s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3831 conf=0.365 → target_w=0.0000 px=$325.07 eq=$99,204.78 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.299s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:29:00+00:00 px=282.18\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.18 eq=$99,204.78 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.525s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.020s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=120s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3833 conf=0.366 → target_w=0.0000 px=$324.95 eq=$99,204.47 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.291s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:30:00+00:00 px=282.24\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.24 eq=$99,204.47 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.517s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.004s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=180s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3833 conf=0.366 → target_w=0.0000 px=$324.95 eq=$99,202.95 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.293s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:31:00+00:00 px=282.08\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.08 eq=$99,202.95 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.534s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.024s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=240s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3833 conf=0.366 → target_w=0.0000 px=$324.95 eq=$99,201.60 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.306s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:32:00+00:00 px=281.99\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$281.99 eq=$99,201.60 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.513s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.147s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3835 conf=0.366 → target_w=0.0000 px=$324.51 eq=$99,202.87 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.302s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:33:00+00:00 px=282.02\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.02 eq=$99,202.87 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.558s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.055s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=120s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3835 conf=0.366 → target_w=0.0000 px=$324.51 eq=$99,202.08 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.304s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:35:00+00:00 px=282.03\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.03 eq=$99,202.08 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.552s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.054s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=120s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3834 conf=0.366 → target_w=0.0000 px=$324.48 eq=$99,205.42 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.312s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:35:00+00:00 px=282.20\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.20 eq=$99,205.42 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.571s\n",
            "INFO:root:Perf: cum_return=-0.00% | sharpe=-82.15 | maxDD=-0.00%\n",
            "INFO:root:[TIMER] full-cycle active time: 1.369s (cooldown=1 min)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2025-12-10/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=120s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3836 conf=0.366 → target_w=0.0000 px=$324.40 eq=$99,207.01 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.295s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:36:00+00:00 px=282.31\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.31 eq=$99,207.01 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.514s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.002s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=180s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3836 conf=0.366 → target_w=0.0000 px=$324.40 eq=$99,206.65 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.312s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:37:00+00:00 px=282.33\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.33 eq=$99,206.65 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.562s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.235s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=240s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3836 conf=0.366 → target_w=0.0000 px=$324.40 eq=$99,206.93 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.292s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:37:00+00:00 px=282.33\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=120s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.33 eq=$99,206.93 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.616s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.105s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=300s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3836 conf=0.366 → target_w=0.0000 px=$324.40 eq=$99,208.13 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.291s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:39:00+00:00 px=282.38\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.38 eq=$99,208.13 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.523s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.010s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=360s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3836 conf=0.366 → target_w=0.0000 px=$324.40 eq=$99,208.13 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.293s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:40:00+00:00 px=282.40\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.40 eq=$99,208.13 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.514s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.005s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=420s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3836 conf=0.366 → target_w=0.0000 px=$324.40 eq=$99,208.13 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.295s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:40:00+00:00 px=282.40\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=120s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.40 eq=$99,208.13 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.527s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.016s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=480s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3836 conf=0.366 → target_w=0.0000 px=$324.40 eq=$99,208.37 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.302s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:42:00+00:00 px=282.46\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.46 eq=$99,208.37 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.537s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.032s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=540s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3836 conf=0.366 → target_w=0.0000 px=$324.40 eq=$99,208.60 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.313s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:44:00+00:00 px=282.43\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.43 eq=$99,208.60 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.521s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.189s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=600s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] Observation stale (age=600s ≥ 600s); skipping this cycle.\n",
            "INFO:root:[TIMER] UNH symbol work: 0.193s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:44:00+00:00 px=282.43\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.43 eq=$99,208.13 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.521s\n",
            "INFO:root:[TIMER] full-cycle active time: 0.909s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T18:44:00+00:00 px=324.11\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=120s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3836 conf=0.366 → target_w=0.0000 px=$324.11 eq=$99,209.08 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.374s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:45:00+00:00 px=282.46\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.46 eq=$99,209.08 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.516s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.086s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T18:44:00+00:00 px=324.11\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=180s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3836 conf=0.366 → target_w=0.0000 px=$324.11 eq=$99,208.45 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.381s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:46:00+00:00 px=282.46\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.46 eq=$99,208.45 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.542s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.122s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T18:47:00+00:00 px=323.93\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3837 conf=0.366 → target_w=0.0000 px=$323.93 eq=$99,208.92 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.369s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:47:00+00:00 px=282.48\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.48 eq=$99,208.92 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.542s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.108s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T18:48:00+00:00 px=323.91\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3837 conf=0.366 → target_w=0.0000 px=$323.91 eq=$99,209.72 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.354s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:49:00+00:00 px=282.49\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.49 eq=$99,209.72 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.517s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.176s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T18:48:00+00:00 px=323.91\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=120s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3837 conf=0.366 → target_w=0.0000 px=$323.91 eq=$99,210.51 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.362s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:49:00+00:00 px=282.66\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.66 eq=$99,210.51 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.531s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.092s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T18:50:00+00:00 px=324.24\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3836 conf=0.366 → target_w=0.0000 px=$324.24 eq=$99,213.30 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.358s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:50:00+00:00 px=282.70\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.70 eq=$99,213.30 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.508s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.061s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T18:51:00+00:00 px=324.18\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3836 conf=0.366 → target_w=0.0000 px=$324.18 eq=$99,213.54 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.362s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:51:00+00:00 px=282.79\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.79 eq=$99,213.54 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.560s\n",
            "INFO:root:Perf: cum_return=0.00% | sharpe=27.28 | maxDD=-0.00%\n",
            "INFO:root:[TIMER] full-cycle active time: 1.458s (cooldown=1 min)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2025-12-10/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T18:51:00+00:00 px=324.18\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=120s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3836 conf=0.366 → target_w=0.0000 px=$324.18 eq=$99,214.81 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.360s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:52:00+00:00 px=282.84\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.84 eq=$99,214.81 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.515s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.075s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T18:53:00+00:00 px=324.18\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3836 conf=0.366 → target_w=0.0000 px=$324.18 eq=$99,214.41 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.362s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:53:00+00:00 px=282.78\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.78 eq=$99,214.41 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.557s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.256s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T18:54:00+00:00 px=324.24\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3836 conf=0.366 → target_w=0.0000 px=$324.24 eq=$99,211.47 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.362s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:54:00+00:00 px=282.63\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.63 eq=$99,211.47 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.565s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.122s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T18:55:00+00:00 px=324.29\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3836 conf=0.366 → target_w=0.0000 px=$324.29 eq=$99,211.95 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.380s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:55:00+00:00 px=282.66\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.66 eq=$99,211.95 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.519s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.098s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T18:55:00+00:00 px=324.29\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=120s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3836 conf=0.366 → target_w=0.0000 px=$324.29 eq=$99,211.79 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.374s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:56:00+00:00 px=282.61\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.61 eq=$99,211.79 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.588s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.160s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T18:57:00+00:00 px=324.40\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3836 conf=0.366 → target_w=0.0000 px=$324.40 eq=$99,212.03 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.364s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:57:00+00:00 px=282.64\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.64 eq=$99,212.03 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.531s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.091s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T18:58:00+00:00 px=324.49\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3836 conf=0.366 → target_w=0.0000 px=$324.49 eq=$99,211.47 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.367s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:58:00+00:00 px=282.69\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.69 eq=$99,211.47 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.522s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.086s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T18:58:00+00:00 px=324.49\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=120s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3836 conf=0.366 → target_w=0.0000 px=$324.49 eq=$99,207.49 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.362s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T18:59:00+00:00 px=282.36\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.36 eq=$99,207.49 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.513s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.323s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:00:00+00:00 px=324.88\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3835 conf=0.366 → target_w=0.0000 px=$324.88 eq=$99,220.22 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.408s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:00:00+00:00 px=283.28\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.28 eq=$99,220.22 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.532s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.136s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:01:00+00:00 px=325.16\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3835 conf=0.366 → target_w=0.0000 px=$325.16 eq=$99,216.56 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.358s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:01:00+00:00 px=283.09\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.09 eq=$99,216.56 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.550s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.107s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:02:00+00:00 px=325.23\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3835 conf=0.366 → target_w=0.0000 px=$325.23 eq=$99,212.27 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.365s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:02:00+00:00 px=282.66\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.66 eq=$99,212.27 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.521s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.084s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:02:00+00:00 px=325.23\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=120s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3835 conf=0.366 → target_w=0.0000 px=$325.23 eq=$99,213.14 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.362s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:03:00+00:00 px=282.71\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.71 eq=$99,213.14 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.520s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.078s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:04:00+00:00 px=325.35\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3834 conf=0.366 → target_w=0.0000 px=$325.35 eq=$99,214.81 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.380s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:04:00+00:00 px=282.85\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.85 eq=$99,214.81 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.519s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.214s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:05:00+00:00 px=325.47\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3834 conf=0.366 → target_w=0.0000 px=$325.47 eq=$99,216.88 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.377s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:05:00+00:00 px=283.00\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.00 eq=$99,216.88 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.564s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.138s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:06:00+00:00 px=325.66\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3834 conf=0.366 → target_w=0.0000 px=$325.66 eq=$99,225.00 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.364s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:06:00+00:00 px=283.33\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.33 eq=$99,225.00 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.537s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.098s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:07:00+00:00 px=325.62\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3834 conf=0.366 → target_w=0.0000 px=$325.62 eq=$99,229.78 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.363s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:07:00+00:00 px=283.76\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0453 px=$283.76 eq=$99,229.78 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.518s\n",
            "INFO:root:Perf: cum_return=0.01% | sharpe=31.54 | maxDD=-0.01%\n",
            "INFO:root:[TIMER] full-cycle active time: 1.451s (cooldown=1 min)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2025-12-10/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:08:00+00:00 px=325.49\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3834 conf=0.366 → target_w=0.0000 px=$325.49 eq=$99,231.13 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.392s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:09:00+00:00 px=283.76\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0453 px=$283.76 eq=$99,231.13 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.514s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.102s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:09:00+00:00 px=325.44\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3834 conf=0.366 → target_w=0.0000 px=$325.44 eq=$99,225.96 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.373s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:09:00+00:00 px=283.56\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0453 px=$283.56 eq=$99,225.96 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.526s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.236s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:10:00+00:00 px=325.42\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3834 conf=0.366 → target_w=0.0000 px=$325.42 eq=$99,228.82 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.365s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:10:00+00:00 px=283.77\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0453 px=$283.77 eq=$99,228.82 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.520s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.082s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:11:00+00:00 px=325.25\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3835 conf=0.366 → target_w=0.0000 px=$325.25 eq=$99,224.05 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.363s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:12:00+00:00 px=283.39\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.39 eq=$99,224.05 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.551s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.109s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:12:00+00:00 px=325.06\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3835 conf=0.366 → target_w=0.0000 px=$325.06 eq=$99,223.20 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.361s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:12:00+00:00 px=283.40\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.40 eq=$99,223.20 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.521s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.077s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:13:00+00:00 px=324.98\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3835 conf=0.366 → target_w=0.0000 px=$324.98 eq=$99,222.13 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.371s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:13:00+00:00 px=283.24\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.24 eq=$99,222.13 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.523s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.090s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:14:00+00:00 px=324.98\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3835 conf=0.366 → target_w=0.0000 px=$324.98 eq=$99,218.79 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.379s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:15:00+00:00 px=283.08\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.08 eq=$99,218.79 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.544s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.294s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:15:00+00:00 px=325.08\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3835 conf=0.366 → target_w=0.0000 px=$325.08 eq=$99,227.39 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.381s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:15:00+00:00 px=283.28\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.28 eq=$99,227.39 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.532s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.116s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:16:00+00:00 px=325.31\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3834 conf=0.366 → target_w=0.0000 px=$325.31 eq=$99,221.02 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.365s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:16:00+00:00 px=283.21\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.21 eq=$99,221.02 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.519s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.085s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:17:00+00:00 px=325.22\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3835 conf=0.366 → target_w=0.0000 px=$325.22 eq=$99,217.20 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.361s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:17:00+00:00 px=283.08\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.08 eq=$99,217.20 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.525s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.084s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:17:00+00:00 px=325.22\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=120s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3835 conf=0.366 → target_w=0.0000 px=$325.22 eq=$99,214.65 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.368s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:18:00+00:00 px=282.84\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.84 eq=$99,214.65 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.525s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.095s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:19:00+00:00 px=325.28\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3834 conf=0.366 → target_w=0.0000 px=$325.28 eq=$99,214.10 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.397s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:19:00+00:00 px=282.77\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.77 eq=$99,214.10 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.533s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.243s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:20:00+00:00 px=325.21\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3835 conf=0.366 → target_w=0.0000 px=$325.21 eq=$99,212.66 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.516s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:21:00+00:00 px=282.70\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=1s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.70 eq=$99,212.66 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.532s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.534s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:21:00+00:00 px=325.26\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3834 conf=0.366 → target_w=0.0000 px=$325.26 eq=$99,212.03 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.377s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:22:00+00:00 px=282.69\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.69 eq=$99,212.03 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.523s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.102s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:22:00+00:00 px=325.22\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3835 conf=0.366 → target_w=0.0000 px=$325.22 eq=$99,209.56 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.364s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:23:00+00:00 px=282.49\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.49 eq=$99,209.56 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.540s\n",
            "INFO:root:Perf: cum_return=0.01% | sharpe=14.84 | maxDD=-0.01%\n",
            "INFO:root:[TIMER] full-cycle active time: 1.428s (cooldown=1 min)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2025-12-10/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:23:00+00:00 px=325.06\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3835 conf=0.366 → target_w=0.0000 px=$325.06 eq=$99,205.74 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.386s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:23:00+00:00 px=282.34\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.34 eq=$99,205.74 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.529s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.116s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:24:00+00:00 px=325.00\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3835 conf=0.366 → target_w=0.0000 px=$325.00 eq=$99,204.15 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.400s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:24:00+00:00 px=282.16\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.16 eq=$99,204.15 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.534s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.365s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:25:00+00:00 px=325.06\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3835 conf=0.366 → target_w=0.0000 px=$325.06 eq=$99,202.24 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.361s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:25:00+00:00 px=282.06\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.06 eq=$99,202.24 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.530s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.093s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:26:00+00:00 px=324.95\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3835 conf=0.366 → target_w=0.0000 px=$324.95 eq=$99,200.33 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.364s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:26:00+00:00 px=281.94\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$281.94 eq=$99,200.33 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.511s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.073s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:27:00+00:00 px=324.95\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3835 conf=0.366 → target_w=0.0000 px=$324.95 eq=$99,199.61 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.400s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:27:00+00:00 px=281.87\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$281.87 eq=$99,199.61 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.583s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.178s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:28:00+00:00 px=324.88\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3835 conf=0.366 → target_w=0.0000 px=$324.88 eq=$99,198.26 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.359s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:29:00+00:00 px=281.78\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$281.78 eq=$99,198.26 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.573s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.127s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:29:00+00:00 px=325.37\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3834 conf=0.366 → target_w=0.0000 px=$325.37 eq=$99,204.40 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.444s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:29:00+00:00 px=282.08\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.08 eq=$99,204.40 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.540s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.327s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:30:00+00:00 px=325.77\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3834 conf=0.366 → target_w=0.0000 px=$325.77 eq=$99,207.49 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.397s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:30:00+00:00 px=282.33\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.33 eq=$99,207.49 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.529s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.122s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:30:00+00:00 px=325.77\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=120s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3834 conf=0.366 → target_w=0.0000 px=$325.77 eq=$99,205.42 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.362s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:32:00+00:00 px=282.28\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.28 eq=$99,205.42 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.530s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.089s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:32:00+00:00 px=325.98\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3833 conf=0.366 → target_w=0.0000 px=$325.98 eq=$99,207.70 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.371s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:32:00+00:00 px=282.34\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.34 eq=$99,207.70 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.580s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.152s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:33:00+00:00 px=326.37\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3833 conf=0.366 → target_w=0.0000 px=$326.37 eq=$99,209.32 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.383s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:33:00+00:00 px=282.49\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.49 eq=$99,209.32 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.531s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.111s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:34:00+00:00 px=326.63\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3832 conf=0.366 → target_w=0.0000 px=$326.63 eq=$99,206.85 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.373s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:34:00+00:00 px=282.35\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.35 eq=$99,206.85 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.530s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.105s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:35:00+00:00 px=326.80\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3832 conf=0.365 → target_w=0.0000 px=$326.80 eq=$99,211.63 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.367s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:35:00+00:00 px=282.54\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.54 eq=$99,211.63 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.515s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.191s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:36:00+00:00 px=326.31\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3833 conf=0.366 → target_w=0.0000 px=$326.31 eq=$99,202.40 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.367s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:37:00+00:00 px=281.99\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$281.99 eq=$99,202.40 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.536s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.101s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:37:00+00:00 px=326.33\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3833 conf=0.366 → target_w=0.0000 px=$326.33 eq=$99,200.01 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.361s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:37:00+00:00 px=281.89\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$281.89 eq=$99,200.01 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.515s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.075s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:38:00+00:00 px=326.79\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3832 conf=0.365 → target_w=0.0000 px=$326.79 eq=$99,199.85 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.372s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:38:00+00:00 px=281.93\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$281.93 eq=$99,199.85 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.515s\n",
            "INFO:root:Perf: cum_return=0.01% | sharpe=8.81 | maxDD=-0.02%\n",
            "INFO:root:[TIMER] full-cycle active time: 1.422s (cooldown=1 min)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2025-12-10/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:39:00+00:00 px=327.03\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3832 conf=0.365 → target_w=0.0000 px=$327.03 eq=$99,200.01 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.364s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:39:00+00:00 px=282.03\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.03 eq=$99,200.01 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.541s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.103s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:40:00+00:00 px=326.46\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3833 conf=0.366 → target_w=0.0000 px=$326.46 eq=$99,194.28 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.358s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:40:00+00:00 px=281.42\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$281.42 eq=$99,194.28 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.525s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.304s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:41:00+00:00 px=326.22\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3833 conf=0.366 → target_w=0.0000 px=$326.22 eq=$99,195.87 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.373s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:41:00+00:00 px=281.47\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$281.47 eq=$99,195.87 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.512s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.081s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:42:00+00:00 px=326.11\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3833 conf=0.366 → target_w=0.0000 px=$326.11 eq=$99,196.90 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.381s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:42:00+00:00 px=281.72\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$281.72 eq=$99,196.90 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.533s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.120s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:44:00+00:00 px=325.87\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3834 conf=0.366 → target_w=0.0000 px=$325.87 eq=$99,197.14 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.386s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:43:00+00:00 px=281.91\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$281.91 eq=$99,197.14 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.533s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.127s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:44:00+00:00 px=326.06\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3833 conf=0.366 → target_w=0.0000 px=$326.06 eq=$99,197.78 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.375s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:45:00+00:00 px=281.76\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$281.76 eq=$99,197.78 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.517s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.091s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:45:00+00:00 px=326.53\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3833 conf=0.366 → target_w=0.0000 px=$326.53 eq=$99,199.53 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.361s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:45:00+00:00 px=281.86\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$281.86 eq=$99,199.53 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.520s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.219s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:46:00+00:00 px=326.44\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3833 conf=0.366 → target_w=0.0000 px=$326.44 eq=$99,201.92 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.360s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:46:00+00:00 px=282.01\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.01 eq=$99,201.92 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.515s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.076s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:47:00+00:00 px=326.63\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3832 conf=0.366 → target_w=0.0000 px=$326.63 eq=$99,200.64 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.358s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:47:00+00:00 px=282.04\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.04 eq=$99,200.64 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.518s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.075s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:48:00+00:00 px=326.49\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3833 conf=0.366 → target_w=0.0000 px=$326.49 eq=$99,203.03 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.362s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:49:00+00:00 px=282.07\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.07 eq=$99,203.03 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.515s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.072s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:49:00+00:00 px=326.39\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3833 conf=0.366 → target_w=0.0000 px=$326.39 eq=$99,199.85 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.364s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:49:00+00:00 px=281.93\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$281.93 eq=$99,199.85 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.522s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.083s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:50:00+00:00 px=326.31\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3833 conf=0.366 → target_w=0.0000 px=$326.31 eq=$99,203.03 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.357s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:51:00+00:00 px=282.14\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1832 conf=0.181 → target_w=0.0453 px=$282.14 eq=$99,203.03 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.519s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.072s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:51:00+00:00 px=326.40\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3833 conf=0.366 → target_w=0.0000 px=$326.40 eq=$99,209.00 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.361s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:52:00+00:00 px=282.45\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.45 eq=$99,209.00 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.560s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.254s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:52:00+00:00 px=326.58\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3832 conf=0.366 → target_w=0.0000 px=$326.58 eq=$99,214.18 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.371s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:52:00+00:00 px=282.78\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.78 eq=$99,214.18 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.546s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.115s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:53:00+00:00 px=326.76\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3832 conf=0.366 → target_w=0.0000 px=$326.76 eq=$99,216.72 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.387s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:54:00+00:00 px=283.00\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.00 eq=$99,216.72 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.526s\n",
            "INFO:root:Perf: cum_return=0.00% | sharpe=3.17 | maxDD=-0.03%\n",
            "INFO:root:[TIMER] full-cycle active time: 1.486s (cooldown=1 min)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2025-12-10/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:54:00+00:00 px=326.70\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3832 conf=0.366 → target_w=0.0000 px=$326.70 eq=$99,216.88 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.371s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:54:00+00:00 px=283.02\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.02 eq=$99,216.88 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.520s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.088s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:55:00+00:00 px=326.87\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3832 conf=0.365 → target_w=0.0000 px=$326.87 eq=$99,218.16 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.367s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:56:00+00:00 px=283.03\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.03 eq=$99,218.16 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.521s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.085s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:56:00+00:00 px=326.75\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3832 conf=0.366 → target_w=0.0000 px=$326.75 eq=$99,221.02 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.390s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:56:00+00:00 px=283.31\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.31 eq=$99,221.02 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.571s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.278s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:57:00+00:00 px=326.56\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3833 conf=0.366 → target_w=0.0000 px=$326.56 eq=$99,219.26 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.367s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:57:00+00:00 px=283.07\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.07 eq=$99,219.26 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.528s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.096s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:58:00+00:00 px=326.69\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3832 conf=0.366 → target_w=0.0000 px=$326.69 eq=$99,226.75 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.392s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T19:58:00+00:00 px=283.56\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0453 px=$283.56 eq=$99,226.75 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.533s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.126s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T19:59:00+00:00 px=326.59\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3832 conf=0.366 → target_w=0.0000 px=$326.59 eq=$99,226.35 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.380s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:00:00+00:00 px=283.55\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0453 px=$283.55 eq=$99,226.35 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.552s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.131s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:00:00+00:00 px=326.61\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3832 conf=0.366 → target_w=0.0000 px=$326.61 eq=$99,218.31 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.436s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:00:00+00:00 px=283.00\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.00 eq=$99,218.31 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.539s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.168s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:01:00+00:00 px=326.50\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3833 conf=0.366 → target_w=0.0000 px=$326.50 eq=$99,220.38 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.383s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:01:00+00:00 px=283.28\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.28 eq=$99,220.38 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.541s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.118s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:02:00+00:00 px=326.65\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3832 conf=0.366 → target_w=0.0000 px=$326.65 eq=$99,222.37 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.384s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:03:00+00:00 px=283.30\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.30 eq=$99,222.37 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.559s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.255s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:03:00+00:00 px=326.74\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3832 conf=0.366 → target_w=0.0000 px=$326.74 eq=$99,222.61 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.360s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:03:00+00:00 px=283.33\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.33 eq=$99,222.61 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.534s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.086s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:04:00+00:00 px=326.83\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3832 conf=0.365 → target_w=0.0000 px=$326.83 eq=$99,222.77 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.370s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:04:00+00:00 px=283.32\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.32 eq=$99,222.77 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.509s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.075s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:05:00+00:00 px=327.00\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3832 conf=0.365 → target_w=0.0000 px=$327.00 eq=$99,220.70 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.391s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:05:00+00:00 px=283.16\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.16 eq=$99,220.70 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.524s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.107s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:06:00+00:00 px=326.89\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3832 conf=0.365 → target_w=0.0000 px=$326.89 eq=$99,219.11 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.360s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:06:00+00:00 px=283.14\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.14 eq=$99,219.11 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.505s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.065s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:07:00+00:00 px=327.00\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3832 conf=0.365 → target_w=0.0000 px=$327.00 eq=$99,219.35 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.360s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:07:00+00:00 px=283.11\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.11 eq=$99,219.35 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.532s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.089s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:08:00+00:00 px=327.26\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3831 conf=0.365 → target_w=0.0000 px=$327.26 eq=$99,218.95 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.367s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:08:00+00:00 px=283.08\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.08 eq=$99,218.95 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.558s\n",
            "INFO:root:Perf: cum_return=0.01% | sharpe=11.92 | maxDD=-0.03%\n",
            "INFO:root:[TIMER] full-cycle active time: 1.607s (cooldown=1 min)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2025-12-10/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:08:00+00:00 px=327.26\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=120s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3831 conf=0.365 → target_w=0.0000 px=$327.26 eq=$99,218.31 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.375s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:09:00+00:00 px=283.04\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.04 eq=$99,218.31 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.507s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.074s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:10:00+00:00 px=327.37\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3831 conf=0.365 → target_w=0.0000 px=$327.37 eq=$99,217.85 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.356s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:10:00+00:00 px=282.97\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.97 eq=$99,217.85 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.525s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.076s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:11:00+00:00 px=327.46\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3831 conf=0.365 → target_w=0.0000 px=$327.46 eq=$99,214.26 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.359s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:11:00+00:00 px=282.81\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.81 eq=$99,214.26 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.522s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.081s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:11:00+00:00 px=327.46\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=120s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3831 conf=0.365 → target_w=0.0000 px=$327.46 eq=$99,214.65 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.364s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:12:00+00:00 px=282.79\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.79 eq=$99,214.65 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.512s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.070s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:13:00+00:00 px=327.56\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3831 conf=0.365 → target_w=0.0000 px=$327.56 eq=$99,216.40 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.355s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:13:00+00:00 px=282.92\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.92 eq=$99,216.40 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.509s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.059s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:14:00+00:00 px=327.60\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3831 conf=0.365 → target_w=0.0000 px=$327.60 eq=$99,217.68 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.366s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:14:00+00:00 px=282.96\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.96 eq=$99,217.68 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.547s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.221s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:15:00+00:00 px=327.67\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3831 conf=0.365 → target_w=0.0000 px=$327.67 eq=$99,216.88 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.372s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:15:00+00:00 px=282.88\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$282.88 eq=$99,216.88 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.534s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.103s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:16:00+00:00 px=327.69\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3831 conf=0.365 → target_w=0.0000 px=$327.69 eq=$99,222.45 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.374s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:17:00+00:00 px=283.31\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.31 eq=$99,222.45 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.529s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.100s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:16:00+00:00 px=327.69\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=120s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3831 conf=0.365 → target_w=0.0000 px=$327.69 eq=$99,222.93 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.370s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:17:00+00:00 px=283.33\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.33 eq=$99,222.93 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.517s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.082s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:18:00+00:00 px=327.38\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3831 conf=0.365 → target_w=0.0000 px=$327.38 eq=$99,224.52 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.376s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:18:00+00:00 px=283.30\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1831 conf=0.181 → target_w=0.0453 px=$283.30 eq=$99,224.52 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.540s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.114s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:19:00+00:00 px=327.62\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3831 conf=0.365 → target_w=0.0000 px=$327.62 eq=$99,226.27 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.368s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:19:00+00:00 px=283.55\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0453 px=$283.55 eq=$99,226.27 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.608s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.279s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:20:00+00:00 px=328.15\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$328.15 eq=$99,228.88 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.517s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:20:00+00:00 px=283.73\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=61s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0453 px=$283.73 eq=$99,228.88 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.812s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.526s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:21:00+00:00 px=328.25\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$328.25 eq=$99,238.18 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.360s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:21:00+00:00 px=284.19\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0452 px=$284.19 eq=$99,238.18 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.516s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.070s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:23:00+00:00 px=327.94\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$327.94 eq=$99,233.76 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.391s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:22:00+00:00 px=283.83\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0453 px=$283.83 eq=$99,233.76 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.513s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.100s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:23:00+00:00 px=327.69\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3831 conf=0.365 → target_w=0.0000 px=$327.69 eq=$99,233.60 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.374s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:23:00+00:00 px=284.07\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0452 px=$284.07 eq=$99,233.60 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.547s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.119s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:24:00+00:00 px=327.77\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3831 conf=0.365 → target_w=0.0000 px=$327.77 eq=$99,233.76 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.380s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:24:00+00:00 px=284.04\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0452 px=$284.04 eq=$99,233.76 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.578s\n",
            "INFO:root:Perf: cum_return=0.02% | sharpe=16.63 | maxDD=-0.03%\n",
            "INFO:root:[TIMER] full-cycle active time: 1.684s (cooldown=1 min)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2025-12-10/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:25:00+00:00 px=328.03\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$328.03 eq=$99,245.06 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.365s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:25:00+00:00 px=284.73\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1829 conf=0.181 → target_w=0.0452 px=$284.73 eq=$99,245.06 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.554s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.222s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:26:00+00:00 px=328.17\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$328.17 eq=$99,245.85 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.360s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:26:00+00:00 px=284.75\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1829 conf=0.181 → target_w=0.0452 px=$284.75 eq=$99,245.85 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.514s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.068s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:27:00+00:00 px=328.12\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$328.12 eq=$99,247.45 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.361s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:27:00+00:00 px=284.88\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1829 conf=0.181 → target_w=0.0452 px=$284.88 eq=$99,247.45 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.524s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.080s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:28:00+00:00 px=328.07\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$328.07 eq=$99,245.69 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.375s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:28:00+00:00 px=284.81\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1829 conf=0.181 → target_w=0.0452 px=$284.81 eq=$99,245.69 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.516s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.087s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:28:00+00:00 px=328.07\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=120s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$328.07 eq=$99,242.03 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.364s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:29:00+00:00 px=284.52\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0452 px=$284.52 eq=$99,242.03 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.541s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.100s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:30:00+00:00 px=327.86\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3831 conf=0.365 → target_w=0.0000 px=$327.86 eq=$99,243.47 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.359s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:30:00+00:00 px=284.62\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1829 conf=0.181 → target_w=0.0452 px=$284.62 eq=$99,243.47 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.523s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.081s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:31:00+00:00 px=327.95\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$327.95 eq=$99,243.15 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.358s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:31:00+00:00 px=284.62\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1829 conf=0.181 → target_w=0.0452 px=$284.62 eq=$99,243.15 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.514s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.197s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:32:00+00:00 px=328.04\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$328.04 eq=$99,246.65 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.374s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:32:00+00:00 px=284.85\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1829 conf=0.181 → target_w=0.0452 px=$284.85 eq=$99,246.65 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.681s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.249s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:33:00+00:00 px=328.15\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$328.15 eq=$99,245.38 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.357s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:33:00+00:00 px=284.74\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1829 conf=0.181 → target_w=0.0452 px=$284.74 eq=$99,245.38 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.514s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.063s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:34:00+00:00 px=328.17\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$328.17 eq=$99,242.19 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.358s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:34:00+00:00 px=284.73\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1829 conf=0.181 → target_w=0.0452 px=$284.73 eq=$99,242.19 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.532s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.082s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:35:00+00:00 px=328.25\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$328.25 eq=$99,239.33 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.412s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:35:00+00:00 px=284.39\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0452 px=$284.39 eq=$99,239.33 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.552s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.157s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:36:00+00:00 px=327.97\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$327.97 eq=$99,238.37 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.365s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:36:00+00:00 px=284.25\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0452 px=$284.25 eq=$99,238.37 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.521s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.236s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:37:00+00:00 px=328.00\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$328.00 eq=$99,236.62 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.519s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:37:00+00:00 px=284.26\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0452 px=$284.26 eq=$99,236.62 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.681s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.392s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:38:00+00:00 px=328.04\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$328.04 eq=$99,233.90 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.374s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:39:00+00:00 px=284.05\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0452 px=$284.05 eq=$99,233.90 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.528s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.097s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:39:00+00:00 px=328.06\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$328.06 eq=$99,232.08 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.368s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:39:00+00:00 px=283.85\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0453 px=$283.85 eq=$99,232.08 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.553s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.114s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:40:00+00:00 px=327.79\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3831 conf=0.365 → target_w=0.0000 px=$327.79 eq=$99,232.96 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.366s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:40:00+00:00 px=283.96\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0452 px=$283.96 eq=$99,232.96 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.536s\n",
            "INFO:root:Perf: cum_return=0.03% | sharpe=22.58 | maxDD=-0.03%\n",
            "INFO:root:[TIMER] full-cycle active time: 1.447s (cooldown=1 min)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2025-12-10/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:41:00+00:00 px=327.72\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3831 conf=0.365 → target_w=0.0000 px=$327.72 eq=$99,232.96 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.365s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:41:00+00:00 px=284.08\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0452 px=$284.08 eq=$99,232.96 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.511s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.072s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:42:00+00:00 px=327.68\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3831 conf=0.365 → target_w=0.0000 px=$327.68 eq=$99,231.21 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.354s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:42:00+00:00 px=283.85\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0453 px=$283.85 eq=$99,231.21 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.581s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.272s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:43:00+00:00 px=327.61\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3831 conf=0.365 → target_w=0.0000 px=$327.61 eq=$99,230.73 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.365s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:43:00+00:00 px=283.85\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0453 px=$283.85 eq=$99,230.73 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.525s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.083s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:44:00+00:00 px=327.67\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3831 conf=0.365 → target_w=0.0000 px=$327.67 eq=$99,228.10 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.363s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:44:00+00:00 px=283.67\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0453 px=$283.67 eq=$99,228.10 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.564s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.129s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:45:00+00:00 px=327.84\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3831 conf=0.365 → target_w=0.0000 px=$327.84 eq=$99,230.89 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.382s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:45:00+00:00 px=283.83\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0453 px=$283.83 eq=$99,230.89 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.518s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.095s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:46:00+00:00 px=328.12\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$328.12 eq=$99,231.92 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.377s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:46:00+00:00 px=283.91\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0452 px=$283.91 eq=$99,231.92 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.536s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.105s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:47:00+00:00 px=327.99\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$327.99 eq=$99,230.33 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.401s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:47:00+00:00 px=283.81\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0453 px=$283.81 eq=$99,230.33 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.549s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.258s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:48:00+00:00 px=327.81\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3831 conf=0.365 → target_w=0.0000 px=$327.81 eq=$99,229.14 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.384s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:48:00+00:00 px=283.71\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0453 px=$283.71 eq=$99,229.14 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.537s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.118s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:49:00+00:00 px=328.08\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$328.08 eq=$99,232.00 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.381s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:50:00+00:00 px=284.00\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0452 px=$284.00 eq=$99,232.00 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.558s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.135s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:50:00+00:00 px=328.02\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$328.02 eq=$99,232.40 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.379s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:50:00+00:00 px=283.93\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0452 px=$283.93 eq=$99,232.40 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.540s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.119s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:51:00+00:00 px=328.02\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$328.02 eq=$99,231.92 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.368s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:52:00+00:00 px=283.89\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0452 px=$283.89 eq=$99,231.92 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.548s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.110s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:52:00+00:00 px=328.01\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$328.01 eq=$99,234.87 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.371s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:52:00+00:00 px=284.17\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0452 px=$284.17 eq=$99,234.87 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.521s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.086s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:53:00+00:00 px=328.05\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$328.05 eq=$99,233.28 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.363s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:53:00+00:00 px=283.98\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0452 px=$283.98 eq=$99,233.28 have=15.918472631\n",
            "INFO:root:[TIMER] GE symbol work: 0.531s\n",
            "INFO:root:[GE] close_position submitted\n",
            "INFO:root:Flattened all positions into the close.\n",
            "INFO:root:[TIMER] full-cycle active time: 1.579s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:54:00+00:00 px=328.29\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$328.29 eq=$99,232.64 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.363s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:55:00+00:00 px=284.08\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0452 px=$284.08 eq=$99,232.64 have=0.0\n",
            "INFO:root:[GE] Submitted buy notional=$7.50\n",
            "INFO:root:[GE] Submitted buy notional=$4482.29\n",
            "INFO:root:[TIMER] GE symbol work: 0.654s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.322s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:55:00+00:00 px=328.48\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$328.48 eq=$99,233.90 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.360s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:56:00+00:00 px=284.10\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0452 px=$284.10 eq=$99,233.90 have=15.807926036\n",
            "INFO:root:[TIMER] GE symbol work: 0.526s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.079s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:56:00+00:00 px=328.69\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3829 conf=0.365 → target_w=0.0000 px=$328.69 eq=$99,233.74 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.391s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:56:00+00:00 px=284.07\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0452 px=$284.07 eq=$99,233.74 have=15.807926036\n",
            "INFO:root:[TIMER] GE symbol work: 0.538s\n",
            "INFO:root:Perf: cum_return=0.03% | sharpe=16.84 | maxDD=-0.03%\n",
            "INFO:root:[TIMER] full-cycle active time: 1.428s (cooldown=1 min)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved equity curve → /content/drive/MyDrive/AlpacaPaper/results/2025-12-10/equity_curve.png\n",
            "Updated latest copy → /content/drive/MyDrive/AlpacaPaper/results/latest/equity_curve.png\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:57:00+00:00 px=328.17\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$328.17 eq=$99,229.48 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.388s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:58:00+00:00 px=283.82\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0453 px=$283.82 eq=$99,229.48 have=15.807926036\n",
            "INFO:root:[TIMER] GE symbol work: 0.535s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.116s (cooldown=1 min)\n",
            "INFO:root:[UNH] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[UNH] Patched stale bars with synthetic trade bar @ 2025-12-10T20:58:00+00:00 px=328.38\n",
            "INFO:root:[UNH] obs_shape=(10, 2) | exp_shape=(10, 2) | age=60s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[UNH] raw=-0.3830 conf=0.365 → target_w=0.0000 px=$328.38 eq=$99,225.68 have=0.0\n",
            "INFO:root:[TIMER] UNH symbol work: 0.373s\n",
            "INFO:root:[GE] fetching 200 1Min bars (feed='iex')\n",
            "INFO:root:[GE] Patched stale bars with synthetic trade bar @ 2025-12-10T20:59:00+00:00 px=283.57\n",
            "INFO:root:[GE] obs_shape=(10, 2) | exp_shape=(10, 2) | age=0s | vecnorm=VecNormalize(training=False, norm_reward=False)\n",
            "INFO:root:[GE] raw=0.1830 conf=0.181 → target_w=0.0453 px=$283.57 eq=$99,225.68 have=15.807926036\n",
            "INFO:root:[TIMER] GE symbol work: 0.531s\n",
            "INFO:root:[TIMER] full-cycle active time: 1.097s (cooldown=1 min)\n",
            "INFO:root:Market closed. Sleeping 62999s until next open.\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "#Python standard library\n",
        "import csv\n",
        "import gc\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "import shutil\n",
        "import time\n",
        "import warnings\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from decimal import Decimal, ROUND_DOWN, ROUND_HALF_UP\n",
        "from functools import lru_cache\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Mapping, Optional, Tuple, Union\n",
        "\n",
        "#Scientific / data stack\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "#Alpaca trading API\n",
        "import alpaca_trade_api as tradeapi\n",
        "from alpaca_trade_api.rest import APIError, TimeFrame\n",
        "\n",
        "#Reinforcement learning models\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import VecNormalize\n",
        "\n",
        "\n",
        "#(Optional) Colab helpers\n",
        "IN_COLAB = False\n",
        "try:\n",
        "    import google.colab  #type: ignore\n",
        "    from google.colab import drive, files  #type: ignore\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "#Utils / Paths\n",
        "def round_to_cents(x: float) -> float:\n",
        "    return float(Decimal(str(x)).quantize(Decimal(\"0.01\"), rounding=ROUND_DOWN))\n",
        "\n",
        "if IN_COLAB:\n",
        "    try:\n",
        "        drive.mount(\"/content/drive\", force_remount=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "#Project root (Drive in Colab; cwd locally)\n",
        "if IN_COLAB:\n",
        "    PROJECT_ROOT = Path(\"/content/drive/MyDrive/AlpacaPaper\")\n",
        "else:\n",
        "    PROJECT_ROOT = Path.cwd() / \"AlpacaPaper\"\n",
        "PROJECT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#--- lightweight per-symbol cooldown to prevent rapid re-fires ---\n",
        "_LAST_ORDER_TS: dict = {}\n",
        "\n",
        "SESSION_OPEN_EQUITY: Optional[float] = None\n",
        "_last_kill_ts: float = 0.0\n",
        "\n",
        "#Faster cooldown for the very first seed fill (rebalances still use 30s)\n",
        "_SEED_COOLDOWN_SEC = 10\n",
        "\n",
        "_NO_POS_CYCLE_COUNT: Dict[str, int] = {}\n",
        "\n",
        "#Re-entry cooldown after a forced flatten (end-of-day, kill-switch, manual flatten).\n",
        "_REENTRY_BLOCK_UNTIL: Dict[str, float] = {}\n",
        "REENTRY_COOLDOWN_SEC = int(os.getenv(\"REENTRY_COOLDOWN_SEC\", \"300\"))\n",
        "\n",
        "#End-of-day flatten policy. If enabled, flatten positions into the final minutes.\n",
        "FLATTEN_INTO_CLOSE = os.getenv(\"FLATTEN_INTO_CLOSE\", \"0\").strip().lower() in (\"1\", \"true\", \"yes\", \"on\")\n",
        "FORCE_FIRST_BUY = os.getenv(\"FORCE_FIRST_BUY\", \"0\").strip().lower() in (\"1\",\"true\",\"yes\",\"on\")\n",
        "\n",
        "def _too_soon(symbol: str, min_gap_sec: int = 30) -> bool:\n",
        "    now = time.time()\n",
        "    last = _LAST_ORDER_TS.get(symbol, 0.0)\n",
        "    if (now - last) < float(min_gap_sec):\n",
        "        return True\n",
        "    _LAST_ORDER_TS[symbol] = now\n",
        "    return False\n",
        "\n",
        "#---------------------------------- Upload / Conversion Helpers --------------------------------\n",
        "def upload_env_and_artifacts_in_colab():\n",
        "    \"\"\"Prompt for .env and model artifacts when running in Colab.\"\"\"\n",
        "    if not IN_COLAB:\n",
        "        return\n",
        "\n",
        "    target_dir = Path(os.getenv(\"ARTIFACTS_DIR\", str(PROJECT_ROOT / \"artifacts\")))\n",
        "    target_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(\"Upload your .env (or Alpaca_keys.env.txt). Cancel if already on Drive.\")\n",
        "    up = files.upload()\n",
        "    if up:\n",
        "        if \"Alpaca_keys.env.txt\" in up:\n",
        "            src = Path(\"Alpaca_keys.env.txt\")\n",
        "            dst = PROJECT_ROOT / \".env\"\n",
        "            shutil.move(str(src), str(dst))\n",
        "            print(f\"Saved env → {dst}\")\n",
        "        elif \".env\" in up:\n",
        "            src = Path(\".env\")\n",
        "            dst = PROJECT_ROOT / \".env\"\n",
        "            shutil.move(str(src), str(dst))\n",
        "            print(f\"Saved env → {dst}\")\n",
        "        else:\n",
        "            any_name = next(iter(up.keys()))\n",
        "            src = Path(any_name)\n",
        "            dst = PROJECT_ROOT / \".env\"\n",
        "            shutil.move(str(src), str(dst))\n",
        "            print(f\"Saved env (renamed {any_name}) → {dst}\")\n",
        "\n",
        "    print(\"Upload your artifacts (ppo_*_model.zip, *_vecnorm*.pkl, *_features*.json or .txt).\")\n",
        "    up2 = files.upload()\n",
        "    for name in up2.keys():\n",
        "        shutil.move(name, target_dir / name)\n",
        "    print(\"Artifacts now in:\", sorted(p.name for p in target_dir.iterdir()))\n",
        "\n",
        "def _maybe_convert_features_txt_to_json():\n",
        "    \"\"\"Convert any 'features_<TICKER>.txt' into 'ppo_<TICKER>_features.json' (simple list).\"\"\"\n",
        "    art_dir = Path(os.getenv(\"ARTIFACTS_DIR\", str(PROJECT_ROOT / \"artifacts\")))\n",
        "    art_dir.mkdir(parents=True, exist_ok=True)\n",
        "    for p in art_dir.glob(\"features_*.txt\"):\n",
        "        ticker = re.sub(r\"^features_|\\.txt$\", \"\", p.name, flags=re.IGNORECASE)\n",
        "        try:\n",
        "            raw = p.read_text().strip()\n",
        "            items = [x.strip() for x in raw.replace(\",\", \"\\n\").splitlines() if x.strip()]\n",
        "            out = {\"features\": items}\n",
        "            out_path = art_dir / f\"ppo_{ticker}_features.json\"\n",
        "            out_path.write_text(json.dumps(out, indent=2))\n",
        "            print(f\"Converted {p.name} → {out_path.name}  ({len(items)} features)\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not convert {p.name}: {e}\")\n",
        "\n",
        "def _maybe_rename_vecnorm_scaler():\n",
        "    \"\"\"Rename any 'scaler_<TICKER>.pkl' to 'ppo_<TICKER>_vecnorm.pkl'.\"\"\"\n",
        "    art_dir = Path(os.getenv(\"ARTIFACTS_DIR\", str(PROJECT_ROOT / \"artifacts\")))\n",
        "    art_dir.mkdir(parents=True, exist_ok=True)\n",
        "    for p in art_dir.glob(\"scaler_*.pkl\"):\n",
        "        ticker = re.sub(r\"^scaler_|\\.pkl$\", \"\", p.name, flags=re.IGNORECASE)\n",
        "        dst = art_dir / f\"ppo_{ticker}_vecnorm.pkl\"\n",
        "        if not dst.exists():\n",
        "            shutil.move(str(p), str(dst))\n",
        "            print(f\"Renamed {p.name} → {dst.name}\")\n",
        "\n",
        "#---------------------------------- Env & logging ---------------------------------------------\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#Load env (supports PROJECT_ROOT/.env)\n",
        "env_candidates = [PROJECT_ROOT / \".env\", Path(\".env\")]\n",
        "for env_path in env_candidates:\n",
        "    if env_path.exists():\n",
        "        load_dotenv(dotenv_path=env_path, override=True)\n",
        "        break\n",
        "else:\n",
        "    load_dotenv(override=True)  #fallback\n",
        "\n",
        "#Default timeout for portfolio history fetches (overridable via env)\n",
        "os.environ.setdefault(\"PH_TIMEOUT_SEC\", \"8\")\n",
        "\n",
        "#DEBUG idle-seed knobs (env-driven defaults). Override in .env or shell.\n",
        "os.environ.setdefault(\"DEBUG_FORCE_SEED_IF_IDLE\", \"0\")\n",
        "os.environ.setdefault(\"DEBUG_SEED_IDLE_CYCLES\", \"10\")  #leave default 10; set 3 in .env while testing\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
        "logging.getLogger().setLevel(\n",
        "    getattr(logging, os.getenv(\"LOG_LEVEL\", \"INFO\").upper(), logging.INFO)\n",
        ")\n",
        "\n",
        "\n",
        "#---------------------------------- Config dataclass -------------------------------------------\n",
        "def _to_bool(x: str) -> bool:\n",
        "    return str(x).strip().lower() in (\"1\",\"true\",\"yes\",\"y\",\"on\")\n",
        "\n",
        "def _to_list_csv(x: str) -> list:\n",
        "    return [s.strip().upper() for s in str(x).split(\",\") if s.strip()]\n",
        "\n",
        "@dataclass\n",
        "class Knobs:\n",
        "    #API / mode\n",
        "    APCA_API_BASE_URL: str = \"https://paper-api.alpaca.markets\"\n",
        "    DRY_RUN: bool          = False        #False => place PAPER orders on PAPER endpoint\n",
        "    AUTO_RUN_LIVE: bool    = True\n",
        "    INF_DETERMINISTIC: bool= True\n",
        "\n",
        "    #Equity logging controls\n",
        "    EQUITY_LOG_THROTTLE_SEC: int = 900     #log at most every 15m unless a trade happens\n",
        "    SKIP_EQUITY_WHEN_DRY_RUN: bool = True  #don’t log equity during dry-run\n",
        "\n",
        "    #Universe / files\n",
        "    TICKERS: list          = None\n",
        "    ARTIFACTS_DIR: str     = \"\"\n",
        "    RESULTS_ROOT: str      = \"\"\n",
        "\n",
        "    #Data feed / cadence / staleness\n",
        "    BARS_FEED: str         = \"iex\"          #\"\" lets Alpaca choose; \"iex\" for IEX\n",
        "    COOLDOWN_MIN: int      = 1\n",
        "    STALE_MAX_SEC: int     = 600\n",
        "\n",
        "    #Sizing & entry/exit sensitivity\n",
        "    SIZING_MODE: str       = \"linear\"    #\"linear\" | \"threshold\"\n",
        "    WEIGHT_CAP: float      = 0.35\n",
        "    CONF_FLOOR: float      = 0.20        #threshold-mode only\n",
        "    ENTER_CONF_MIN: float  = 0.00\n",
        "    ENTER_WEIGHT_MIN: float= 0.003\n",
        "    EXIT_WEIGHT_MAX: float = 0.004\n",
        "    REBALANCE_MIN_NOTIONAL: float = 5.00\n",
        "    USE_FRACTIONALS: bool  = True\n",
        "    SEED_FIRST_SHARE: bool = True\n",
        "    ALLOW_SHORTS: bool     = False\n",
        "\n",
        "    #add-ons\n",
        "    DELTA_WEIGHT_MIN: float = 0.000\n",
        "    RAW_POS_MIN: float = -1.0\n",
        "    RAW_NEG_MAX: float = 0.00\n",
        "\n",
        "    #Risk\n",
        "    TAKE_PROFIT_PCT: float = 0.05\n",
        "    STOP_LOSS_PCT: float   = 0.03\n",
        "\n",
        "    #Misc\n",
        "    STALE_BEST_WINDOW: str = \"\"    #e.g. \"3\" (exposed as BEST_WINDOW_ENV)\n",
        "\n",
        "    #Secrets\n",
        "    APCA_API_KEY_ID: str   = \"\"\n",
        "    APCA_API_SECRET_KEY: str = \"\"\n",
        "\n",
        "    #kill-switch\n",
        "    MAX_DAILY_DRAWDOWN_PCT: float = 0.05   #flatten if equity falls 5% from session open\n",
        "    KILL_SWITCH_COOLDOWN_MIN: int = 30\n",
        "\n",
        "    #exit\n",
        "    EXIT_AFTER_CLOSE: bool = False\n",
        "\n",
        "    @classmethod\n",
        "    def from_env(cls, defaults: \"Knobs\", project_root: Path, env: Mapping[str, str], overrides: Mapping[str, object] = None):\n",
        "        kv = {**defaults.__dict__}\n",
        "        kv.update({\n",
        "            \"APCA_API_BASE_URL\": env.get(\"APCA_API_BASE_URL\", kv[\"APCA_API_BASE_URL\"]),\n",
        "            \"AUTO_RUN_LIVE\":     _to_bool(env.get(\"AUTO_RUN_LIVE\", str(kv[\"AUTO_RUN_LIVE\"]))),\n",
        "            \"DRY_RUN\":           _to_bool(env.get(\"DRY_RUN\",       str(kv[\"DRY_RUN\"]))),\n",
        "            \"INF_DETERMINISTIC\": _to_bool(env.get(\"INF_DETERMINISTIC\", str(kv[\"INF_DETERMINISTIC\"]))),\n",
        "\n",
        "            \"EQUITY_LOG_THROTTLE_SEC\": int(env.get(\"EQUITY_LOG_THROTTLE_SEC\", str(kv[\"EQUITY_LOG_THROTTLE_SEC\"]))),\n",
        "            \"SKIP_EQUITY_WHEN_DRY_RUN\": _to_bool(env.get(\"SKIP_EQUITY_WHEN_DRY_RUN\", str(kv[\"SKIP_EQUITY_WHEN_DRY_RUN\"]))),\n",
        "\n",
        "            \"USE_FRACTIONALS\":   _to_bool(env.get(\"USE_FRACTIONALS\", str(kv[\"USE_FRACTIONALS\"]))),\n",
        "            \"SEED_FIRST_SHARE\":  _to_bool(env.get(\"SEED_FIRST_SHARE\", str(kv[\"SEED_FIRST_SHARE\"]))),\n",
        "            \"ALLOW_SHORTS\":      _to_bool(env.get(\"ALLOW_SHORTS\",     str(kv[\"ALLOW_SHORTS\"]))),\n",
        "\n",
        "            \"TICKERS\":           _to_list_csv(env.get(\"TICKERS\", \",\".join(kv[\"TICKERS\"] or [\"UNH\",\"GE\"]))),\n",
        "            \"ARTIFACTS_DIR\":     env.get(\"ARTIFACTS_DIR\", kv[\"ARTIFACTS_DIR\"] or str(project_root / \"artifacts\")),\n",
        "            \"RESULTS_ROOT\":      env.get(\"RESULTS_ROOT\",  kv[\"RESULTS_ROOT\"]  or str(project_root / \"results\")),\n",
        "\n",
        "            \"BARS_FEED\":         env.get(\"BARS_FEED\", kv[\"BARS_FEED\"]),\n",
        "            \"COOLDOWN_MIN\":      int(env.get(\"COOLDOWN_MIN\", str(kv[\"COOLDOWN_MIN\"])) or kv[\"COOLDOWN_MIN\"]),\n",
        "            \"STALE_MAX_SEC\":     int(env.get(\"STALE_MAX_SEC\", str(kv[\"STALE_MAX_SEC\"])) or kv[\"STALE_MAX_SEC\"]),\n",
        "\n",
        "            \"SIZING_MODE\":       env.get(\"SIZING_MODE\", kv[\"SIZING_MODE\"]),\n",
        "            \"WEIGHT_CAP\":        float(env.get(\"WEIGHT_CAP\",        str(kv[\"WEIGHT_CAP\"]))),\n",
        "            \"CONF_FLOOR\":        float(env.get(\"CONF_FLOOR\",        str(kv[\"CONF_FLOOR\"]))),\n",
        "            \"ENTER_CONF_MIN\":    float(env.get(\"ENTER_CONF_MIN\",    str(kv[\"ENTER_CONF_MIN\"]))),\n",
        "            \"ENTER_WEIGHT_MIN\":  float(env.get(\"ENTER_WEIGHT_MIN\",  str(kv[\"ENTER_WEIGHT_MIN\"]))),\n",
        "            \"EXIT_WEIGHT_MAX\":   float(env.get(\"EXIT_WEIGHT_MAX\",   str(kv[\"EXIT_WEIGHT_MAX\"]))),\n",
        "            \"REBALANCE_MIN_NOTIONAL\": float(env.get(\"REBALANCE_MIN_NOTIONAL\", str(kv[\"REBALANCE_MIN_NOTIONAL\"]))),\n",
        "\n",
        "            \"TAKE_PROFIT_PCT\":   float(env.get(\"TAKE_PROFIT_PCT\",   str(kv[\"TAKE_PROFIT_PCT\"]))),\n",
        "            \"STOP_LOSS_PCT\":     float(env.get(\"STOP_LOSS_PCT\",     str(kv[\"STOP_LOSS_PCT\"]))),\n",
        "\n",
        "            \"DELTA_WEIGHT_MIN\": float(env.get(\"DELTA_WEIGHT_MIN\", str(kv.get(\"DELTA_WEIGHT_MIN\", 0.02)))),\n",
        "            \"RAW_POS_MIN\":      float(env.get(\"RAW_POS_MIN\",      str(kv.get(\"RAW_POS_MIN\", 0.00)))),\n",
        "            \"RAW_NEG_MAX\":      float(env.get(\"RAW_NEG_MAX\",      str(kv.get(\"RAW_NEG_MAX\", 0.00)))),\n",
        "            \"EXIT_AFTER_CLOSE\": _to_bool(env.get(\"EXIT_AFTER_CLOSE\", str(kv.get(\"EXIT_AFTER_CLOSE\", False)))),\n",
        "\n",
        "            \"STALE_BEST_WINDOW\": env.get(\"BEST_WINDOW\", kv[\"STALE_BEST_WINDOW\"]),\n",
        "        })\n",
        "        kv[\"APCA_API_KEY_ID\"]     = env.get(\"APCA_API_KEY_ID\")     or env.get(\"ALPACA_API_KEY_ID\", \"\")     or \"\"\n",
        "        kv[\"APCA_API_SECRET_KEY\"] = env.get(\"APCA_API_SECRET_KEY\") or env.get(\"ALPACA_API_SECRET_KEY\", \"\") or \"\"\n",
        "        if overrides:\n",
        "            for k, v in overrides.items():\n",
        "                key = str(k)\n",
        "                if key.upper() == \"TICKERS\" and isinstance(v, str):\n",
        "                    v = _to_list_csv(v)\n",
        "                kv[key] = v\n",
        "        return cls(**kv)\n",
        "\n",
        "    def apply_to_globals(self):\n",
        "        g = globals()\n",
        "        g[\"BASE_URL\"]           = self.APCA_API_BASE_URL\n",
        "        g[\"DRY_RUN\"]            = bool(self.DRY_RUN)\n",
        "        g[\"INF_DETERMINISTIC\"]  = bool(self.INF_DETERMINISTIC)\n",
        "\n",
        "        g[\"TICKERS\"]            = list(self.TICKERS or [\"UNH\",\"GE\"])\n",
        "        g[\"ARTIFACTS_DIR\"]      = Path(self.ARTIFACTS_DIR)\n",
        "        g[\"RESULTS_ROOT\"]       = Path(self.RESULTS_ROOT)\n",
        "        g[\"RESULTS_DIR\"]        = RESULTS_ROOT / datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
        "        g[\"LATEST_DIR\"]         = RESULTS_ROOT / \"latest\"\n",
        "        for p in (ARTIFACTS_DIR, RESULTS_DIR, LATEST_DIR):\n",
        "            p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        g[\"BARS_FEED\"]          = str(self.BARS_FEED).strip()\n",
        "        g[\"COOLDOWN_MIN\"]       = int(self.COOLDOWN_MIN)\n",
        "        g[\"STALE_MAX_SEC\"]      = int(self.STALE_MAX_SEC)\n",
        "\n",
        "        g[\"SIZING_MODE\"]        = self.SIZING_MODE\n",
        "        g[\"WEIGHT_CAP\"]         = float(self.WEIGHT_CAP)\n",
        "        g[\"ENTER_CONF_MIN\"]     = float(self.ENTER_CONF_MIN)\n",
        "        g[\"ENTER_WEIGHT_MIN\"]   = float(self.ENTER_WEIGHT_MIN)\n",
        "        g[\"EXIT_WEIGHT_MAX\"]    = float(self.EXIT_WEIGHT_MAX)\n",
        "        g[\"REBALANCE_MIN_NOTIONAL\"] = float(self.REBALANCE_MIN_NOTIONAL)\n",
        "        g[\"USE_FRACTIONALS\"]    = bool(self.USE_FRACTIONALS)\n",
        "        g[\"SEED_FIRST_SHARE\"]   = bool(self.SEED_FIRST_SHARE)\n",
        "        g[\"ALLOW_SHORTS\"]       = bool(self.ALLOW_SHORTS)\n",
        "        g[\"CONF_FLOOR\"]         = float(self.CONF_FLOOR)\n",
        "        g[\"TAKE_PROFIT_PCT\"]    = float(self.TAKE_PROFIT_PCT)\n",
        "        g[\"STOP_LOSS_PCT\"]      = float(self.STOP_LOSS_PCT)\n",
        "\n",
        "        g[\"BEST_WINDOW_ENV\"]    = (self.STALE_BEST_WINDOW or None)\n",
        "\n",
        "        g[\"API_KEY\"]    = self.APCA_API_KEY_ID or \"\"\n",
        "        g[\"API_SECRET\"] = self.APCA_API_SECRET_KEY or \"\"\n",
        "\n",
        "        g[\"TRADE_LOG_CSV\"]      = RESULTS_DIR / \"trade_log_master.csv\"\n",
        "        g[\"EQUITY_LOG_CSV\"]     = RESULTS_DIR / \"equity_log.csv\"\n",
        "        g[\"PLOT_PATH\"]          = RESULTS_DIR / \"equity_curve.png\"\n",
        "        g[\"PLOT_PATH_LATEST\"]   = LATEST_DIR / \"equity_curve.png\"\n",
        "        g[\"EQUITY_LOG_LATEST\"]  = LATEST_DIR / \"equity_log.csv\"\n",
        "        g[\"TRADE_LOG_LATEST\"]   = LATEST_DIR / \"trade_log_master.csv\"\n",
        "        g[\"DELTA_WEIGHT_MIN\"]   = float(self.DELTA_WEIGHT_MIN)\n",
        "        g[\"RAW_POS_MIN\"]        = float(self.RAW_POS_MIN)\n",
        "        g[\"RAW_NEG_MAX\"]        = float(self.RAW_NEG_MAX)\n",
        "\n",
        "        os.environ[\"APCA_API_BASE_URL\"] = self.APCA_API_BASE_URL\n",
        "        os.environ[\"DRY_RUN\"]           = \"1\" if self.DRY_RUN else \"0\"\n",
        "        os.environ[\"AUTO_RUN_LIVE\"]     = \"1\" if self.AUTO_RUN_LIVE else \"0\"\n",
        "        os.environ[\"BARS_FEED\"]         = self.BARS_FEED\n",
        "\n",
        "        g[\"EQUITY_LOG_THROTTLE_SEC\"]   = int(self.EQUITY_LOG_THROTTLE_SEC)\n",
        "        g[\"SKIP_EQUITY_WHEN_DRY_RUN\"]  = bool(self.SKIP_EQUITY_WHEN_DRY_RUN)\n",
        "        g[\"_LAST_EQUITY_LOG_TS\"]       = 0\n",
        "        g[\"_TRADE_EVENT_FLAG\"]         = False   #set true when an order is (would be) submitted\n",
        "\n",
        "        g[\"MAX_DAILY_DRAWDOWN_PCT\"]  = float(self.MAX_DAILY_DRAWDOWN_PCT)\n",
        "        g[\"KILL_SWITCH_COOLDOWN_MIN\"] = int(self.KILL_SWITCH_COOLDOWN_MIN)\n",
        "        g[\"EXIT_AFTER_CLOSE\"] = bool(self.EXIT_AFTER_CLOSE)\n",
        "        os.environ[\"EXIT_AFTER_CLOSE\"] = \"1\" if self.EXIT_AFTER_CLOSE else \"0\"\n",
        "\n",
        "\n",
        "def configure_knobs(overrides: Mapping[str, object] = None) -> Knobs:\n",
        "    defaults = Knobs(\n",
        "        TICKERS=_to_list_csv(os.getenv(\"TICKERS\", \"UNH,GE\")),\n",
        "        ARTIFACTS_DIR=os.getenv(\"ARTIFACTS_DIR\", str(PROJECT_ROOT / \"artifacts\")),\n",
        "        RESULTS_ROOT=os.getenv(\"RESULTS_ROOT\",  str(PROJECT_ROOT / \"results\")),\n",
        "    )\n",
        "    cfg = Knobs.from_env(defaults, PROJECT_ROOT, os.environ, overrides=overrides)\n",
        "    cfg.apply_to_globals()\n",
        "    return cfg\n",
        "\n",
        "#---------------------------------- Time helpers -----------------------------------------------\n",
        "def now_utc() -> datetime:\n",
        "    return datetime.now(timezone.utc)\n",
        "\n",
        "def utc_ts(dt_like) -> int:\n",
        "    if isinstance(dt_like, (int, np.integer)):\n",
        "        return int(dt_like)\n",
        "    if isinstance(dt_like, (float, np.floating)):\n",
        "        return int(dt_like)\n",
        "    ts = pd.Timestamp(dt_like)\n",
        "    if ts.tzinfo is None:\n",
        "        ts = ts.tz_localize(\"UTC\")\n",
        "    else:\n",
        "        ts = ts.tz_convert(\"UTC\")\n",
        "    return int(ts.value // 10**9)\n",
        "\n",
        "def utcnow_iso() -> str:\n",
        "    return datetime.now(timezone.utc).isoformat()\n",
        "\n",
        "def _sleep_to_next_minute_block(n: int):\n",
        "    n = max(1, int(n))\n",
        "    now = now_utc()\n",
        "    base = now.replace(second=0, microsecond=0)\n",
        "    remainder = base.minute % n\n",
        "    add = n if remainder == 0 else (n - remainder)\n",
        "    next_slot = base + timedelta(minutes=add)\n",
        "    time.sleep(max(0, (next_slot - now).total_seconds()))\n",
        "\n",
        "#--------------------------------- CSV logging (master, optional) ------------------------------\n",
        "#put near the top (after paths)\n",
        "TRADE_FIELDS = [\"datetime_utc\",\"ticker\",\"signal\",\"action\",\"price\",\"equity\",\"qty\",\"comment\"]\n",
        "\n",
        "def ensure_trade_log_header():\n",
        "    if not TRADE_LOG_CSV.exists():\n",
        "        pd.DataFrame(columns=TRADE_FIELDS).to_csv(TRADE_LOG_CSV, index=False)\n",
        "\n",
        "def log_trade(ticker:str, signal:float, action:str, price:float, equity:float, qty:float=None, comment:str=\"\"):\n",
        "    ensure_trade_log_header()\n",
        "    row = {\n",
        "        \"datetime_utc\": utcnow_iso(),\n",
        "        \"ticker\": ticker,\n",
        "        \"signal\": int(signal) if signal is not None else \"\",\n",
        "        \"action\": action,\n",
        "        \"price\": (float(price)  if price  is not None and np.isfinite(price)  else \"\"),\n",
        "        \"equity\": (float(equity) if equity is not None and np.isfinite(equity) else \"\"),\n",
        "        \"qty\":    (float(qty)    if qty    is not None and np.isfinite(qty)    else \"\"),\n",
        "        \"comment\": (str(comment) if comment else \"\")\n",
        "    }\n",
        "    with TRADE_LOG_CSV.open(\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        csv.DictWriter(f, fieldnames=TRADE_FIELDS).writerow(row)\n",
        "    try: shutil.copy2(TRADE_LOG_CSV, TRADE_LOG_LATEST)\n",
        "    except Exception: pass\n",
        "\n",
        "#--------------------------------- Alpaca API init --------------------------------------------\n",
        "def init_alpaca() -> \"tradeapi.REST\":\n",
        "    if not (globals().get(\"API_KEY\") and globals().get(\"API_SECRET\")):\n",
        "        raise RuntimeError(\"Missing Alpaca API keys (check your .env).\")\n",
        "    return tradeapi.REST(API_KEY, API_SECRET, base_url=BASE_URL)\n",
        "\n",
        "#-------- Timeout-safe Alpaca calls for portfolio history --------\n",
        "from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError\n",
        "\n",
        "def _call_with_timeout(func, timeout_sec: int, *args, **kwargs):\n",
        "    with ThreadPoolExecutor(max_workers=1) as ex:\n",
        "        fut = ex.submit(func, *args, **kwargs)\n",
        "        try:\n",
        "            return fut.result(timeout=timeout_sec)\n",
        "        except FuturesTimeoutError:\n",
        "            raise TimeoutError(f\"Timed out after {timeout_sec}s\")\n",
        "        except Exception as e:\n",
        "            raise e\n",
        "\n",
        "def get_portfolio_history_safe(api, period=\"1M\", timeframe=\"1H\", timeout_sec: int = 8, retries: int = 1):\n",
        "    \"\"\"Call get_portfolio_history with a hard timeout and light retry.\"\"\"\n",
        "    last_exc = None\n",
        "    for _ in range(max(1, retries + 1)):\n",
        "        try:\n",
        "            return _call_with_timeout(api.get_portfolio_history, timeout_sec, period=period, timeframe=timeframe)\n",
        "        except Exception as e:\n",
        "            last_exc = e\n",
        "            time.sleep(0.5)\n",
        "    logging.warning(f\"get_portfolio_history_safe failed: {last_exc}\")\n",
        "    return None\n",
        "\n",
        "#------------------------- Portfolio equity logging + metrics ---------------------------------\n",
        "def fetch_portfolio_history(period=\"1M\", timeframe=\"1H\", api_in=None):\n",
        "    a = api_in if api_in is not None else globals().get(\"api\", None)\n",
        "    if a is None:\n",
        "        return pd.DataFrame(columns=[\"timestamp_utc\", \"equity\"])\n",
        "\n",
        "    hist = get_portfolio_history_safe(\n",
        "        a, period=period, timeframe=timeframe,\n",
        "        timeout_sec=int(os.getenv(\"PH_TIMEOUT_SEC\", \"8\")), retries=1\n",
        "    )\n",
        "\n",
        "    #Single guard + local CSV fallback\n",
        "    if (not hist) or (not getattr(hist, \"timestamp\", None)) or (not getattr(hist, \"equity\", None)):\n",
        "        if EQUITY_LOG_CSV.exists():\n",
        "            try:\n",
        "                df = pd.read_csv(EQUITY_LOG_CSV, parse_dates=[\"datetime_utc\"])\n",
        "                return df.rename(columns={\"datetime_utc\": \"timestamp_utc\"})[[\"timestamp_utc\", \"equity\"]]\n",
        "            except Exception:\n",
        "                pass\n",
        "        return pd.DataFrame(columns=[\"timestamp_utc\", \"equity\"])\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        \"timestamp_utc\": pd.to_datetime(hist.timestamp, unit=\"s\", utc=True),\n",
        "        \"equity\": pd.Series(hist.equity, dtype=\"float64\")\n",
        "    }).dropna()\n",
        "\n",
        "def log_equity_snapshot(api_in=None):\n",
        "    snap = fetch_portfolio_history(period=\"1D\", timeframe=\"5Min\", api_in=api_in)\n",
        "    if snap.empty:\n",
        "        return\n",
        "    latest = snap.iloc[-1:].copy().rename(columns={\"timestamp_utc\": \"datetime_utc\"})\n",
        "\n",
        "    if EQUITY_LOG_CSV.exists():\n",
        "        df_old = pd.read_csv(EQUITY_LOG_CSV, parse_dates=[\"datetime_utc\"])\n",
        "        if not df_old.empty and pd.to_datetime(df_old[\"datetime_utc\"].iloc[-1]) == latest[\"datetime_utc\"].iloc[0]:\n",
        "            return  #nothing new; skip write/copy\n",
        "        pd.concat([df_old, latest], ignore_index=True)\\\n",
        "          .drop_duplicates(subset=[\"datetime_utc\"], keep=\"last\")\\\n",
        "          .to_csv(EQUITY_LOG_CSV, index=False)\n",
        "    else:\n",
        "        latest.to_csv(EQUITY_LOG_CSV, index=False)\n",
        "    try:\n",
        "        shutil.copy2(EQUITY_LOG_CSV, EQUITY_LOG_LATEST)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def maybe_log_equity_snapshot(api_in=None, reason: str = \"cycle\"):\n",
        "    \"\"\"Log equity when appropriate based on run mode, timing, or trade events.\"\"\"\n",
        "    global _LAST_EQUITY_LOG_TS, _TRADE_EVENT_FLAG\n",
        "    if bool(globals().get(\"DRY_RUN\", False)) and bool(globals().get(\"SKIP_EQUITY_WHEN_DRY_RUN\", True)):\n",
        "        return\n",
        "\n",
        "    now_ts = time.time()\n",
        "    force  = reason in {\"trade\", \"finalize\", \"close\"}\n",
        "    if force or (now_ts - float(_LAST_EQUITY_LOG_TS)) >= int(globals().get(\"EQUITY_LOG_THROTTLE_SEC\", 900)):\n",
        "        try:\n",
        "            log_equity_snapshot(api_in=api_in)\n",
        "            _LAST_EQUITY_LOG_TS = now_ts\n",
        "        except Exception as e:\n",
        "            logging.debug(f\"maybe_log_equity_snapshot skipped/log failed: {e}\")\n",
        "    #reset trade flag after we had a chance to log\n",
        "    if reason == \"trade\":\n",
        "        _TRADE_EVENT_FLAG = False\n",
        "\n",
        "def plot_equity_curve(from_equity_csv: bool = True):\n",
        "    with plt.ioff():\n",
        "        if from_equity_csv and EQUITY_LOG_CSV.exists():\n",
        "            df = pd.read_csv(EQUITY_LOG_CSV, parse_dates=[\"datetime_utc\"]).sort_values(\"datetime_utc\")\n",
        "        else:\n",
        "            df = fetch_portfolio_history(period=\"3M\", timeframe=\"1H\").rename(columns={\"timestamp_utc\":\"datetime_utc\"})\n",
        "        if df.empty:\n",
        "            print(\"No equity data to plot yet.\")\n",
        "            return\n",
        "        fig, ax = plt.subplots(figsize=(10, 4))\n",
        "        ax.plot(df[\"datetime_utc\"], df[\"equity\"])\n",
        "        ax.set_title(\"Portfolio Value Over Time (Paper)\")\n",
        "        ax.set_xlabel(\"Time (UTC)\")\n",
        "        ax.set_ylabel(\"Equity ($)\")\n",
        "        fig.tight_layout()\n",
        "        fig.savefig(PLOT_PATH, bbox_inches=\"tight\")\n",
        "        fig.savefig(PLOT_PATH_LATEST, bbox_inches=\"tight\")\n",
        "        plt.close(fig)\n",
        "        print(f\"Saved equity curve → {PLOT_PATH}\")\n",
        "        print(f\"Updated latest copy → {PLOT_PATH_LATEST}\")\n",
        "\n",
        "def compute_performance_metrics(df_equity: pd.DataFrame):\n",
        "    if df_equity.empty or df_equity[\"equity\"].isna().all():\n",
        "        return {\"cum_return\": np.nan, \"sharpe\": np.nan, \"max_drawdown\": np.nan}\n",
        "\n",
        "    df = df_equity.sort_values(\"datetime_utc\")\n",
        "    e = df[\"equity\"].astype(float)\n",
        "    r = e.pct_change().dropna()\n",
        "    if r.empty:\n",
        "        return {\"cum_return\": 0.0, \"sharpe\": np.nan, \"max_drawdown\": np.nan}\n",
        "\n",
        "    #estimate periods/year from median spacing\n",
        "    dt_sec = df[\"datetime_utc\"].diff().dt.total_seconds().dropna().median()\n",
        "    if not (isinstance(dt_sec, (int, float)) and dt_sec > 0):\n",
        "        periods_per_year = 252 * 78  #~5-min bars as fallback\n",
        "    else:\n",
        "        periods_per_day = (6.5 * 3600) / dt_sec\n",
        "        periods_per_year = 252 * periods_per_day\n",
        "\n",
        "    sharpe = (r.mean() / (r.std() + 1e-12)) * math.sqrt(periods_per_year)\n",
        "    cum = (1 + r).cumprod()\n",
        "    peak = cum.cummax()\n",
        "    dd = (cum / peak - 1.0).min()\n",
        "    cum_return = e.iloc[-1] / e.iloc[0] - 1.0\n",
        "\n",
        "    return {\"cum_return\": float(cum_return), \"sharpe\": float(sharpe), \"max_drawdown\": float(dd)}\n",
        "\n",
        "#-------------------------------- Per-ticker CSV logging --------------------------------------\n",
        "def _append_csv_row(path: Path, row: dict):\n",
        "    fieldnames = list(row.keys())\n",
        "    if not path.exists():\n",
        "        with path.open(\"w\", newline=\"\") as f:\n",
        "            w = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "            w.writeheader()\n",
        "            w.writerow(row)\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        with path.open(\"r\", newline=\"\") as f:\n",
        "            r = csv.reader(f)\n",
        "            old_header = next(r)\n",
        "    except Exception:\n",
        "        old_header = []\n",
        "\n",
        "    if old_header != fieldnames:\n",
        "        tmp = path.with_suffix(\".tmp\")\n",
        "        with tmp.open(\"w\", newline=\"\") as wf, path.open(\"r\", newline=\"\") as rf:\n",
        "            r = csv.DictReader(rf) if old_header else None\n",
        "            w = csv.DictWriter(wf, fieldnames=fieldnames)\n",
        "            w.writeheader()\n",
        "            if r:\n",
        "                for old_row in r:\n",
        "                    merged = {k: old_row.get(k, \"\") for k in fieldnames}\n",
        "                    w.writerow(merged)\n",
        "        tmp.replace(path)\n",
        "\n",
        "    with path.open(\"a\", newline=\"\") as f:\n",
        "        w = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "        w.writerow(row)\n",
        "\n",
        "def log_trade_symbol(symbol: str,\n",
        "                     bar_time,\n",
        "                     signal: int,\n",
        "                     raw_action: float,\n",
        "                     weight: float,\n",
        "                     confidence: float,\n",
        "                     price: float,\n",
        "                     equity: float,\n",
        "                     dry_run: bool,\n",
        "                     note: str = \"\"):\n",
        "    try:\n",
        "        if bar_time is not None and not pd.isna(bar_time):\n",
        "            ts = pd.to_datetime(bar_time, utc=True)\n",
        "            bt_iso = ts.isoformat()\n",
        "            age_sec = max(0, int((now_utc() - ts).total_seconds()))\n",
        "        else:\n",
        "            bt_iso, age_sec = \"\", \"\"\n",
        "    except Exception:\n",
        "        bt_iso, age_sec = \"\", \"\"\n",
        "\n",
        "    resolved_feed = (os.getenv(\"BARS_FEED\", \"\") or \"\").strip() or \"default\"\n",
        "\n",
        "    #Derive a simple decision label (unless 'note' is explicitly set)\n",
        "    try:\n",
        "        ew = float(weight) if np.isfinite(weight) else 0.0\n",
        "        cf = float(confidence) if np.isfinite(confidence) else 0.0\n",
        "    except Exception:\n",
        "        ew, cf = 0.0, 0.0\n",
        "\n",
        "    decision = note or (\n",
        "        \"rebalance\" if (abs(ew) >= float(globals().get(\"ENTER_WEIGHT_MIN\", 0.0))\n",
        "                        and cf >= float(globals().get(\"ENTER_CONF_MIN\", 0.0)))\n",
        "        else (\"flatten\" if abs(ew) <= float(globals().get(\"EXIT_WEIGHT_MAX\", 0.0)) else \"hold\")\n",
        "    )\n",
        "\n",
        "    row = {\n",
        "        \"log_time\": now_utc().isoformat(),\n",
        "        \"symbol\": symbol,\n",
        "        \"bar_time\": bt_iso,\n",
        "        \"bar_age_sec\": age_sec,\n",
        "        \"feed\": resolved_feed,\n",
        "        \"signal\": \"BUY\" if int(signal) == 1 else \"NEUTRAL_OR_SELL\",\n",
        "        \"raw_action\": float(raw_action) if np.isfinite(raw_action) else \"\",\n",
        "        \"weight\": float(weight) if np.isfinite(weight) else \"\",\n",
        "        \"confidence\": float(confidence) if np.isfinite(confidence) else \"\",\n",
        "        \"price\": float(price) if np.isfinite(price) else \"\",\n",
        "        \"equity\": float(equity) if np.isfinite(equity) else \"\",\n",
        "        \"dry_run\": int(bool(dry_run)),\n",
        "        \"decision\": decision,\n",
        "        \"note\": note,\n",
        "    }\n",
        "\n",
        "    _append_csv_row(RESULTS_DIR / f\"trade_log_{symbol}.csv\", row)\n",
        "\n",
        "    try:\n",
        "        ensure_trade_log_header()\n",
        "        log_trade(\n",
        "            ticker=symbol,\n",
        "            signal=1 if int(signal) == 1 else 0,\n",
        "            action=row[\"decision\"],\n",
        "            #pass None (not \"\") so log_trade doesn't choke on float(\"\")\n",
        "            price=(row[\"price\"] if row[\"price\"] != \"\" else None),\n",
        "            equity=(row[\"equity\"] if row[\"equity\"] != \"\" else None),\n",
        "            qty=None,  #or compute from position if you prefer\n",
        "            comment=row[\"note\"] or row[\"decision\"],\n",
        "        )\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "#-------------------------------- Artifacts: picker & loaders ---------------------------------\n",
        "def _extract_window_idx(path: Path) -> Optional[int]:\n",
        "    m = re.search(r\"_window(\\d+)_\", path.stem, re.IGNORECASE)\n",
        "    if not m:\n",
        "        return None\n",
        "    try:\n",
        "        return int(m.group(1))\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def pick_artifacts_for_ticker(\n",
        "    ticker: str,\n",
        "    artifacts_dir: str,\n",
        "    best_window: Optional[str] = None\n",
        ") -> Dict[str, Optional[Path]]:\n",
        "    p = Path(artifacts_dir)\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f\"Artifacts directory not found: {p.resolve()}\")\n",
        "\n",
        "    models = sorted(p.glob(f\"ppo_{ticker}_window*_model*.zip\"))\n",
        "    if not models:\n",
        "        models = sorted(p.glob(f\"ppo_{ticker}_model*.zip\")) or sorted(p.glob(f\"*{ticker}*model*.zip\"))\n",
        "    if not models:\n",
        "        raise FileNotFoundError(f\"No PPO model zip found for {ticker} in {p}\")\n",
        "\n",
        "    def _model_sort_key(path: Path):\n",
        "        w = _extract_window_idx(path)\n",
        "        return (w if w is not None else -1, \" (1)\" in path.stem)\n",
        "\n",
        "    models = sorted(models, key=_model_sort_key)\n",
        "\n",
        "    chosen: Optional[Path] = None\n",
        "    if best_window:\n",
        "        chosen = next((m for m in models if f\"_window{best_window}_\" in m.stem), None)\n",
        "        if chosen is None:\n",
        "            logging.warning(\"BEST_WINDOW=%s not found; falling back to best available.\", best_window)\n",
        "\n",
        "    if chosen is None:\n",
        "        with_idx = [(m, _extract_window_idx(m)) for m in models]\n",
        "        with_idx = [(m, w) for (m, w) in with_idx if w is not None]\n",
        "        chosen = max(with_idx, key=lambda t: t[1])[0] if with_idx else models[-1]\n",
        "\n",
        "    base = chosen.stem.replace(\"_model\", \"\")\n",
        "    base_nodup = re.sub(r\"\\s\\(\\d+\\)$\", \"\", base)\n",
        "\n",
        "    vec_candidates = list(p.glob(base + \"_vecnorm*.pkl\")) + \\\n",
        "                     list(p.glob(base_nodup + \"_vecnorm*.pkl\")) + \\\n",
        "                     list(p.glob(f\"ppo_{ticker}_*_vecnorm*.pkl\"))\n",
        "    feat_candidates = list(p.glob(base + \"_features*.json\")) + \\\n",
        "                      list(p.glob(base_nodup + \"_features*.json\")) + \\\n",
        "                      list(p.glob(f\"ppo_{ticker}_*_features*.json\"))\n",
        "\n",
        "    vecnorm = sorted(vec_candidates)[0] if vec_candidates else None\n",
        "    feats   = sorted(feat_candidates)[0] if feat_candidates else None\n",
        "\n",
        "    logging.info(f\"[{ticker}] model={chosen.name} | vecnorm={bool(vecnorm)} | features={bool(feats)}\")\n",
        "    return {\"model\": chosen, \"vecnorm\": vecnorm, \"features\": feats}\n",
        "\n",
        "def load_vecnormalize(path: Optional[Path]):\n",
        "    if path is None:\n",
        "        return None\n",
        "    try:\n",
        "        with open(path, \"rb\") as f:\n",
        "            return pickle.load(f)\n",
        "    except Exception as e:\n",
        "        #Try SB3's native VecNormalize loader, then fall back to None\n",
        "        try:\n",
        "            from stable_baselines3.common.vec_env import VecNormalize as _VN\n",
        "            return _VN.load(str(path))\n",
        "        except Exception:\n",
        "            logging.warning(\"VecNormalize load failed (%s). Proceeding without it.\", e)\n",
        "            return None\n",
        "\n",
        "def load_features(path: Optional[Path]):\n",
        "    if path is None:\n",
        "        return None\n",
        "    with open(path, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def load_ppo_model(model_path: Path):\n",
        "    return PPO.load(str(model_path))\n",
        "\n",
        "#---- Cached asset flags (tradable / fractionable / shortable) ----\n",
        "@lru_cache(maxsize=256)\n",
        "def _asset_flags(symbol: str) -> Tuple[bool, bool, bool]:\n",
        "    \"\"\"Return tradable, fractionable, and shortable flags for a symbol.\"\"\"\n",
        "    try:\n",
        "        _api = globals().get(\"api\") or init_alpaca()\n",
        "        a = _api.get_asset(symbol)\n",
        "        return (\n",
        "            bool(getattr(a, \"tradable\", True)),\n",
        "            bool(getattr(a, \"fractionable\", False)),\n",
        "            bool(getattr(a, \"shortable\", False)),\n",
        "        )\n",
        "    except Exception:\n",
        "        #conservative fallback\n",
        "        return True, False, False\n",
        "\n",
        "def _can_seed_short(api, symbol: str) -> Tuple[bool, str]:\n",
        "    \"\"\"Decide whether short seeding is allowed and why.\"\"\"\n",
        "    if not globals().get(\"ALLOW_SHORTS\", False):\n",
        "        return False, \"shorts_disabled_seed\"\n",
        "    try:\n",
        "        a = api.get_asset(symbol)\n",
        "        if not getattr(a, \"shortable\", False):\n",
        "            return False, \"not_shortable_seed\"\n",
        "        return True, \"\"\n",
        "    except Exception as e:\n",
        "        logging.info(f\"[{symbol}] get_asset shortable check failed: {e}\")\n",
        "        return False, \"shortable_check_error\"\n",
        "\n",
        "#---------------------------- Market data + account helpers -----------------------------------\n",
        "def get_recent_bars(api, symbol: str, limit: int = 200, timeframe=TimeFrame.Minute) -> pd.DataFrame:\n",
        "    def _as_df(bars):\n",
        "        if hasattr(bars, \"df\"):\n",
        "            df = bars.df.copy()\n",
        "            if not df.empty:\n",
        "                if isinstance(df.index, pd.MultiIndex):\n",
        "                    try:\n",
        "                        df = df.xs(symbol, level=0)\n",
        "                    except KeyError:\n",
        "                        df = df.reset_index(level=0, drop=True)\n",
        "                df.index = pd.to_datetime(df.index, utc=True, errors=\"coerce\")\n",
        "                df = df.rename(columns={\"open\": \"Open\", \"high\": \"High\", \"low\": \"Low\",\n",
        "                                        \"close\": \"Close\", \"volume\": \"Volume\"})\n",
        "                cols = [c for c in [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"] if c in df.columns]\n",
        "                return df[cols].sort_index()\n",
        "            return pd.DataFrame(columns=[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"])\n",
        "\n",
        "        rows = []\n",
        "        for b in bars:\n",
        "            ts = getattr(b, \"t\", None)\n",
        "            ts = pd.to_datetime(ts, utc=True) if ts is not None else pd.NaT\n",
        "            rows.append({\n",
        "                \"timestamp\": ts,\n",
        "                \"Open\":   float(getattr(b, \"o\", getattr(b, \"open\",  np.nan))),\n",
        "                \"High\":   float(getattr(b, \"h\", getattr(b, \"high\",  np.nan))),\n",
        "                \"Low\":    float(getattr(b, \"l\", getattr(b, \"low\",   np.nan))),\n",
        "                \"Close\":  float(getattr(b, \"c\", getattr(b, \"close\", np.nan))),\n",
        "                \"Volume\": float(getattr(b, \"v\", getattr(b, \"volume\",np.nan))),\n",
        "            })\n",
        "        df = pd.DataFrame(rows)\n",
        "        if df.empty:\n",
        "            return pd.DataFrame(columns=[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"])\n",
        "        return df.set_index(pd.to_datetime(df[\"timestamp\"], utc=True)).drop(columns=[\"timestamp\"]).sort_index()\n",
        "\n",
        "    feed = os.getenv(\"BARS_FEED\", \"\").strip()\n",
        "    try:\n",
        "        logging.info(f\"[{symbol}] fetching {limit} {timeframe} bars (feed='{feed or 'default'}')\")\n",
        "        bars = api.get_bars(symbol, timeframe, limit=limit, feed=feed) if feed else api.get_bars(symbol, timeframe, limit=limit)\n",
        "        df = _as_df(bars)\n",
        "        if not df.empty:\n",
        "            return df\n",
        "        if feed:\n",
        "            logging.info(f\"[{symbol}] explicit feed empty; retrying with default feed\")\n",
        "            df2 = _as_df(api.get_bars(symbol, timeframe, limit=limit))\n",
        "            if not df2.empty:\n",
        "                return df2\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[{symbol}] get_bars(limit) failed: {e}\")\n",
        "\n",
        "    try:\n",
        "        end_dt = datetime.now(timezone.utc).replace(microsecond=0)\n",
        "        start_dt = end_dt - timedelta(days=5)\n",
        "        end = end_dt.isoformat().replace(\"+00:00\", \"Z\")\n",
        "        start = start_dt.isoformat().replace(\"+00:00\", \"Z\")\n",
        "        logging.info(f\"[{symbol}] retry window start={start} end={end} (feed='{feed or 'default'}')\")\n",
        "        bars = api.get_bars(symbol, timeframe, start=start, end=end, feed=feed) if feed else api.get_bars(symbol, timeframe, start=start, end=end)\n",
        "        return _as_df(bars)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[{symbol}] get_bars(start/end) failed: {e}\")\n",
        "        return pd.DataFrame(columns=[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"])\n",
        "\n",
        "def get_account_equity(api) -> float:\n",
        "    return float(api.get_account().equity)\n",
        "\n",
        "def get_position(api, symbol: str):\n",
        "    try:\n",
        "        return api.get_position(symbol)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def get_position_qty(api, symbol: str):\n",
        "    try:\n",
        "        pos = api.get_position(symbol)\n",
        "    except Exception:\n",
        "        pos = None\n",
        "    if not pos:\n",
        "        return 0.0 if USE_FRACTIONALS else 0\n",
        "    try:\n",
        "        q = float(pos.qty)\n",
        "        return q if USE_FRACTIONALS else int(round(q))\n",
        "    except Exception:\n",
        "        return 0.0 if USE_FRACTIONALS else 0\n",
        "\n",
        "def get_last_price(api, symbol: str) -> float:\n",
        "    try:\n",
        "        tr = api.get_latest_trade(symbol)\n",
        "        price = getattr(tr, \"price\", None)\n",
        "        if price is None:\n",
        "            price = getattr(tr, \"p\", None)\n",
        "        if price is not None and np.isfinite(price):\n",
        "            return float(price)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        feed = os.getenv(\"BARS_FEED\", \"\").strip() or None\n",
        "        bars = api.get_bars(symbol, TimeFrame.Minute, limit=1, feed=feed) if feed else api.get_bars(symbol, TimeFrame.Minute, limit=1)\n",
        "        if hasattr(bars, \"df\"):\n",
        "            df = bars.df.copy()\n",
        "            if isinstance(df.index, pd.MultiIndex):\n",
        "                try:\n",
        "                    df = df.xs(symbol, level=0)\n",
        "                except Exception:\n",
        "                    df = df.reset_index(level=0, drop=True)\n",
        "            if not df.empty:\n",
        "                if \"close\" in df.columns: return float(df[\"close\"].iloc[-1])\n",
        "                if \"Close\" in df.columns: return float(df[\"Close\"].iloc[-1])\n",
        "        elif bars:\n",
        "            b = bars[0]\n",
        "            close = getattr(b, \"c\", getattr(b, \"close\", None))\n",
        "            if close is not None:\n",
        "                return float(close)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[{symbol}] get_last_price via bars failed: {e}\")\n",
        "\n",
        "    try:\n",
        "        qt = api.get_latest_quote(symbol)\n",
        "        ap = getattr(qt, \"ap\", None) or getattr(qt, \"ask_price\", None)\n",
        "        bp = getattr(qt, \"bp\", None) or getattr(qt, \"bid_price\", None)\n",
        "        if ap and bp:\n",
        "            return float((float(ap) + float(bp)) / 2.0)\n",
        "        if ap: return float(ap)\n",
        "        if bp: return float(bp)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        pos = api.get_position(symbol)\n",
        "        return float(pos.avg_entry_price)\n",
        "    except Exception:\n",
        "        return float(\"nan\")\n",
        "\n",
        "def cancel_open_symbol_orders(api, symbol: str):\n",
        "    try:\n",
        "        for o in api.list_orders(status=\"open\"):\n",
        "            if o.symbol == symbol:\n",
        "                api.cancel_order(o.id)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[{symbol}] cancel orders failed: {e}\")\n",
        "\n",
        "def to_2dp_str(x) -> str:\n",
        "    return format(Decimal(str(x)).quantize(Decimal(\"0.01\"), rounding=ROUND_HALF_UP), \"f\")\n",
        "\n",
        "def to_6dp_str(x) -> str:\n",
        "    return format(Decimal(str(x)).quantize(Decimal(\"0.000001\"), rounding=ROUND_DOWN), \"f\")\n",
        "\n",
        "def market_order(api, symbol: str, side: str, qty=None, notional: float=None):\n",
        "    if qty is not None and notional is not None:\n",
        "        logging.warning(f\"[{symbol}] Both qty and notional provided; preferring notional and ignoring qty.\")\n",
        "        qty = None\n",
        "\n",
        "    if qty is None and notional is None:\n",
        "        logging.warning(f\"[{symbol}] No order size provided; skipping.\")\n",
        "        return None\n",
        "    if qty is not None:\n",
        "        try:\n",
        "            if float(qty) <= 0:\n",
        "                logging.warning(f\"[{symbol}] Non-positive qty ({qty}); skipping.\")\n",
        "                return None\n",
        "        except Exception:\n",
        "            pass\n",
        "    if notional is not None and notional <= 0:\n",
        "        logging.warning(f\"[{symbol}] Non-positive notional (${notional}); skipping.\")\n",
        "        return None\n",
        "\n",
        "    #ignore dust-sized or non-finite orders\n",
        "    try:\n",
        "        if (notional is not None and (not np.isfinite(float(notional)) or float(notional) < 0.01)) or \\\n",
        "           (qty is not None and (not np.isfinite(float(qty)) or float(qty) == 0.0)):\n",
        "            logging.info(f\"[{symbol}] Order size ~0; skipping.\")\n",
        "            return None\n",
        "    except Exception:\n",
        "        logging.info(f\"[{symbol}] Order size parse issue; skipping.\")\n",
        "        return None\n",
        "\n",
        "    if DRY_RUN:\n",
        "        #Safe stringification for display\n",
        "        notional_str = to_2dp_str(notional) if notional is not None else None\n",
        "        logging.info(\n",
        "            f\"[DRY_RUN] Would submit {side} \"\n",
        "            f\"{('notional=$' + str(notional_str)) if notional_str is not None else ('qty=' + str(qty))} \"\n",
        "            f\"{symbol} (market, day)\"\n",
        "        )\n",
        "        globals()[\"_TRADE_EVENT_FLAG\"] = True  #ensures equity snapshot logs this cycle\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        qty_arg = None\n",
        "        if qty is not None:\n",
        "            qty_arg = to_6dp_str(float(qty)) if USE_FRACTIONALS else int(qty)\n",
        "        notional_arg = to_2dp_str(float(notional)) if notional is not None else None\n",
        "\n",
        "        o = api.submit_order(\n",
        "            symbol=symbol,\n",
        "            side=side,\n",
        "            type=\"market\",\n",
        "            time_in_force=\"day\",\n",
        "            qty=qty_arg,\n",
        "            notional=notional_arg,\n",
        "        )\n",
        "\n",
        "        logging.info(\n",
        "            f\"[{symbol}] Submitted {side} \"\n",
        "            f\"{('notional=$' + str(notional_arg)) if notional_arg is not None else ('qty=' + str(qty_arg))}\"\n",
        "        )\n",
        "        globals()[\"_TRADE_EVENT_FLAG\"] = True\n",
        "        return o\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"[{symbol}] submit_order failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def market_order_to_qty(api, symbol: str, side: str, qty: Union[int, float, str]):\n",
        "    \"\"\"Submit a market order for a quantity, handling fractional shares.\"\"\"\n",
        "    if USE_FRACTIONALS:\n",
        "        qf = float(qty)\n",
        "        q = int(round(qf)) if abs(qf - round(qf)) < 1e-8 else to_6dp_str(qf)\n",
        "    else:\n",
        "        q = int(qty)\n",
        "    return market_order(api, symbol, side=side, qty=q)\n",
        "\n",
        "#----------------------------- Sizing / risk + (un)flatten / rebalance ------------------------\n",
        "def action_to_weight(action) -> Tuple[float, float, float]:\n",
        "    \"\"\"Map a model action to target weight and confidence.\"\"\"\n",
        "    a = float(np.array(action).squeeze())\n",
        "    conf = float(abs(np.tanh(a)))\n",
        "    if a == 0:\n",
        "        return 0.0, conf, a\n",
        "    if a < 0:\n",
        "        if not globals().get(\"ALLOW_SHORTS\", False):\n",
        "            return 0.0, conf, a\n",
        "        w = -WEIGHT_CAP * conf if SIZING_MODE == \"linear\" else (\n",
        "            0.0 if conf < CONF_FLOOR else -WEIGHT_CAP * (conf - CONF_FLOOR) / (1.0 - CONF_FLOOR)\n",
        "        )\n",
        "        w = max(-WEIGHT_CAP, min(0.0, float(w)))\n",
        "        return w, conf, a\n",
        "    #a > 0 (long)\n",
        "    if SIZING_MODE == \"linear\":\n",
        "        w = WEIGHT_CAP * conf\n",
        "    else:\n",
        "        w = 0.0 if conf < CONF_FLOOR else WEIGHT_CAP * (conf - CONF_FLOOR) / (1.0 - CONF_FLOOR)\n",
        "    w = max(0.0, min(WEIGHT_CAP, float(w)))\n",
        "    return w, conf, a\n",
        "\n",
        "def compute_target_qty_by_cash(equity: float, price: float, target_weight: float, api=None) -> int:\n",
        "    if not np.isfinite(price) or price <= 0:\n",
        "        return 0\n",
        "\n",
        "    #Determine budget (buying power if available, else equity fallback)\n",
        "    if api:\n",
        "        try:\n",
        "            acct = api.get_account()\n",
        "            budget = float(getattr(acct, \"buying_power\", getattr(acct, \"cash\", equity)))\n",
        "        except Exception:\n",
        "            budget = equity\n",
        "    else:\n",
        "        budget = equity\n",
        "\n",
        "    target_notional = equity * float(target_weight)           #desired exposure (can be negative)\n",
        "    allowed = min(budget, abs(target_notional))               #cap by budget\n",
        "    qty = int(allowed // price)                               #whole shares only (used in non-fractional path)\n",
        "\n",
        "    #Return signed qty (negative only if shorts are allowed)\n",
        "    if target_weight > 0:\n",
        "        return max(0, qty)\n",
        "    else:\n",
        "        return min(0, -qty) if globals().get(\"ALLOW_SHORTS\", False) else 0\n",
        "\n",
        "def _close_symbol_position(\n",
        "    api,\n",
        "    symbol: str,\n",
        "    qty_hint: Optional[float] = None,\n",
        "    cancel_orders: bool = True,\n",
        "):\n",
        "    qty = qty_hint if qty_hint is not None else get_position_qty(api, symbol)\n",
        "\n",
        "    if (USE_FRACTIONALS and abs(qty) < 1e-8) or (not USE_FRACTIONALS and int(qty) == 0):\n",
        "        return\n",
        "\n",
        "    if cancel_orders:\n",
        "        cancel_open_symbol_orders(api, symbol)\n",
        "\n",
        "    if DRY_RUN:\n",
        "        logging.info(f\"[DRY_RUN] Would close position {symbol} (qty={qty})\")\n",
        "        globals()[\"_TRADE_EVENT_FLAG\"] = True\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        globals()[\"_TRADE_EVENT_FLAG\"] = True\n",
        "        api.close_position(symbol)\n",
        "        logging.info(f\"[{symbol}] close_position submitted\")\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[{symbol}] close_position failed ({e}); falling back to market order\")\n",
        "        side = \"sell\" if qty > 0 else \"buy\"\n",
        "        market_order_to_qty(api, symbol, side=side, qty=abs(qty))\n",
        "\n",
        "\n",
        "def flatten_symbol(api, symbol: str):\n",
        "    #per-symbol flatten: cancel that symbol’s open orders, then close\n",
        "    _close_symbol_position(api, symbol, qty_hint=None, cancel_orders=True)\n",
        "\n",
        "\n",
        "def flatten_all_positions(api) -> None:\n",
        "    try:\n",
        "        positions = api.list_positions()\n",
        "    except Exception as e:\n",
        "        logging.warning(\"Flatten-all failed (list_positions): %s\", e)\n",
        "        return\n",
        "\n",
        "    #In DRY_RUN, avoid account-mutating calls (cancel/close). We still log intent.\n",
        "    if not DRY_RUN:\n",
        "        #Cancel all open orders once (faster)\n",
        "        try:\n",
        "            open_orders = api.list_orders(status=\"open\")\n",
        "            pos_syms = {getattr(p, \"symbol\", None) for p in positions}\n",
        "            for o in open_orders:\n",
        "                if getattr(o, \"symbol\", None) in pos_syms:\n",
        "                    try:\n",
        "                        api.cancel_order(o.id)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "        except Exception as e:\n",
        "            logging.warning(\"Flatten-all cancel pass failed: %s\", e)\n",
        "\n",
        "    #Now close each position (this is already DRY_RUN-safe inside _close_symbol_position)\n",
        "    for p in positions:\n",
        "        sym = getattr(p, \"symbol\", None)\n",
        "        if not sym:\n",
        "            continue\n",
        "        try:\n",
        "            qty = float(getattr(p, \"qty\", 0.0) or 0.0)\n",
        "        except Exception:\n",
        "            qty = None\n",
        "\n",
        "        _close_symbol_position(api, sym, qty_hint=qty, cancel_orders=False)\n",
        "\n",
        "def rebalance_to_weight(api, symbol: str, equity: float, target_weight: float):\n",
        "    \"\"\"Adjust the position toward a target weight, applying safety guards.\"\"\"\n",
        "    price = get_last_price(api, symbol)\n",
        "    if not np.isfinite(price) or price <= 0:\n",
        "        logging.warning(f\"[{symbol}] Price unavailable; skipping rebalance this cycle.\")\n",
        "        return\n",
        "\n",
        "    tradable, fractionable, shortable = _asset_flags(symbol)\n",
        "    if not tradable:\n",
        "        logging.info(f\"[{symbol}] Not tradable; skipping rebalance.\")\n",
        "        return\n",
        "    use_fractionals = bool(USE_FRACTIONALS and fractionable)\n",
        "\n",
        "    have_qty        = get_position_qty(api, symbol)          #signed (negative if short)\n",
        "    have_notional   = have_qty * price                       #current exposure\n",
        "    target_notional = equity * float(target_weight)          #desired exposure\n",
        "    delta_notional  = target_notional - have_notional        #change in exposure\n",
        "\n",
        "    if abs(delta_notional) < 1e-9:\n",
        "        return\n",
        "\n",
        "    #Compute delta_weight safely and log gates\n",
        "    delta_weight = abs(delta_notional) / max(float(equity), 1e-9)\n",
        "    logging.debug(\n",
        "        f\"[{symbol}] have_notional={have_notional:.2f} \"\n",
        "        f\"target_notional={target_notional:.2f} delta_notional={delta_notional:.2f} \"\n",
        "        f\"delta_weight={delta_weight:.4f} gates: \"\n",
        "        f\"Δw_min={DELTA_WEIGHT_MIN} notional_min={REBALANCE_MIN_NOTIONAL}\"\n",
        "    )\n",
        "    if delta_weight < float(globals().get(\"DELTA_WEIGHT_MIN\", 0.0)):\n",
        "        return\n",
        "\n",
        "    if use_fractionals:\n",
        "        dn = round_to_cents(abs(delta_notional))\n",
        "        if dn < float(globals().get(\"REBALANCE_MIN_NOTIONAL\", 0.0)):\n",
        "            return\n",
        "\n",
        "        side = \"buy\" if delta_notional > 0 else \"sell\"\n",
        "        shorting = (target_notional < 0) and (side == \"sell\")  #increasing a short\n",
        "        covering = (have_qty < 0) and (side == \"buy\")         #reducing a short\n",
        "\n",
        "        if shorting:\n",
        "            if not shortable:\n",
        "                logging.info(f\"[{symbol}] Not shortable; skipping rebalance toward short.\")\n",
        "                return\n",
        "            qty = max(1, int(math.floor(dn / price))) if np.isfinite(price) and price > 0 else 1\n",
        "            market_order_to_qty(api, symbol, side=\"sell\", qty=qty)\n",
        "            return\n",
        "\n",
        "        if covering:\n",
        "            #Covering shorts: buy whole shares (avoid fractional buy vs integer short)\n",
        "            qty = max(1, int(math.ceil(dn / price))) if np.isfinite(price) and price > 0 else 1\n",
        "            qty = min(int(abs(have_qty)), qty) if have_qty < 0 else qty\n",
        "            market_order_to_qty(api, symbol, side=\"buy\", qty=qty)\n",
        "            return\n",
        "\n",
        "        #Long exposure changes can safely use notional\n",
        "        market_order(api, symbol, side=side, notional=dn)\n",
        "        return\n",
        "\n",
        "    #---- Non-fractional mode (whole shares only) ----\n",
        "    want_qty  = compute_target_qty_by_cash(equity, price, target_weight, api)\n",
        "    delta_qty = want_qty - have_qty\n",
        "    if delta_qty == 0:\n",
        "        return\n",
        "\n",
        "    approx_delta_notional = abs(delta_qty) * price\n",
        "    if equity > 0 and approx_delta_notional / equity < float(globals().get(\"DELTA_WEIGHT_MIN\", 0.0)):\n",
        "        return\n",
        "    if approx_delta_notional < float(globals().get(\"REBALANCE_MIN_NOTIONAL\", 0.0)):\n",
        "        return\n",
        "\n",
        "    side = \"buy\" if delta_qty > 0 else \"sell\"\n",
        "    shorting = (target_notional < 0) and (side == \"sell\")\n",
        "    if shorting and not shortable:\n",
        "        logging.info(f\"[{symbol}] Not shortable; skipping rebalance toward short.\")\n",
        "        return\n",
        "\n",
        "    market_order_to_qty(api, symbol, side=side, qty=int(abs(delta_qty)))\n",
        "\n",
        "def check_tp_sl_and_maybe_flatten(api, symbol: str) -> bool:\n",
        "    if TAKE_PROFIT_PCT <= 0 and STOP_LOSS_PCT <= 0:\n",
        "        return False\n",
        "    pos = get_position(api, symbol)\n",
        "    if not pos:\n",
        "        return False\n",
        "    try:\n",
        "        plpc = float(pos.unrealized_plpc)\n",
        "    except Exception:\n",
        "        return False\n",
        "    if TAKE_PROFIT_PCT > 0 and plpc >= TAKE_PROFIT_PCT:\n",
        "        logging.info(f\"[{symbol}] TP hit ({plpc:.4f} >= {TAKE_PROFIT_PCT:.4f}). Flattening.\")\n",
        "        flatten_symbol(api, symbol)\n",
        "        return True\n",
        "    if STOP_LOSS_PCT > 0 and plpc <= -abs(STOP_LOSS_PCT):\n",
        "        logging.info(f\"[{symbol}] SL hit ({plpc:.4f} <= {-abs(STOP_LOSS_PCT):.4f}). Flattening.\")\n",
        "        flatten_symbol(api, symbol)\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "#----------------------------- Inference / obs building ---------------------------------------\n",
        "def expected_obs_shape(model, vecnorm) -> Optional[tuple]:\n",
        "    for src in (vecnorm, model):\n",
        "        try:\n",
        "            shp = tuple(src.observation_space.shape)\n",
        "            if shp:\n",
        "                return shp\n",
        "        except Exception:\n",
        "            pass\n",
        "    return None\n",
        "\n",
        "def compute_art_feat_order(features_hint: Any, df: pd.DataFrame) -> List[str]:\n",
        "    if features_hint is None:\n",
        "        return [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
        "    feats = features_hint.get(\"features\", features_hint) if isinstance(features_hint, dict) else list(features_hint)\n",
        "    drop = {\"datetime\", \"symbol\", \"target\", \"return\"}\n",
        "    return [c for c in feats if c not in drop and (c in df.columns) and pd.api.types.is_numeric_dtype(df[c])]\n",
        "\n",
        "def build_obs_from_row(row: pd.Series, order: List[str]) -> np.ndarray:\n",
        "    vals = []\n",
        "    for c in order:\n",
        "        v = row.get(c, np.nan)\n",
        "        vals.append(0.0 if (pd.isna(v) or v is None or v is False) else float(v))\n",
        "    return np.array(vals, dtype=np.float32)\n",
        "\n",
        "def _pick_columns_for_channels(features_hint: Any, df: pd.DataFrame, channels: int) -> List[str]:\n",
        "    numeric = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
        "    cols: List[str] = []\n",
        "    if isinstance(features_hint, dict) and \"features\" in features_hint:\n",
        "        cand = [c for c in features_hint[\"features\"] if c in df.columns and pd.api.types.is_numeric_dtype(df[c])]\n",
        "        if len(cand) >= channels:\n",
        "            cols = cand[:channels]\n",
        "    if not cols:\n",
        "        pref = [\"Close\", \"Volume\", \"Adj Close\", \"Open\", \"High\", \"Low\"]\n",
        "        cols = [c for c in pref if c in numeric]\n",
        "        cols += [c for c in numeric if c not in cols]\n",
        "        cols = cols[:channels]\n",
        "    if len(cols) < channels and cols:\n",
        "        while len(cols) < channels:\n",
        "            cols.append(cols[-1])\n",
        "    return cols[:channels]\n",
        "\n",
        "def add_regime(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df[\"Vol20\"] = df[\"Close\"].pct_change().rolling(20).std()\n",
        "    df[\"Ret20\"] = df[\"Close\"].pct_change(20)\n",
        "    vol_hi   = (df[\"Vol20\"] > df[\"Vol20\"].median()).astype(int)\n",
        "    trend_hi = (df[\"Ret20\"].abs() > df[\"Ret20\"].abs().median()).astype(int)\n",
        "    df[\"Regime4\"] = vol_hi * 2 + trend_hi\n",
        "    return df\n",
        "\n",
        "def denoise_wavelet(series: pd.Series, wavelet: str = \"db1\", level: int = 2) -> pd.Series:\n",
        "    try:\n",
        "        import pywt\n",
        "    except Exception:\n",
        "        return pd.Series(series).astype(float).ffill().bfill().ewm(span=5, adjust=False).mean()\n",
        "    s = pd.Series(series).astype(float).ffill().bfill()\n",
        "    arr = s.to_numpy()\n",
        "    try:\n",
        "        w = pywt.Wavelet(wavelet)\n",
        "        maxlvl = pywt.dwt_max_level(len(arr), w.dec_len)\n",
        "        lvl = int(max(0, min(level, maxlvl)))\n",
        "        if lvl < 1:\n",
        "            return s\n",
        "        coeffs = pywt.wavedec(arr, w, mode=\"symmetric\", level=lvl)\n",
        "        for i in range(1, len(coeffs)):\n",
        "            coeffs[i] = np.zeros_like(coeffs[i])\n",
        "        rec = pywt.waverec(coeffs, w, mode=\"symmetric\")\n",
        "        return pd.Series(rec[:len(arr)], index=s.index)\n",
        "    except Exception:\n",
        "        return s.ewm(span=5, adjust=False).mean()\n",
        "\n",
        "def add_features_live(\n",
        "    df: pd.DataFrame,\n",
        "    use_sentiment: bool = False,\n",
        "    rsi_wilder: bool = True,\n",
        "    atr_wilder: bool = True,\n",
        ") -> pd.DataFrame:\n",
        "    df = df.copy().sort_index()\n",
        "    cols_ci = {c.lower(): c for c in df.columns}\n",
        "    rename = {}\n",
        "    for final, alts in {\n",
        "        \"Open\": [\"open\"], \"High\": [\"high\"], \"Low\": [\"low\"],\n",
        "        \"Close\": [\"close\",\"close*\",\"last\"], \"Adj Close\":[\"adj close\",\"adj_close\",\"adjclose\",\"adjusted close\"],\n",
        "        \"Volume\":[\"volume\",\"vol\"]\n",
        "    }.items():\n",
        "        for a in [final.lower()] + alts:\n",
        "            if a in cols_ci:\n",
        "                rename[cols_ci[a]] = final\n",
        "                break\n",
        "    df = df.rename(columns=rename)\n",
        "    if \"Adj Close\" not in df.columns and \"Close\" in df.columns:\n",
        "        df[\"Adj Close\"] = df[\"Close\"]\n",
        "\n",
        "    #--- Classic techs ---\n",
        "    df[\"SMA_20\"] = df[\"Close\"].rolling(20).mean()\n",
        "    df[\"STD_20\"] = df[\"Close\"].rolling(20).std()\n",
        "    df[\"Upper_Band\"] = df[\"SMA_20\"] + 2 * df[\"STD_20\"]\n",
        "    df[\"Lower_Band\"] = df[\"SMA_20\"] - 2 * df[\"STD_20\"]\n",
        "\n",
        "    df[\"Lowest_Low\"]   = df[\"Low\"].rolling(14).min()\n",
        "    df[\"Highest_High\"] = df[\"High\"].rolling(14).max()\n",
        "    denom = (df[\"Highest_High\"] - df[\"Lowest_Low\"]).replace(0, np.nan)\n",
        "    df[\"Stoch\"] = ((df[\"Close\"] - df[\"Lowest_Low\"]) / denom) * 100\n",
        "\n",
        "    df[\"ROC\"] = df[\"Close\"].pct_change(10)\n",
        "    sign = np.sign(df[\"Close\"].diff().fillna(0))\n",
        "    df[\"OBV\"] = (sign * df[\"Volume\"].fillna(0)).cumsum()\n",
        "\n",
        "    tp = (df[\"High\"] + df[\"Low\"] + df[\"Close\"]) / 3.0\n",
        "    sma_tp = tp.rolling(20).mean()\n",
        "    md = (tp - sma_tp).abs().rolling(20).mean().replace(0, np.nan)\n",
        "    df[\"CCI\"] = (tp - sma_tp) / (0.015 * md)\n",
        "\n",
        "    df[\"EMA_10\"] = df[\"Close\"].ewm(span=10, adjust=False).mean()\n",
        "    df[\"EMA_50\"] = df[\"Close\"].ewm(span=50, adjust=False).mean()\n",
        "    ema12 = df[\"Close\"].ewm(span=12, adjust=False).mean()\n",
        "    ema26 = df[\"Close\"].ewm(span=26, adjust=False).mean()\n",
        "    df[\"MACD_Line\"]   = ema12 - ema26\n",
        "    df[\"MACD_Signal\"] = df[\"MACD_Line\"].ewm(span=9, adjust=False).mean()\n",
        "\n",
        "    d = df[\"Close\"].diff()\n",
        "    gain = d.clip(lower=0)\n",
        "    loss = (-d.clip(upper=0))\n",
        "    if rsi_wilder:\n",
        "        avg_gain = gain.ewm(alpha=1/14, adjust=False).mean()\n",
        "        avg_loss = loss.ewm(alpha=1/14, adjust=False).mean()\n",
        "    else:\n",
        "        avg_gain = gain.rolling(14).mean()\n",
        "        avg_loss = loss.rolling(14).mean()\n",
        "    rs = avg_gain / avg_loss.replace(0, np.nan)\n",
        "    df[\"RSI\"] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    tr = pd.concat([\n",
        "        (df[\"High\"] - df[\"Low\"]),\n",
        "        (df[\"High\"] - df[\"Close\"].shift()).abs(),\n",
        "        (df[\"Low\"]  - df[\"Close\"].shift()).abs(),\n",
        "    ], axis=1).max(axis=1)\n",
        "    df[\"ATR\"] = tr.ewm(alpha=1/14, adjust=False).mean() if atr_wilder else tr.rolling(14).mean()\n",
        "\n",
        "    df[\"Volatility\"]     = df[\"Close\"].pct_change().rolling(20).std()\n",
        "    df[\"Denoised_Close\"] = denoise_wavelet(df[\"Close\"])\n",
        "\n",
        "    df = add_regime(df)\n",
        "    df[\"SentimentScore\"] = (df.get(\"SentimentScore\", 0.0) if use_sentiment else 0.0)\n",
        "    df[\"Delta\"] = df[\"Close\"].pct_change(1).fillna(0.0)\n",
        "    df[\"Gamma\"] = df[\"Delta\"].diff().fillna(0.0)\n",
        "\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    return df\n",
        "\n",
        "def prepare_observation_from_bars(\n",
        "    bars_df: pd.DataFrame,\n",
        "    features_hint: Any = None,\n",
        "    min_required_rows: int = 60,\n",
        "    expected_shape: Optional[tuple] = None,\n",
        ") -> Tuple[np.ndarray, int]:\n",
        "    feats_df = add_features_live(bars_df).replace([np.inf, -np.inf], np.nan)\n",
        "    ts = pd.Timestamp.utcnow()\n",
        "    try:\n",
        "        idx_ts = pd.Timestamp(feats_df.index[-1])\n",
        "        ts = idx_ts.tz_convert(\"UTC\") if idx_ts.tzinfo else idx_ts.tz_localize(\"UTC\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    if expected_shape is not None:\n",
        "        if len(expected_shape) == 2:\n",
        "            lookback, channels = int(expected_shape[0]), int(expected_shape[1])\n",
        "            cols = _pick_columns_for_channels(features_hint, feats_df, channels)\n",
        "            window_df = feats_df[cols].tail(lookback).fillna(0.0)\n",
        "            arr = window_df.to_numpy(dtype=np.float32)\n",
        "            if arr.shape[0] < lookback:\n",
        "                pad_rows = lookback - arr.shape[0]\n",
        "                arr = np.vstack([np.zeros((pad_rows, channels), dtype=np.float32), arr])\n",
        "            arr = arr[-lookback:, :channels]\n",
        "            return arr.reshape(lookback, channels), int(ts.timestamp())\n",
        "\n",
        "        elif len(expected_shape) == 1:\n",
        "            n = int(expected_shape[0])\n",
        "            cand = compute_art_feat_order(features_hint, feats_df)\n",
        "            if len(feats_df) < max(20, min_required_rows):\n",
        "                raise ValueError(f\"Not enough bars to compute features robustly (have {len(feats_df)}).\")\n",
        "            last = feats_df.iloc[-1]\n",
        "            vals = []\n",
        "            for c in cand[:n]:\n",
        "                v = last.get(c, np.nan)\n",
        "                vals.append(0.0 if (pd.isna(v) or v is None) else float(v))\n",
        "            if len(vals) < n:\n",
        "                vals += [0.0] * (n - len(vals))\n",
        "            return np.asarray(vals, dtype=np.float32), int(ts.timestamp())\n",
        "\n",
        "    order = compute_art_feat_order(features_hint, feats_df)\n",
        "    if not order:\n",
        "        raise ValueError(\"No usable features after resolving artifact order.\")\n",
        "    feats_df = feats_df.dropna(subset=order)\n",
        "    if len(feats_df) < max(20, min_required_rows):\n",
        "        raise ValueError(f\"Not enough bars to compute features robustly (have {len(feats_df)}).\")\n",
        "    last = feats_df.iloc[-1]\n",
        "    obs = build_obs_from_row(last, order)\n",
        "    return obs.astype(np.float32), int(ts.timestamp())\n",
        "\n",
        "#-------------------------------- Live loop helpers -------------------------------------------\n",
        "def ensure_market_open(api) -> bool:\n",
        "    try:\n",
        "        return bool(api.get_clock().is_open)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def _sleep_until_open(api):\n",
        "    try:\n",
        "        clock = api.get_clock()\n",
        "        if getattr(clock, \"is_open\", False):\n",
        "            return\n",
        "        nxt = pd.to_datetime(getattr(clock, \"next_open\"), utc=True, errors=\"coerce\")\n",
        "        if pd.isna(nxt):\n",
        "            time.sleep(60)\n",
        "            return\n",
        "        wait = max(1, int((nxt - now_utc()).total_seconds()))\n",
        "        logging.info(\"Market closed. Sleeping %ds until next open.\", wait)\n",
        "        time.sleep(wait)\n",
        "    except Exception:\n",
        "        time.sleep(60)\n",
        "\n",
        "def write_account_info_to_run_config(api) -> None:\n",
        "    \"\"\"Add Alpaca account fields to the run config.\"\"\"\n",
        "    try:\n",
        "        acct = api.get_account()\n",
        "        acct_info = {\n",
        "            \"account_id\": getattr(acct, \"id\", \"\"),\n",
        "            \"status\": getattr(acct, \"status\", \"\"),\n",
        "            \"equity\": getattr(acct, \"equity\", \"\"),\n",
        "            \"cash\": getattr(acct, \"cash\", \"\"),\n",
        "            \"pattern_day_trader\": getattr(acct, \"pattern_day_trader\", \"\"),\n",
        "        }\n",
        "\n",
        "        cfg_path = RESULTS_DIR / \"run_config.json\"\n",
        "        try:\n",
        "            meta = json.loads(cfg_path.read_text()) if cfg_path.exists() else {}\n",
        "        except Exception:\n",
        "            meta = {}\n",
        "\n",
        "        meta[\"alpaca_account\"] = acct_info\n",
        "\n",
        "        #atomic-ish write\n",
        "        tmp = cfg_path.with_suffix(\".tmp\")\n",
        "        tmp.write_text(json.dumps(meta, indent=2))\n",
        "        tmp.replace(cfg_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.warning(\"Could not augment run_config.json with account info: %s\", e)\n",
        "\n",
        "\n",
        "def infer_target_weight(model: PPO, vecnorm: Optional[VecNormalize], obs: np.ndarray) -> Tuple[float, float, float]:\n",
        "    x = obs\n",
        "    if vecnorm is not None and hasattr(vecnorm, \"normalize_obs\"):\n",
        "        try:\n",
        "            x = vecnorm.normalize_obs(x)\n",
        "        except Exception:\n",
        "            try:\n",
        "                x_b = np.expand_dims(obs, axis=0)\n",
        "                x = vecnorm.normalize_obs(x_b)[0]\n",
        "            except Exception:\n",
        "                x = obs\n",
        "\n",
        "    x = np.asarray(x, dtype=np.float32)\n",
        "    try:\n",
        "        action, _ = model.predict(x, deterministic=INF_DETERMINISTIC)\n",
        "    except Exception:\n",
        "        action, _ = model.predict(np.expand_dims(x, axis=0), deterministic=INF_DETERMINISTIC)\n",
        "\n",
        "    return action_to_weight(action)\n",
        "\n",
        "    try:\n",
        "        action, _ = model.predict(x, deterministic=INF_DETERMINISTIC)\n",
        "    except Exception:\n",
        "        x_b = np.expand_dims(x, axis=0)\n",
        "        action, _ = model.predict(x_b, deterministic=INF_DETERMINISTIC)\n",
        "        #peel off batch dim if present\n",
        "        if isinstance(action, (list, np.ndarray)):\n",
        "            action = np.array(action)\n",
        "            if action.ndim > 0:\n",
        "                action = action[0]\n",
        "\n",
        "    return action_to_weight(action)\n",
        "\n",
        "def maybe_patch_stale_with_latest_trade(api, symbol: str, bars_df: pd.DataFrame, max_age_sec: int = None) -> pd.DataFrame:\n",
        "    if bars_df.empty:\n",
        "        return bars_df\n",
        "    max_age_sec = max_age_sec or int(globals().get(\"STALE_MAX_SEC\", 600))\n",
        "    try:\n",
        "        last_ts = pd.Timestamp(bars_df.index[-1])\n",
        "        last_ts = last_ts.tz_convert(\"UTC\") if last_ts.tzinfo else last_ts.tz_localize(\"UTC\")\n",
        "        age_sec = int((now_utc() - last_ts).total_seconds())\n",
        "        if age_sec <= max_age_sec:\n",
        "            return bars_df\n",
        "\n",
        "        lt = api.get_latest_trade(symbol)\n",
        "        price = float(getattr(lt, \"price\", getattr(lt, \"p\", float(\"nan\"))))\n",
        "        ts = pd.to_datetime(getattr(lt, \"timestamp\", getattr(lt, \"t\", None)), utc=True)\n",
        "        if not (pd.notna(ts) and np.isfinite(price)):\n",
        "            return bars_df\n",
        "\n",
        "        lt_age = int((now_utc() - ts).total_seconds())\n",
        "        if lt_age > max_age_sec:\n",
        "            return bars_df\n",
        "\n",
        "        synth_time = max(last_ts + pd.Timedelta(minutes=1), ts.floor(\"min\"))\n",
        "        row = pd.DataFrame(\n",
        "            {\"Open\":[price], \"High\":[price], \"Low\":[price], \"Close\":[price], \"Volume\":[0.0]},\n",
        "            index=pd.DatetimeIndex([synth_time], tz=\"UTC\")\n",
        "        )\n",
        "        patched = pd.concat([bars_df, row]).sort_index()\n",
        "        patched = patched[~patched.index.duplicated(keep=\"last\")]\n",
        "        logging.info(f\"[{symbol}] Patched stale bars with synthetic trade bar @ {synth_time.isoformat()} px={price:.2f}\")\n",
        "        return patched\n",
        "    except Exception as e:\n",
        "        logging.debug(f\"[{symbol}] maybe_patch_stale_with_latest_trade failed: {e}\")\n",
        "        return bars_df\n",
        "\n",
        "#-------------------------------- Single-symbol live step -------------------------------------\n",
        "def run_live_once_for_symbol(\n",
        "    api,\n",
        "    symbol: str,\n",
        "    model: PPO,\n",
        "    vecnorm: Optional[VecNormalize],\n",
        "    features_hint: Optional[dict] = None,\n",
        "    cycle_equity: Optional[float] = None,\n",
        "):\n",
        "    \"\"\"Fetch recent data and run one live decision cycle for a single symbol.\"\"\"\n",
        "\n",
        "    # Resolve expected obs shape and fetch bars\n",
        "    shape = expected_obs_shape(model, vecnorm)\n",
        "    lookback = int(shape[0]) if (shape and len(shape) == 2) else None\n",
        "    bars_need = max(200, (lookback or 0) * 3)\n",
        "    bars_df = get_recent_bars(api, symbol, limit=bars_need, timeframe=TimeFrame.Minute)\n",
        "    if bars_df is None or bars_df.empty:\n",
        "        logging.warning(\"[%s] No recent bars; skipping.\", symbol)\n",
        "        return\n",
        "\n",
        "    # Patch stale last bar if needed\n",
        "    bars_df = maybe_patch_stale_with_latest_trade(api, symbol, bars_df)\n",
        "\n",
        "    # Respect any re-entry cooldown (e.g., after EOD flatten)\n",
        "    block_until = _REENTRY_COOLDOWN_SEC = _REENTRY_BLOCK_UNTIL.get(symbol, 0.0)\n",
        "    if time.time() < block_until:\n",
        "        remaining = int(max(0, block_until - time.time()))\n",
        "        logging.info(f\"[{symbol}] Re-entry cooldown active ({remaining}s left); skipping this cycle.\")\n",
        "        try:\n",
        "            eq = float(cycle_equity) if cycle_equity is not None else float(get_account_equity(api))\n",
        "        except Exception:\n",
        "            eq = float(\"nan\")\n",
        "        try:\n",
        "            px = float(bars_df[\"Close\"].iloc[-1]) if not bars_df.empty else float(get_last_price(api, symbol))\n",
        "        except Exception:\n",
        "            px = float(\"nan\")\n",
        "\n",
        "        _NO_POS_CYCLE_COUNT[symbol] = 0\n",
        "        log_trade_symbol(\n",
        "            symbol,\n",
        "            bars_df.index[-1] if not bars_df.empty else pd.NaT,\n",
        "            signal=0,\n",
        "            raw_action=0.0,\n",
        "            weight=0.0,\n",
        "            confidence=0.0,\n",
        "            price=px,\n",
        "            equity=eq,\n",
        "            dry_run=DRY_RUN,\n",
        "            note=\"reentry_cooldown\"\n",
        "        )\n",
        "        return\n",
        "\n",
        "    # ---- Build observation (this was missing) ----\n",
        "    min_rows_needed = max(20, int(shape[0]) if (shape and len(shape) == 2) else 60)\n",
        "    try:\n",
        "        obs, obs_ts = prepare_observation_from_bars(\n",
        "            bars_df,\n",
        "            features_hint=features_hint,\n",
        "            min_required_rows=min_rows_needed,\n",
        "            expected_shape=shape,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logging.info(\"[%s] Could not prepare observation (%s); skipping.\", symbol, e)\n",
        "        try:\n",
        "            eq = float(cycle_equity) if cycle_equity is not None else float(get_account_equity(api))\n",
        "        except Exception:\n",
        "            eq = float(\"nan\")\n",
        "        try:\n",
        "            px = float(bars_df[\"Close\"].iloc[-1]) if not bars_df.empty else float(get_last_price(api, symbol))\n",
        "        except Exception:\n",
        "            px = float(\"nan\")\n",
        "        log_trade_symbol(\n",
        "            symbol,\n",
        "            bars_df.index[-1] if not bars_df.empty else pd.NaT,\n",
        "            signal=0,\n",
        "            raw_action=0.0,\n",
        "            weight=0.0,\n",
        "            confidence=0.0,\n",
        "            price=px,\n",
        "            equity=eq,\n",
        "            dry_run=DRY_RUN,\n",
        "            note=\"obs_build_failed\"\n",
        "        )\n",
        "        return\n",
        "\n",
        "    # Observation diagnostics (now safe because obs/obs_ts exist)\n",
        "    _obs_shape = getattr(obs, \"shape\", None)\n",
        "    _vecnorm_str = (\n",
        "        f\"{type(vecnorm).__name__}(training={getattr(vecnorm,'training',None)}, \"\n",
        "        f\"norm_reward={getattr(vecnorm,'norm_reward',None)})\"\n",
        "    ) if vecnorm is not None else \"None\"\n",
        "    _now_ts = utc_ts(now_utc())\n",
        "    _age = _now_ts - int(obs_ts)\n",
        "    logging.info(\"[%s] obs_shape=%s | exp_shape=%s | age=%ss | vecnorm=%s\",\n",
        "                 symbol, _obs_shape, shape, _age, _vecnorm_str)\n",
        "\n",
        "    # Staleness guard\n",
        "    if _now_ts - obs_ts >= STALE_MAX_SEC:\n",
        "        logging.info(f\"[{symbol}] Observation stale (age={_now_ts-obs_ts}s ≥ {STALE_MAX_SEC}s); skipping.\")\n",
        "        try:\n",
        "            eq = get_account_equity(api)\n",
        "            px = float(bars_df[\"Close\"].iloc[-1]) if not bars_df.empty else get_last_price(api, symbol)\n",
        "        except Exception:\n",
        "            eq, px = float(\"nan\"), float(\"nan\")\n",
        "        log_trade_symbol(\n",
        "            symbol,\n",
        "            bars_df.index[-1] if not bars_df.empty else pd.NaT,\n",
        "            0, 0.0, 0.0, 0.0, px, eq, DRY_RUN,\n",
        "            note=\"skip_stale\"\n",
        "        )\n",
        "        return\n",
        "\n",
        "    # TP/SL guard\n",
        "    if check_tp_sl_and_maybe_flatten(api, symbol):\n",
        "        return\n",
        "\n",
        "    # Inference\n",
        "    target_w, conf, raw = infer_target_weight(model, vecnorm, obs)\n",
        "    eq = float(cycle_equity) if cycle_equity is not None else get_account_equity(api)\n",
        "    px = float(bars_df[\"Close\"].iloc[-1]) if not bars_df.empty else get_last_price(api, symbol)\n",
        "    have = get_position_qty(api, symbol)\n",
        "\n",
        "    logging.info(\n",
        "        f\"[{symbol}] raw={raw:.4f} conf={conf:.3f} → target_w={target_w:.4f} \"\n",
        "        f\"px=${px:.2f} eq=${eq:,.2f} have={have}\"\n",
        "    )\n",
        "    logging.debug(\n",
        "        f\"[{symbol}] Gates: conf≥ENTER_CONF_MIN? {conf>=ENTER_CONF_MIN} | \"\n",
        "        f\"|target_w|≥ENTER_WEIGHT_MIN? {abs(target_w)>=ENTER_WEIGHT_MIN} | \"\n",
        "        f\"|target_w|≤EXIT_WEIGHT_MAX? {abs(target_w)<=EXIT_WEIGHT_MAX} | \"\n",
        "        f\"Δw floor: {float(globals().get('DELTA_WEIGHT_MIN',0.0))}\"\n",
        "    )\n",
        "\n",
        "    # Optional: auto-seed after N idle cycles (env-controlled)\n",
        "    if os.getenv(\"DEBUG_FORCE_SEED_IF_IDLE\", \"0\").lower() in (\"1\",\"true\",\"yes\"):\n",
        "        if have != 0:\n",
        "            _NO_POS_CYCLE_COUNT[symbol] = 0\n",
        "        else:\n",
        "            _NO_POS_CYCLE_COUNT[symbol] = _NO_POS_CYCLE_COUNT.get(symbol, 0) + 1\n",
        "\n",
        "        idle_cycles = int(os.getenv(\"DEBUG_SEED_IDLE_CYCLES\", \"10\"))\n",
        "        if have == 0 and _NO_POS_CYCLE_COUNT[symbol] >= idle_cycles and ensure_market_open(api):\n",
        "            tradable, fractionable, _ = _asset_flags(symbol)\n",
        "            if not tradable:\n",
        "                log_trade_symbol(symbol, bars_df.index[-1], 0, raw, target_w, conf, px, eq, DRY_RUN,\n",
        "                                 note=\"not_tradable_seed\")\n",
        "                return\n",
        "            seed_amt = round_to_cents(REBALANCE_MIN_NOTIONAL)\n",
        "            if USE_FRACTIONALS and fractionable:\n",
        "                market_order(api, symbol, side=\"buy\", notional=seed_amt)\n",
        "            else:\n",
        "                market_order_to_qty(api, symbol, side=\"buy\", qty=1)\n",
        "            log_trade_symbol(symbol, bars_df.index[-1], 1, raw, target_w, conf, px, eq, DRY_RUN, note=\"debug_force_seed\")\n",
        "            return\n",
        "\n",
        "    # Raw-action gates\n",
        "    RAW_POS_MIN = float(globals().get(\"RAW_POS_MIN\", 0.0))\n",
        "    if target_w > 0 and raw < RAW_POS_MIN:\n",
        "        logging.info(f\"[{symbol}] Raw {raw:.4f} < RAW_POS_MIN {RAW_POS_MIN:.4f}; no action.\")\n",
        "        log_trade_symbol(symbol, bars_df.index[-1], 0, raw, target_w, conf, px, eq, DRY_RUN, note=\"raw_gate_long\")\n",
        "        return\n",
        "\n",
        "    RAW_NEG_GATE = float(globals().get(\"RAW_NEG_MAX\", 0.0))\n",
        "    if target_w < 0 and abs(raw) < RAW_NEG_GATE:\n",
        "        logging.info(f\"[{symbol}] |raw| {abs(raw):.4f} < RAW_NEG_GATE {RAW_NEG_GATE:.4f}; no action.\")\n",
        "        log_trade_symbol(symbol, bars_df.index[-1], 0, raw, target_w, conf, px, eq, DRY_RUN, note=\"raw_gate_short\")\n",
        "        return\n",
        "\n",
        "    # Flatten if model near-flat and we have a position\n",
        "    pos = get_position(api, symbol)\n",
        "    if abs(target_w) <= EXIT_WEIGHT_MAX and pos:\n",
        "        logging.info(f\"[{symbol}] Model near-flat (≤{EXIT_WEIGHT_MAX:.3f}); flattening.\")\n",
        "        flatten_symbol(api, symbol)\n",
        "        log_trade_symbol(symbol, bars_df.index[-1], int(target_w > 0), raw, target_w, conf, px, eq, DRY_RUN, note=\"flatten\")\n",
        "        return\n",
        "\n",
        "    # Low confidence and near-flat → no action\n",
        "    if conf < ENTER_CONF_MIN and abs(target_w) <= EXIT_WEIGHT_MAX:\n",
        "        logging.info(f\"[{symbol}] Below conf/near-flat gates; no action.\")\n",
        "        log_trade_symbol(symbol, bars_df.index[-1], int(target_w > 0), raw, target_w, conf, px, eq, DRY_RUN, note=\"no_action\")\n",
        "        return\n",
        "\n",
        "    # Entry / rebalance\n",
        "    if abs(target_w) >= ENTER_WEIGHT_MIN and conf >= ENTER_CONF_MIN:\n",
        "        if SEED_FIRST_SHARE and have == 0:\n",
        "            if _too_soon(f\"{symbol}#seed\", _SEED_COOLDOWN_SEC):\n",
        "                log_trade_symbol(symbol, bars_df.index[-1], 0, raw, target_w, conf, px, eq, DRY_RUN, note=\"seed_cooldown\")\n",
        "                return\n",
        "\n",
        "            seed_notional = round_to_cents(REBALANCE_MIN_NOTIONAL)\n",
        "            side = \"buy\" if target_w > 0 else \"sell\"\n",
        "\n",
        "            tradable, fractionable, _shortable = _asset_flags(symbol)\n",
        "            if not tradable:\n",
        "                log_trade_symbol(symbol, bars_df.index[-1], 0, raw, target_w, conf, px, eq, DRY_RUN, note=\"not_tradable_seed\")\n",
        "                return\n",
        "\n",
        "            if side == \"sell\":\n",
        "                ok, note = _can_seed_short(api, symbol)\n",
        "                if not ok:\n",
        "                    log_trade_symbol(symbol, bars_df.index[-1], 0, raw, target_w, conf, px, eq, DRY_RUN, note=note)\n",
        "                    return\n",
        "                market_order_to_qty(api, symbol, side=\"sell\", qty=1)\n",
        "            else:\n",
        "                if USE_FRACTIONALS and fractionable:\n",
        "                    market_order(api, symbol, side=\"buy\", notional=seed_notional)\n",
        "                else:\n",
        "                    market_order_to_qty(api, symbol, side=\"buy\", qty=1)\n",
        "\n",
        "            rebalance_to_weight(api, symbol, eq, target_w)\n",
        "            log_trade_symbol(symbol, bars_df.index[-1], int(target_w > 0), raw, target_w, conf, px, eq, DRY_RUN, note=\"seed_open\")\n",
        "            _LAST_ORDER_TS[symbol] = time.time()\n",
        "            return\n",
        "\n",
        "        if _too_soon(symbol, 30):\n",
        "            log_trade_symbol(symbol, bars_df.index[-1], 0, raw, target_w, conf, px, eq, DRY_RUN, note=\"rebalance_cooldown\")\n",
        "            return\n",
        "\n",
        "        rebalance_to_weight(api, symbol, eq, target_w)\n",
        "        log_trade_symbol(symbol, bars_df.index[-1], int(target_w > 0), raw, target_w, conf, px, eq, DRY_RUN, note=\"rebalance_try\")\n",
        "        return\n",
        "\n",
        "#--------------------------------- Live runner -------------------------------------------------\n",
        "def run_live(tickers: List[str], api: tradeapi.REST):\n",
        "    def minutes_to_close(api: tradeapi.REST) -> Optional[int]:\n",
        "        clk = api.get_clock()\n",
        "        if getattr(clk, \"is_open\", False):\n",
        "            close = pd.to_datetime(clk.next_close, utc=True)\n",
        "            m = int(max(0, (close - now_utc()).total_seconds() // 60))\n",
        "            return m\n",
        "        return None\n",
        "\n",
        "    api_local = api\n",
        "\n",
        "    per_ticker: Dict[str, Tuple[PPO, Optional[VecNormalize], Optional[dict]]] = {}\n",
        "    best = (globals().get(\"BEST_WINDOW_ENV\") or None)\n",
        "\n",
        "    for t in tickers:\n",
        "        try:\n",
        "            picks   = pick_artifacts_for_ticker(t, os.getenv(\"ARTIFACTS_DIR\", str(ARTIFACTS_DIR)), best_window=best)\n",
        "            model   = load_ppo_model(picks[\"model\"])\n",
        "            vecnorm = load_vecnormalize(picks.get(\"vecnorm\"))\n",
        "            if vecnorm and hasattr(vecnorm, \"training\"): vecnorm.training = False\n",
        "            if vecnorm and hasattr(vecnorm, \"norm_reward\"): vecnorm.norm_reward = False\n",
        "            feats   = load_features(picks.get(\"features\"))\n",
        "            per_ticker[t] = (model, vecnorm, feats)\n",
        "            logging.info(\"[%s] Artifacts loaded and ready.\", t)\n",
        "        except Exception as e:\n",
        "            logging.exception(\"[%s] Failed to load artifacts: %s\", t, e)\n",
        "\n",
        "    if not per_ticker:\n",
        "        raise RuntimeError(\"No models loaded for any ticker. Check artifacts directory and names.\")\n",
        "\n",
        "    loaded_syms = list(per_ticker.keys())\n",
        "    logging.info(\"Starting live execution for (loaded): %s\", loaded_syms)\n",
        "\n",
        "    cycle = 0\n",
        "    last_plot_ts = 0\n",
        "    flattened_today = False  #ensure we only flatten once into the close\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            if not ensure_market_open(api_local):\n",
        "                flattened_today = False  #reset for next session\n",
        "                #reset the anchor so we re-capture at the next open\n",
        "                globals()[\"SESSION_OPEN_EQUITY\"] = None\n",
        "                _sleep_until_open(api_local)\n",
        "                continue\n",
        "\n",
        "            #Anchor the day's opening equity once per session\n",
        "            if globals().get(\"SESSION_OPEN_EQUITY\") is None:\n",
        "                try:\n",
        "                    globals()[\"SESSION_OPEN_EQUITY\"] = float(api_local.get_account().equity)\n",
        "                    logging.info(\"Session open equity anchor set: %.2f\", globals().get(\"SESSION_OPEN_EQUITY\", np.nan))\n",
        "                except Exception as e:\n",
        "                    logging.debug(f\"Could not set SESSION_OPEN_EQUITY: {e}\")\n",
        "\n",
        "            t_cycle_start = time.perf_counter()\n",
        "            cycle_equity = float(api_local.get_account().equity)\n",
        "\n",
        "            #---- per-symbol work (no sleeping here) ----\n",
        "            for t, (model, vecnorm, feat_hint) in per_ticker.items():\n",
        "                t_sym_start = time.perf_counter()\n",
        "                run_live_once_for_symbol(api_local, t, model, vecnorm, features_hint=feat_hint, cycle_equity=cycle_equity)\n",
        "                logging.info(\"[TIMER] %s symbol work: %.3fs\", t, time.perf_counter() - t_sym_start)\n",
        "\n",
        "            #---- once per cycle (AFTER the for-loop) ----\n",
        "            maybe_log_equity_snapshot(api_in=api_local, reason=(\"trade\" if globals().get(\"_TRADE_EVENT_FLAG\", False) else \"cycle\"))\n",
        "\n",
        "            try:\n",
        "                anchor = globals().get(\"SESSION_OPEN_EQUITY\", None)\n",
        "                if anchor is not None:\n",
        "                    eq_now = float(api_local.get_account().equity)\n",
        "                    dd = (eq_now / max(1e-9, float(anchor))) - 1.0\n",
        "\n",
        "                    max_dd = float(os.getenv(\n",
        "                        \"MAX_DAILY_DRAWDOWN_PCT\",\n",
        "                        getattr(cfg, \"MAX_DAILY_DRAWDOWN_PCT\", globals().get(\"MAX_DAILY_DRAWDOWN_PCT\", 0.05))\n",
        "                    ))\n",
        "                    if dd <= -abs(max_dd):\n",
        "                        global _last_kill_ts\n",
        "                        if time.time() - _last_kill_ts > 60:  #de-bounce\n",
        "                            for sym in per_ticker.keys():\n",
        "                                flatten_symbol(api_local, sym)\n",
        "                            logging.warning(\"KILL-SWITCH: daily drawdown %.2f%% reached. Flattening & pausing.\",\n",
        "                                            100.0 * dd)\n",
        "                            _last_kill_ts = time.time()\n",
        "\n",
        "                            #Cooldown; if DRY_RUN, skip sleeping so your loop remains responsive\n",
        "                            cooldown_min = int(os.getenv(\n",
        "                                \"KILL_SWITCH_COOLDOWN_MIN\",\n",
        "                                getattr(cfg, \"KILL_SWITCH_COOLDOWN_MIN\", globals().get(\"KILL_SWITCH_COOLDOWN_MIN\", 30))\n",
        "                            ))\n",
        "                            if not DRY_RUN:\n",
        "                                time.sleep(60 * cooldown_min)\n",
        "                            continue  #start next cycle after cooldown\n",
        "            except Exception as e:\n",
        "                logging.debug(f\"kill-switch check failed: {e}\")\n",
        "\n",
        "            #Flatten into the close (≤5 min), but only once per session\n",
        "            m2c = minutes_to_close(api_local)\n",
        "            if FLATTEN_INTO_CLOSE and not flattened_today and m2c is not None and m2c <= 5:\n",
        "                for sym in per_ticker.keys():\n",
        "                    flatten_symbol(api_local, sym)\n",
        "                    #start re-entry cooldown after end-of-day flatten\n",
        "                    _REENTRY_BLOCK_UNTIL[sym] = time.time() + REENTRY_COOLDOWN_SEC\n",
        "                logging.info(\"Flattened all positions into the close.\")\n",
        "                maybe_log_equity_snapshot(api_in=api_local, reason=\"close\")\n",
        "                flattened_today = True\n",
        "\n",
        "                if bool(globals().get(\"EXIT_AFTER_CLOSE\", False)):\n",
        "                    logging.info(\"EXIT_AFTER_CLOSE=True — exiting live loop after close flatten.\")\n",
        "                    break\n",
        "\n",
        "            cycle += 1\n",
        "\n",
        "            #Throttled plot/metrics (~15 min)\n",
        "            now_ts = time.time()\n",
        "            if now_ts - last_plot_ts >= 900:\n",
        "                try:\n",
        "                    plot_equity_curve(from_equity_csv=True)\n",
        "                    df = pd.read_csv(EQUITY_LOG_CSV, parse_dates=[\"datetime_utc\"])\n",
        "                    m = compute_performance_metrics(df)\n",
        "                    logging.info(\"Perf: cum_return=%.2f%% | sharpe=%.2f | maxDD=%.2f%%\",\n",
        "                                 100*m[\"cum_return\"], m[\"sharpe\"], 100*m[\"max_drawdown\"])\n",
        "                except Exception as e:\n",
        "                    logging.warning(\"Plot/metrics failed: %s\", e)\n",
        "                last_plot_ts = now_ts\n",
        "\n",
        "            logging.info(\"[TIMER] full-cycle active time: %.3fs (cooldown=%d min)\",\n",
        "                         time.perf_counter() - t_cycle_start, COOLDOWN_MIN)\n",
        "\n",
        "            if (cycle % 12) == 0:\n",
        "                gc.collect()\n",
        "\n",
        "            _sleep_to_next_minute_block(COOLDOWN_MIN)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        logging.info(\"KeyboardInterrupt: stopping live loop.\")\n",
        "        try:\n",
        "            if os.getenv(\"FORCE_FLATTEN_ON_EXIT\", \"0\").lower() in (\"1\", \"true\", \"yes\"):\n",
        "                flatten_all_positions(api_local)\n",
        "        except Exception as e:\n",
        "            logging.warning(\"Flatten-on-exit skipped: %s\", e)\n",
        "\n",
        "        #Finalize logs/plots\n",
        "        try:\n",
        "            maybe_log_equity_snapshot(api_in=api_local, reason=\"finalize\")\n",
        "            plot_equity_curve(from_equity_csv=True)\n",
        "        except Exception as e:\n",
        "            logging.warning(\"Finalization failed: %s\", e)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.exception(\"Live loop exception: %s\", e)\n",
        "        try:\n",
        "            log_equity_snapshot(api_in=api_local)\n",
        "        except Exception:\n",
        "            pass\n",
        "        time.sleep(5)\n",
        "\n",
        "\n",
        "\n",
        "#--------------------------------- Diagnostic runner -------------------------------------------\n",
        "def ticker_diagnostic(ticker: str,\n",
        "                      dry_run: bool = None,\n",
        "                      timeframe: TimeFrame = TimeFrame.Minute,\n",
        "                      limit: int = 300,\n",
        "                      api: Optional[tradeapi.REST] = None):\n",
        "    \"\"\"One-shot diagnostic for a single ticker \"\"\"\n",
        "    if dry_run is None:\n",
        "        dry_run = bool(globals().get(\"DRY_RUN\", True))\n",
        "\n",
        "    print(f\"\\nRunning strategy for {ticker}...\")\n",
        "\n",
        "    #Init Alpaca + baseline state\n",
        "    try:\n",
        "        api_local = api or init_alpaca()\n",
        "        positions_start = len(api_local.list_positions())\n",
        "        orders_start    = len(api_local.list_orders(status=\"open\"))\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Alpaca: {e}\")\n",
        "        return\n",
        "\n",
        "    #Load artifacts\n",
        "    try:\n",
        "        best   = (globals().get(\"BEST_WINDOW_ENV\") or None)\n",
        "        picks  = pick_artifacts_for_ticker(\n",
        "            ticker,\n",
        "            os.getenv(\"ARTIFACTS_DIR\", str(globals().get(\"ARTIFACTS_DIR\", PROJECT_ROOT / \"artifacts\"))),\n",
        "            best_window=best\n",
        "        )\n",
        "        model   = load_ppo_model(picks[\"model\"])\n",
        "        vecnorm = load_vecnormalize(picks.get(\"vecnorm\")) if picks.get(\"vecnorm\") else None\n",
        "        if vecnorm and hasattr(vecnorm, \"training\"): vecnorm.training = False\n",
        "        if vecnorm and hasattr(vecnorm, \"norm_reward\"): vecnorm.norm_reward = False\n",
        "        feats   = load_features(picks.get(\"features\"))\n",
        "        print(f\"Model artifacts loaded for {ticker}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load model for {ticker}: {e}\")\n",
        "        return\n",
        "\n",
        "    #Fetch bars & build initial observation\n",
        "    min_rows_needed = 60\n",
        "    try:\n",
        "        shape     = expected_obs_shape(model, vecnorm)\n",
        "        lookback  = int(shape[0]) if (shape is not None and len(shape) == 2) else None\n",
        "        bars_need = max(200, (lookback or 0) * 3)\n",
        "        bars_df   = get_recent_bars(api_local, ticker, limit=max(limit, bars_need), timeframe=timeframe)\n",
        "        min_rows_needed = max(20, int(shape[0]) if (shape and len(shape)==2) else 20)\n",
        "        if len(bars_df) < min_rows_needed:\n",
        "            print(f\"Not enough data for {ticker}: {len(bars_df)} rows (need ≥ {min_rows_needed})\")\n",
        "            bars_df = pd.DataFrame()\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching bars for {ticker}: {e}\")\n",
        "        bars_df = pd.DataFrame()\n",
        "\n",
        "    obs, obs_ts = None, None\n",
        "    bars_df = maybe_patch_stale_with_latest_trade(api_local, ticker, bars_df)\n",
        "    if not bars_df.empty:\n",
        "        try:\n",
        "            obs, obs_ts = prepare_observation_from_bars(\n",
        "                bars_df,\n",
        "                features_hint=feats,\n",
        "                min_required_rows=min_rows_needed,\n",
        "                expected_shape=shape,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error preparing observation for {ticker}: {e}\")\n",
        "\n",
        "    signal = None\n",
        "    target_w = conf = raw = float(\"nan\")\n",
        "    predictions_made = 0\n",
        "    bar_time = pd.NaT\n",
        "    price = float(\"nan\")\n",
        "    equity = float(\"nan\")\n",
        "\n",
        "    orders_submitted = 0\n",
        "    market_closed = 0\n",
        "\n",
        "    if obs is not None:\n",
        "        try:\n",
        "            #Predict once for the diagnostic summary\n",
        "            target_w, conf, raw = infer_target_weight(model, vecnorm, obs)\n",
        "            signal = int(target_w > 0.0)  #(diagnostic display only)\n",
        "            predictions_made = 1\n",
        "            print(f\"Prediction for {ticker}: {signal} (1 = Buy, 0 = Sell)\")\n",
        "\n",
        "            bar_time = bars_df.index[-1] if not bars_df.empty else pd.NaT\n",
        "            price    = float(bars_df[\"Close\"].iloc[-1]) if not bars_df.empty else get_last_price(api_local, ticker)\n",
        "            equity   = get_account_equity(api_local)\n",
        "            print(f\"raw={raw:.4f} conf={conf:.3f} target_w={target_w:.3f} price=${price:.2f} equity=${equity:,.2f}\")\n",
        "\n",
        "            #Log a diagnostic row (even if we don't submit an order)\n",
        "            log_trade_symbol(\n",
        "                ticker, bar_time, signal, raw, target_w, conf, price, equity,\n",
        "                dry_run=dry_run, note=\"diagnostic\"\n",
        "            )\n",
        "\n",
        "            #--- Inner “clock/orders” section (fixed) ---\n",
        "            try:\n",
        "                clock = api_local.get_clock()\n",
        "                if not getattr(clock, \"is_open\", False):\n",
        "                    print(\"Market is closed.\")\n",
        "                    market_closed = 1\n",
        "            else:\n",
        "                # Rebuild obs right before order logic to reduce staleness\n",
        "                obs, obs_ts = prepare_observation_from_bars(\n",
        "                    bars_df,\n",
        "                    features_hint=feats,\n",
        "                    min_required_rows=min_rows_needed,\n",
        "                    expected_shape=shape,\n",
        "                )\n",
        "\n",
        "                _age = utc_ts(now_utc()) - int(obs_ts)\n",
        "                logging.info(\"[%s] obs_shape=%s | exp_shape=%s | age=%ss | vecnorm=%s\",\n",
        "                            ticker, getattr(obs, \"shape\", None), shape, _age,\n",
        "                            (f\"{type(vecnorm).__name__}(training={getattr(vecnorm,'training',None)}, \"\n",
        "                              f\"norm_reward={getattr(vecnorm,'norm_reward',None)})\") if vecnorm else \"None\")\n",
        "\n",
        "                if utc_ts(now_utc()) - obs_ts > STALE_MAX_SEC:\n",
        "                    print(\"Stale observation; skipping order submission.\")\n",
        "                    log_trade_symbol(\n",
        "                        ticker, bar_time, 0, raw, target_w, conf, price, equity,\n",
        "                        dry_run, note=\"skip_stale_diag\"\n",
        "                    )\n",
        "\n",
        "                elif signal is not None and not dry_run:\n",
        "                    # Do we already hold the ticker?\n",
        "                    try:\n",
        "                        pos = api_local.get_position(ticker)\n",
        "                        has_position = float(pos.qty) != 0.0\n",
        "                    except APIError:\n",
        "                        has_position = False\n",
        "\n",
        "                    have = get_position_qty(api_local, ticker)\n",
        "\n",
        "                    # Optional \"first buy\" helper for diagnostics only\n",
        "                    if FORCE_FIRST_BUY and not has_position and signal == 1:\n",
        "                        market_order(\n",
        "                            api_local, symbol=ticker, side=\"buy\",\n",
        "                            qty=(1 if not USE_FRACTIONALS else None),\n",
        "                            notional=(price if USE_FRACTIONALS else None),\n",
        "                        )\n",
        "                        print(f\"BUY order submitted for {ticker} (FORCE_FIRST_BUY)\")\n",
        "                        orders_submitted += 1\n",
        "\n",
        "                    # Seed logic when no position but confidence/weight pass gates\n",
        "                    elif (\n",
        "                        SEED_FIRST_SHARE\n",
        "                        and have == 0\n",
        "                        and abs(target_w) >= ENTER_WEIGHT_MIN\n",
        "                        and conf >= ENTER_CONF_MIN\n",
        "                    ):\n",
        "                        seed_notional = max(REBALANCE_MIN_NOTIONAL, round_to_cents(price if np.isfinite(price) else 1.00))\n",
        "                        side = \"buy\" if target_w > 0 else \"sell\"\n",
        "\n",
        "                        if side == \"sell\":\n",
        "                            if not globals().get(\"ALLOW_SHORTS\", False):\n",
        "                                print(f\"[{ticker}] Shorts disabled (ALLOW_SHORTS=False); skipping seed short.\")\n",
        "                                log_trade_symbol(ticker, bar_time, 0, raw, target_w, conf, price, equity, dry_run, note=\"shorts_disabled_seed\")\n",
        "                            else:\n",
        "                                try:\n",
        "                                    a = api_local.get_asset(ticker)\n",
        "                                    if not getattr(a, \"shortable\", False):\n",
        "                                        print(f\"[{ticker}] Not shortable; skipping seed short.\")\n",
        "                                        log_trade_symbol(ticker, bar_time, 0, raw, target_w, conf, price, equity, dry_run, note=\"not_shortable_seed\")\n",
        "                                    else:\n",
        "                                        market_order_to_qty(api_local, ticker, side=\"sell\", qty=1)\n",
        "                                        log_trade_symbol(ticker, bar_time, int(target_w > 0), raw, target_w, conf, price, equity, dry_run, note=\"seed_open\")\n",
        "                                        orders_submitted += 1\n",
        "                                except Exception as e:\n",
        "                                    print(f\"[{ticker}] get_asset shortable check failed: {e}\")\n",
        "                                    log_trade_symbol(ticker, bar_time, 0, raw, target_w, conf, price, equity, dry_run, note=\"shortable_check_error\")\n",
        "                        else:\n",
        "                            if USE_FRACTIONALS:\n",
        "                                market_order(api_local, ticker, side=\"buy\", notional=seed_notional)\n",
        "                            else:\n",
        "                                market_order_to_qty(api_local, ticker, side=\"buy\", qty=1)\n",
        "                            log_trade_symbol(ticker, bar_time, int(target_w > 0), raw, target_w, conf, price, equity, dry_run, note=\"seed_open\")\n",
        "                            orders_submitted += 1\n",
        "\n",
        "                    # Simple directional demo for diagnostics\n",
        "                    elif signal == 1 and not has_position:\n",
        "                        market_order(api_local, symbol=ticker, side=\"buy\",\n",
        "                                    qty=(1 if not USE_FRACTIONALS else None),\n",
        "                                    notional=(price if USE_FRACTIONALS else None))\n",
        "                        print(f\"BUY order submitted for {ticker}\")\n",
        "                        orders_submitted += 1\n",
        "\n",
        "                    elif signal == 0 and has_position and have > 0:\n",
        "                        market_order(api_local, symbol=ticker, side=\"sell\",\n",
        "                                    qty=(1 if not USE_FRACTIONALS else None),\n",
        "                                    notional=(price if USE_FRACTIONALS else None))\n",
        "                        print(f\"SELL order submitted for {ticker}\")\n",
        "                        orders_submitted += 1\n",
        "\n",
        "                    else:\n",
        "                        print(f\"No action taken for {ticker}\")\n",
        "\n",
        "                else:\n",
        "                    # dry_run path\n",
        "                    print(f\"(dry-run) No order submitted for {ticker} — signal={signal}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Trade/clock error for {ticker}: {e}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Inference error for {ticker}: {e}\")\n",
        "\n",
        "    #Final summary\n",
        "    try:\n",
        "        positions_end = len(api_local.list_positions())\n",
        "        orders_end    = len(api_local.list_orders(status=\"open\"))\n",
        "        print(\"\\n========== SUMMARY ==========\")\n",
        "        print(f\"Processed:         1\")\n",
        "        print(f\"Models loaded:     1\")\n",
        "        print(f\"Predictions made:  {predictions_made}\")\n",
        "        print(f\"Market closed:     {market_closed}\")\n",
        "        print(f\"Orders submitted:  {orders_submitted} (dry_run={dry_run})\")\n",
        "        print(f\"Existing positions (start -> end): {positions_start} -> {positions_end}\")\n",
        "        print(f\"Open orders        (start -> end): {orders_start} -> {orders_end}\")\n",
        "        print(\"=============================\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return {\n",
        "        \"signal\": signal,\n",
        "        \"target_w\": target_w,\n",
        "        \"conf\": conf,\n",
        "        \"raw\": raw,\n",
        "        \"bar_time\": bar_time,\n",
        "        \"price\": price,\n",
        "        \"equity\": equity,\n",
        "        \"dry_run\": dry_run,\n",
        "    }\n",
        "\n",
        "#--------------------------------- Config banner -----------------------------------------------\n",
        "def log_config_banner():\n",
        "    try:\n",
        "        artifacts_list = sorted(p.name for p in ARTIFACTS_DIR.iterdir()) if ARTIFACTS_DIR.exists() else []\n",
        "    except Exception:\n",
        "        artifacts_list = []\n",
        "\n",
        "    logging.info(\"EXIT_AFTER_CLOSE     : %s\", os.getenv(\"EXIT_AFTER_CLOSE\", \"0\"))\n",
        "    logging.info(\"FORCE_FIRST_BUY       : %s\", FORCE_FIRST_BUY)\n",
        "    logging.info(\"CONFIG\")\n",
        "    logging.info(\"Project root        : %s\", PROJECT_ROOT)\n",
        "    logging.info(\"ARTIFACTS_DIR       : %s\", ARTIFACTS_DIR)\n",
        "    logging.info(\"RESULTS_DIR         : %s\", RESULTS_DIR)\n",
        "    logging.info(\"Tickers             : %s\", TICKERS)\n",
        "    logging.info(\"API base            : %s\", BASE_URL)\n",
        "    logging.info(\"AUTO_RUN_LIVE       : %s\", os.getenv(\"AUTO_RUN_LIVE\", \"\"))\n",
        "    logging.info(\"INF_DETERMINISTIC   : %s\", INF_DETERMINISTIC)\n",
        "    logging.info(\"ALLOW_SHORTS        : %s\", ALLOW_SHORTS)\n",
        "    logging.info(\"FLATTEN_INTO_CLOSE  : %s\", os.getenv(\"FLATTEN_INTO_CLOSE\", str(FLATTEN_INTO_CLOSE)))\n",
        "    logging.info(\"REENTRY_COOLDOWN_SEC: %s\", os.getenv(\"REENTRY_COOLDOWN_SEC\", str(REENTRY_COOLDOWN_SEC)))\n",
        "    logging.info(\n",
        "        \"DRY_RUN: %s | BARS_FEED: %s | USE_FRACTIONALS: %s | COOLDOWN_MIN: %s | STALE_MAX_SEC: %s\",\n",
        "        DRY_RUN, BARS_FEED, USE_FRACTIONALS, COOLDOWN_MIN, STALE_MAX_SEC,\n",
        "    )\n",
        "\n",
        "    logging.info(\n",
        "        \"DEBUG_FORCE_SEED_IF_IDLE: %s | DEBUG_SEED_IDLE_CYCLES: %s\",\n",
        "        os.getenv(\"DEBUG_FORCE_SEED_IF_IDLE\",\"1\"),\n",
        "        os.getenv(\"DEBUG_SEED_IDLE_CYCLES\",\"10\"),\n",
        "    )\n",
        "\n",
        "    logging.info(\"PH_TIMEOUT_SEC       : %s\", os.getenv(\"PH_TIMEOUT_SEC\", \"8\"))\n",
        "    logging.info(\"MAX_DD_PCT: %.3f | KILL_SWITCH_COOLDOWN_MIN: %s\",\n",
        "                float(globals().get(\"MAX_DAILY_DRAWDOWN_PCT\", 0.05)),\n",
        "                os.getenv(\"KILL_SWITCH_COOLDOWN_MIN\",\n",
        "                          str(globals().get(\"KILL_SWITCH_COOLDOWN_MIN\", 30))))\n",
        "\n",
        "    logging.info(\n",
        "        \"WEIGHT_CAP: %.3f | SIZING_MODE: %s | ENTER_CONF_MIN: %.3f | ENTER_WEIGHT_MIN: %.3f | \"\n",
        "        \"EXIT_WEIGHT_MAX: %.3f | REBALANCE_MIN_NOTIONAL: %.2f\",\n",
        "        WEIGHT_CAP, SIZING_MODE, ENTER_CONF_MIN, ENTER_WEIGHT_MIN, EXIT_WEIGHT_MAX, REBALANCE_MIN_NOTIONAL,\n",
        "    )\n",
        "    logging.info(\n",
        "        \"TAKE_PROFIT_PCT: %.3f | STOP_LOSS_PCT: %.3f | BEST_WINDOW_ENV: %s\",\n",
        "        TAKE_PROFIT_PCT, STOP_LOSS_PCT, (BEST_WINDOW_ENV or \"\"),\n",
        "    )\n",
        "    logging.info(\n",
        "        \"DELTA_WEIGHT_MIN: %.3f | RAW_POS_MIN: %.3f | RAW_NEG_MAX: %.3f\",\n",
        "        float(globals().get(\"DELTA_WEIGHT_MIN\", 0.0)),\n",
        "        float(globals().get(\"RAW_POS_MIN\", 0.0)),\n",
        "        float(globals().get(\"RAW_NEG_MAX\", 0.0)),\n",
        "    )\n",
        "    if artifacts_list:\n",
        "        logging.info(\"Artifacts present (%d): %s\", len(artifacts_list), \", \".join(artifacts_list))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if IN_COLAB:\n",
        "        upload_env_and_artifacts_in_colab()\n",
        "        _maybe_convert_features_txt_to_json()\n",
        "        _maybe_rename_vecnorm_scaler()\n",
        "        load_dotenv(dotenv_path=PROJECT_ROOT / \".env\", override=True)\n",
        "\n",
        "    #Always configure (both local and Colab)\n",
        "    cfg = configure_knobs(overrides={\n",
        "        #data freshness\n",
        "        \"BARS_FEED\": \"iex\",\n",
        "        \"STALE_MAX_SEC\": 600,\n",
        "\n",
        "        #entry/exit sensitivity (add gentle friction)\n",
        "        \"ENTER_CONF_MIN\": 0.02,\n",
        "        \"ENTER_WEIGHT_MIN\": 0.010,\n",
        "        \"EXIT_WEIGHT_MAX\": 0.007,\n",
        "\n",
        "        #sizing & minimums\n",
        "        \"WEIGHT_CAP\": 0.25,\n",
        "        \"DELTA_WEIGHT_MIN\": 0.003,\n",
        "        \"REBALANCE_MIN_NOTIONAL\": 7.50,\n",
        "\n",
        "        #posture\n",
        "        \"ALLOW_SHORTS\": False,\n",
        "        \"COOLDOWN_MIN\": 1,\n",
        "\n",
        "        #raw-action gates\n",
        "        \"RAW_POS_MIN\": 0.00,\n",
        "        \"RAW_NEG_MAX\": 0.00,\n",
        "\n",
        "        #risk\n",
        "        \"TAKE_PROFIT_PCT\": 0.02,\n",
        "        \"STOP_LOSS_PCT\": 0.01,\n",
        "\n",
        "        #logging cadence\n",
        "        \"EQUITY_LOG_THROTTLE_SEC\": 300,\n",
        "        \"SKIP_EQUITY_WHEN_DRY_RUN\": False,\n",
        "\n",
        "        #sane kill-switch\n",
        "        \"MAX_DAILY_DRAWDOWN_PCT\": 0.05,\n",
        "\n",
        "    })\n",
        "    globals()[\"cfg\"] = cfg  #so functions that reference `cfg` can see it\n",
        "\n",
        "    #Only block the live loop (not diagnostics) if DRY_RUN is on\n",
        "    if cfg.AUTO_RUN_LIVE:\n",
        "        assert not cfg.DRY_RUN, \"Refusing to start live loop with DRY_RUN=True\"\n",
        "\n",
        "    log_config_banner()\n",
        "\n",
        "    #Save a one-file snapshot of the run config BEFORE starting the loop\n",
        "    try:\n",
        "        cfg_path = RESULTS_DIR / \"run_config.json\"\n",
        "        payload = {\n",
        "            \"time\": utcnow_iso(),\n",
        "            \"tickers\": TICKERS,\n",
        "            \"dry_run\": DRY_RUN,\n",
        "            \"bars_feed\": BARS_FEED,\n",
        "            \"weight_cap\": WEIGHT_CAP,\n",
        "            \"enter_conf_min\": ENTER_CONF_MIN,\n",
        "            \"enter_weight_min\": ENTER_WEIGHT_MIN,\n",
        "            \"exit_weight_max\": EXIT_WEIGHT_MAX,\n",
        "            \"rebalance_min_notional\": REBALANCE_MIN_NOTIONAL,\n",
        "            \"delta_weight_min\": DELTA_WEIGHT_MIN,\n",
        "            \"tp\": TAKE_PROFIT_PCT,\n",
        "            \"sl\": STOP_LOSS_PCT,\n",
        "            \"allow_shorts\": ALLOW_SHORTS,\n",
        "        }\n",
        "        tmp = cfg_path.with_suffix(\".tmp\")\n",
        "        tmp.write_text(json.dumps(payload, indent=2))\n",
        "        tmp.replace(cfg_path)\n",
        "    except Exception as e:\n",
        "        logging.warning(\"Could not write run_config.json: %s\", e)\n",
        "\n",
        "    #Paper safety (BASE_URL is set by cfg.apply_to_globals())\n",
        "    assert \"paper-api\" in BASE_URL.lower(), f\"Refusing to trade: BASE_URL is not paper ({BASE_URL})\"\n",
        "\n",
        "    #Single init\n",
        "    api = init_alpaca()\n",
        "    acct = api.get_account()\n",
        "\n",
        "    #Optional sanity: don't run if Alpaca says trading is blocked\n",
        "    assert not bool(getattr(acct, \"trading_blocked\", False)), f\"Trading is blocked on this account: {getattr(acct,'status','')}\"\n",
        "\n",
        "    logging.info(\"Account status: %s | equity=%s | cash=%s\", acct.status, acct.equity, acct.cash)\n",
        "    write_account_info_to_run_config(api)\n",
        "\n",
        "    if cfg.AUTO_RUN_LIVE:\n",
        "        run_live(TICKERS, api)   #pass the client in\n",
        "    else:\n",
        "        logging.info(\"AUTO_RUN_LIVE disabled; live loop not started.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5c480aa-48e3-414a-cb54-ddb8a0cd1751",
        "id": "aDG1KQ9My9C-"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "2025-12-26 00:44:30,958 - INFO - RUN_RESULTS_DIR   = /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044\n",
            "2025-12-26 00:44:30,960 - INFO - FINAL_MODEL_DIR  = /content/drive/MyDrive/Results_May_2025/ppo_models_master\n",
            "2025-12-26 00:44:30,961 - INFO - BASE_RESULTS_DIR = /content/drive/MyDrive/Results_May_2025\n",
            "2025-12-26 00:44:31,078 - INFO - Training candidate symbols: ['AAPL', 'QCOM', 'COST', 'RTX', 'BRK-B', 'SBUX', 'TMO', 'BAC', 'TSLA', 'CRM', 'AVGO', 'TXN', 'UNH', 'ADBE', 'AMGN', 'UNP', 'AMD', 'ACN', 'AMZN', 'ABT', 'WMT', 'ABBV', 'XOM', 'PFE', 'PM', 'CSCO', 'PG', 'LLY', 'MA', 'MCD', 'META', 'MRK', 'JPM', 'MSFT', 'NVDA', 'NKE', 'INTC', 'LOW', 'CVX', 'PEP', 'HD', 'GOOGL', 'ORCL', 'GE', 'BMY', 'JNJ', 'NEE', 'LIN', 'KO', 'V', 'UPS', 'DHR', 'IBM']\n",
            "2025-12-26 00:44:36,459 - INFO - Final training universe: ['AAPL', 'QCOM', 'COST', 'RTX', 'BRK-B', 'SBUX', 'TMO', 'BAC', 'TSLA', 'CRM', 'AVGO', 'TXN', 'UNH', 'ADBE', 'AMGN', 'UNP', 'AMD', 'ACN', 'AMZN', 'ABT', 'WMT', 'ABBV', 'XOM', 'PFE', 'PM', 'CSCO', 'PG', 'LLY', 'MA', 'MCD', 'META', 'MRK', 'JPM', 'MSFT', 'NVDA', 'NKE', 'INTC', 'LOW', 'CVX', 'PEP', 'HD', 'GOOGL', 'ORCL', 'GE', 'BMY', 'JNJ', 'NEE', 'LIN', 'KO', 'V', 'UPS', 'DHR', 'IBM']\n",
            "2025-12-26 00:44:36,461 - INFO - Test mode: running on ['AAPL', 'NVDA', 'MSFT']\n",
            "2025-12-26 00:44:36,462 - INFO - >>> [TEST_MODE] Processing AAPL\n",
            "2025-12-26 00:44:37,024 - INFO - Will train AAPL | Window 1 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:44:48,713 - INFO - Training AAPL Window 1/3\n",
            "2025-12-26 00:47:22,129 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window1_predictions.csv\n",
            "2025-12-26 00:47:22,205 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window1_predictions_compat.csv\n",
            "2025-12-26 00:47:22,206 - INFO - AAPL | Window 1 runtime: 165.69s\n",
            "2025-12-26 00:47:23,014 - INFO - Will train AAPL | Window 2 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:47:23,024 - INFO - Training AAPL Window 2/3\n",
            "2025-12-26 00:49:38,624 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window2_predictions.csv\n",
            "2025-12-26 00:49:38,718 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window2_predictions_compat.csv\n",
            "2025-12-26 00:49:38,719 - INFO - AAPL | Window 2 runtime: 136.07s\n",
            "2025-12-26 00:49:39,501 - INFO - Will train AAPL | Window 3 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:49:39,509 - INFO - Training AAPL Window 3/3\n",
            "2025-12-26 00:51:55,863 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window3_predictions.csv\n",
            "2025-12-26 00:51:55,987 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window3_predictions_compat.csv\n",
            "2025-12-26 00:51:55,990 - INFO - AAPL | Window 3 runtime: 136.86s\n",
            "2025-12-26 00:51:56,549 - INFO - [QC SAVE] Saved QC artifacts for ppo_AAPL_window3\n",
            "2025-12-26 00:51:56,576 - INFO - [QC SAVE] Saved QC artifacts for ppo_AAPL_window2\n",
            "2025-12-26 00:51:56,603 - INFO - [QC SAVE] Saved QC artifacts for ppo_AAPL_window1\n",
            "2025-12-26 00:51:56,610 - INFO - AAPL: produced 3 window summaries\n",
            "2025-12-26 00:51:56,612 - INFO - >>> [TEST_MODE] Processing NVDA\n",
            "2025-12-26 00:51:57,183 - INFO - Will train NVDA | Window 1 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:51:57,193 - INFO - Training NVDA Window 1/3\n",
            "2025-12-26 00:54:20,438 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window1_predictions.csv\n",
            "2025-12-26 00:54:20,643 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window1_predictions_compat.csv\n",
            "2025-12-26 00:54:20,645 - INFO - NVDA | Window 1 runtime: 143.94s\n",
            "2025-12-26 00:54:22,250 - INFO - Will train NVDA | Window 2 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:54:22,263 - INFO - Training NVDA Window 2/3\n",
            "2025-12-26 00:56:48,511 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window2_predictions.csv\n",
            "2025-12-26 00:56:48,590 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window2_predictions_compat.csv\n",
            "2025-12-26 00:56:48,591 - INFO - NVDA | Window 2 runtime: 147.14s\n",
            "2025-12-26 00:56:49,383 - INFO - Will train NVDA | Window 3 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:56:49,391 - INFO - Training NVDA Window 3/3\n",
            "2025-12-26 00:59:11,463 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window3_predictions.csv\n",
            "2025-12-26 00:59:11,547 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window3_predictions_compat.csv\n",
            "2025-12-26 00:59:11,548 - INFO - NVDA | Window 3 runtime: 142.54s\n",
            "2025-12-26 00:59:12,044 - INFO - [QC SAVE] Saved QC artifacts for ppo_NVDA_window1\n",
            "2025-12-26 00:59:12,081 - INFO - [QC SAVE] Saved QC artifacts for ppo_NVDA_window2\n",
            "2025-12-26 00:59:12,120 - INFO - [QC SAVE] Saved QC artifacts for ppo_NVDA_window3\n",
            "2025-12-26 00:59:12,127 - INFO - NVDA: produced 3 window summaries\n",
            "2025-12-26 00:59:12,127 - INFO - >>> [TEST_MODE] Processing MSFT\n",
            "2025-12-26 00:59:12,587 - INFO - Will train MSFT | Window 1 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:59:12,597 - INFO - Training MSFT Window 1/3\n",
            "2025-12-26 01:01:29,748 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window1_predictions.csv\n",
            "2025-12-26 01:01:29,862 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window1_predictions_compat.csv\n",
            "2025-12-26 01:01:29,865 - INFO - MSFT | Window 1 runtime: 137.7s\n",
            "2025-12-26 01:01:30,841 - INFO - Will train MSFT | Window 2 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 01:01:30,852 - INFO - Training MSFT Window 2/3\n",
            "2025-12-26 01:03:53,747 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window2_predictions.csv\n",
            "2025-12-26 01:03:53,877 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window2_predictions_compat.csv\n",
            "2025-12-26 01:03:53,881 - INFO - MSFT | Window 2 runtime: 143.51s\n",
            "2025-12-26 01:03:55,045 - INFO - Will train MSFT | Window 3 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 01:03:55,057 - INFO - Training MSFT Window 3/3\n",
            "2025-12-26 01:06:09,439 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window3_predictions.csv\n",
            "2025-12-26 01:06:09,522 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window3_predictions_compat.csv\n",
            "2025-12-26 01:06:09,523 - INFO - MSFT | Window 3 runtime: 134.92s\n",
            "2025-12-26 01:06:09,985 - INFO - [QC SAVE] Saved QC artifacts for ppo_MSFT_window2\n",
            "2025-12-26 01:06:10,025 - INFO - [QC SAVE] Saved QC artifacts for ppo_MSFT_window1\n",
            "2025-12-26 01:06:10,086 - INFO - [QC SAVE] Saved QC artifacts for ppo_MSFT_window3\n",
            "2025-12-26 01:06:10,093 - INFO - MSFT: produced 3 window summaries\n",
            "2025-12-26 01:06:10,108 - INFO - Test-mode summary saved to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/summary_test_mode.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aggregated PPO selector saved to → /content/drive/MyDrive/Results_May_2025/ppo_model_selector_FULL.csv\n",
            "Final enhanced PPO selector JSON saved to → /content/drive/MyDrive/Results_May_2025/ppo_model_selector_final.json\n"
          ]
        }
      ],
      "source": [
        "# PPO walkforward training + selector\n",
        "import os, gc, time, json, logging, glob\n",
        "import shutil\n",
        "from threading import Lock\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt  # optional for ad-hoc plots\n",
        "\n",
        "import torch\n",
        "import gymnasium as gym\n",
        "from gymnasium.spaces import Box as GBox\n",
        "\n",
        "import yfinance as yf\n",
        "from gym_anytrading.envs import StocksEnv\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"gymnasium\")\n",
        "# ---- Sharpe annualization helper (intraday heuristic: 6.5 hrs * 252) ----\n",
        "def _annualization_factor(_df_like=None) -> float:\n",
        "    \"\"\"Annualization factor for intraday bars (6.5 trading hours × 252 days).\"\"\"\n",
        "    return np.sqrt(252 * 6.5)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"jupyter_client.session\")\n",
        "warnings.filterwarnings(\"ignore\", message=\".*Gym has been unmaintained.*\")\n",
        "\n",
        "try:\n",
        "    compute_enhanced_features  # type: ignore\n",
        "except NameError:\n",
        "    def compute_enhanced_features(df_in: pd.DataFrame) -> pd.DataFrame:\n",
        "        df_out = df_in.copy()\n",
        "        if \"Datetime\" in df_out.columns:\n",
        "            df_out[\"Datetime\"] = pd.to_datetime(df_out[\"Datetime\"])\n",
        "            df_out = df_out.sort_values(\"Datetime\").reset_index(drop=True)\n",
        "        if \"Close\" not in df_out.columns:\n",
        "            raise ValueError(\"compute_enhanced_features: missing required column 'Close'\")\n",
        "        return df_out\n",
        "\n",
        "set_random_seed(42)\n",
        "\n",
        "BASE_RESULTS_DIR = \"/content/drive/MyDrive/Results_May_2025\"\n",
        "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "RUN_RESULTS_DIR = os.path.join(BASE_RESULTS_DIR, f\"ppo_walkforward_results_{RUN_TAG}\")\n",
        "FINAL_MODEL_DIR = os.path.join(BASE_RESULTS_DIR, \"ppo_models_master\")\n",
        "QC_TOP_DIR      = os.path.join(BASE_RESULTS_DIR, \"ppo_models_QC_TOP\")\n",
        "\n",
        "os.makedirs(QC_TOP_DIR, exist_ok=True)\n",
        "os.makedirs(RUN_RESULTS_DIR, exist_ok=True)\n",
        "os.makedirs(FINAL_MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# Aggregated selector outputs\n",
        "SELECTOR_FULL_PATH = os.path.join(BASE_RESULTS_DIR, \"ppo_model_selector_FULL.csv\")\n",
        "SELECTOR_JSON_PATH = os.path.join(BASE_RESULTS_DIR, \"ppo_model_selector_final.json\")\n",
        "MODEL_NAME = \"PPO\"\n",
        "\n",
        "# Global skip aggregation (thread-safe)\n",
        "SKIP_AGG_PATH = os.path.join(RUN_RESULTS_DIR, \"skipped_windows_global.csv\")\n",
        "SKIP_LOCK = Lock()\n",
        "\n",
        "# Logging Setup\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    force=True\n",
        ")\n",
        "\n",
        "# Flags\n",
        "ENABLE_SENTIMENT = False\n",
        "ENABLE_SLO       = True\n",
        "ENABLE_WAVELET   = True\n",
        "test_mode        = True            # set False for full universe\n",
        "ENABLE_PLOTS     = False\n",
        "LIVE_MODE        = False           # set True to run simple live/paper loop\n",
        "SIM_LATENCY_MS   = 0               # broker latency simulation; 0 = off\n",
        "BROKER           = \"log\"           # \"log\" = do not place orders, just log\n",
        "\n",
        "# Global training settings\n",
        "WINDOW_SIZE = 3500\n",
        "STEP_SIZE   = 500\n",
        "TIMESTEPS   = 150_000  # overridden in test_mode block to smaller value\n",
        "\n",
        "\n",
        "DATA_PATH = \"multi_stock_feature_engineered_dataset.csv\"\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    raise FileNotFoundError(\"Required feature-engineered dataset not found!\")\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "df[\"Datetime\"] = pd.to_datetime(df[\"Datetime\"])\n",
        "\n",
        "# Wavelet fallback\n",
        "if ENABLE_WAVELET and \"Denoised_Close\" not in df.columns:\n",
        "    logging.warning(\"ENABLE_WAVELET=True but 'Denoised_Close' missing; \"\n",
        "                    \"falling back to Close->Denoised_Close.\")\n",
        "    df[\"Denoised_Close\"] = df[\"Close\"]\n",
        "\n",
        "\n",
        "def record_skips_global(ticker: str, skipped_windows: list,\n",
        "                        total_windows: int = None, fully_skipped: bool = False):\n",
        "    \"\"\"Append skipped windows to the global skip log.\"\"\"\n",
        "    if not skipped_windows and not fully_skipped:\n",
        "        return\n",
        "    import csv\n",
        "    with SKIP_LOCK:\n",
        "        new_file = not os.path.exists(SKIP_AGG_PATH)\n",
        "        with open(SKIP_AGG_PATH, \"a\", newline=\"\") as f:\n",
        "            w = csv.writer(f)\n",
        "            if new_file:\n",
        "                w.writerow([\"Ticker\", \"Window\", \"FullySkipped\", \"TotalWindows\"])\n",
        "            if fully_skipped:\n",
        "                w.writerow([ticker, \"ALL\", True, total_windows if total_windows is not None else \"\"])\n",
        "            else:\n",
        "                for wname in skipped_windows:\n",
        "                    try:\n",
        "                        _, win_str = wname.split(\"_window\")\n",
        "                        win = int(win_str)\n",
        "                    except Exception:\n",
        "                        win = \"\"\n",
        "                    w.writerow([ticker, win, False, total_windows if total_windows is not None else \"\"])\n",
        "\n",
        "\n",
        "ENV_KWARGS = dict(\n",
        "    window_size=10,\n",
        "    cost_rate=0.0002,\n",
        "    slip_rate=0.0003,\n",
        "\n",
        "    k_alpha=0.0,\n",
        "    k_mom=0.15,\n",
        "    k_sent=(0.01 if ENABLE_SENTIMENT else 0.0),\n",
        "    mom_source=\"denoised\",\n",
        "    mom_lookback=20,\n",
        "\n",
        "    min_trade_delta=0.08,\n",
        "    cooldown=10,\n",
        "\n",
        "    reward_clip=0.05,\n",
        "    k_vol=0.00,\n",
        "    k_dd=0.00,\n",
        ")\n",
        "\n",
        "\n",
        "class ContinuousPositionEnv(StocksEnv):\n",
        "    def __init__(self, df, frame_bound, **kwargs):\n",
        "        # Require window_size from ENV_KWARGS\n",
        "        if \"window_size\" not in kwargs:\n",
        "            raise ValueError(\"ContinuousPositionEnv requires window_size (pass via ENV_KWARGS).\")\n",
        "\n",
        "        window_size = int(kwargs.pop(\"window_size\"))\n",
        "\n",
        "        # Pull params (all defaults live in ENV_KWARGS; these are just safety fallbacks)\n",
        "        cost_rate       = float(kwargs.pop(\"cost_rate\", 0.0002))\n",
        "        slip_rate       = float(kwargs.pop(\"slip_rate\", 0.0003))\n",
        "        k_alpha         = float(kwargs.pop(\"k_alpha\", 0.0))\n",
        "        k_mom           = float(kwargs.pop(\"k_mom\", 0.15))\n",
        "        k_sent          = float(kwargs.pop(\"k_sent\", 0.0))\n",
        "        mom_source      = str(kwargs.pop(\"mom_source\", \"denoised\"))\n",
        "        mom_lookback    = int(kwargs.pop(\"mom_lookback\", 20))\n",
        "        min_trade_delta = float(kwargs.pop(\"min_trade_delta\", 0.04))\n",
        "        cooldown        = int(kwargs.pop(\"cooldown\", 6))\n",
        "        reward_clip     = float(kwargs.pop(\"reward_clip\", 0.05))\n",
        "        k_vol           = float(kwargs.pop(\"k_vol\", 0.0))\n",
        "        k_dd            = float(kwargs.pop(\"k_dd\", 0.0))\n",
        "\n",
        "        # Fail fast on unexpected env kwargs\n",
        "        if kwargs:\n",
        "            raise ValueError(f\"Unexpected env kwargs: {list(kwargs.keys())}\")\n",
        "\n",
        "        super().__init__(\n",
        "            df=df.reset_index(drop=True),\n",
        "            frame_bound=frame_bound,\n",
        "            window_size=window_size\n",
        "        )\n",
        "\n",
        "        if isinstance(self.observation_space, gym.spaces.Box):\n",
        "            self.observation_space = GBox(\n",
        "                low=self.observation_space.low,\n",
        "                high=self.observation_space.high,\n",
        "                shape=self.observation_space.shape,\n",
        "                dtype=self.observation_space.dtype,\n",
        "            )\n",
        "\n",
        "        self.k_vol = k_vol\n",
        "        self.k_dd  = k_dd\n",
        "\n",
        "        self.ret_history = []\n",
        "        self.nav_history = []\n",
        "        self.peak_nav    = 1.0\n",
        "        self.trade_count = 0\n",
        "\n",
        "        self.action_space = GBox(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)\n",
        "\n",
        "\n",
        "        self.cost_rate       = cost_rate\n",
        "        self.slip_rate       = slip_rate\n",
        "        self.k_alpha         = k_alpha\n",
        "        self.k_mom           = k_mom\n",
        "        self.k_sent          = k_sent\n",
        "        self.mom_source      = mom_source\n",
        "        self.mom_lookback    = mom_lookback\n",
        "        self.min_trade_delta = min_trade_delta\n",
        "        self.cooldown        = cooldown\n",
        "        self.reward_clip     = reward_clip\n",
        "\n",
        "        self.nav = 1.0\n",
        "        self.pos = 0.0\n",
        "        self._last_trade_step = -self.cooldown\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        out = super().reset(**kwargs)\n",
        "        if isinstance(out, tuple):\n",
        "            obs, info = out\n",
        "        else:\n",
        "            obs, info = out, {}\n",
        "\n",
        "        self.nav = 1.0\n",
        "        self.pos = 0.0\n",
        "        self._last_trade_step = -self.cooldown\n",
        "\n",
        "        self.trade_count = 0\n",
        "        self.ret_history = []\n",
        "        self.nav_history = [self.nav]\n",
        "        self.peak_nav    = self.nav\n",
        "\n",
        "        info = info or {}\n",
        "        info.update({\n",
        "            \"nav\": self.nav,\n",
        "            \"pos\": self.pos,\n",
        "            \"trade_count\": int(self.trade_count),\n",
        "\n",
        "        })\n",
        "        return obs, info\n",
        "\n",
        "    def _step_parent_hold(self):\n",
        "        step_result = super().step(2)\n",
        "        if len(step_result) == 5:\n",
        "            obs, _env_rew, terminated, truncated, info = step_result\n",
        "        else:\n",
        "            obs, _env_rew, done, info = step_result\n",
        "            terminated, truncated = bool(done), False\n",
        "        return obs, terminated, truncated, info\n",
        "\n",
        "    def _ret_t(self):\n",
        "        cur  = float(self.df.loc[self._current_tick, \"Close\"])\n",
        "        prev = float(self.df.loc[max(self._current_tick - 1, 0), \"Close\"])\n",
        "        return 0.0 if prev <= 0 else (cur - prev) / prev\n",
        "\n",
        "    def _mom_signal(self):\n",
        "        if self.mom_source == \"macd\" and \"MACD_Line\" in self.df.columns:\n",
        "            recent = self.df[\"MACD_Line\"].iloc[max(self._current_tick - 200, 0):self._current_tick + 1]\n",
        "            return float(np.tanh(\n",
        "                float(self.df.loc[self._current_tick, \"MACD_Line\"]) /\n",
        "                (1e-6 + float(recent.std()))\n",
        "            ))\n",
        "\n",
        "        if \"Denoised_Close\" in self.df.columns and self._current_tick - self.mom_lookback >= 0:\n",
        "            now  = float(self.df.loc[self._current_tick, \"Denoised_Close\"])\n",
        "            then = float(self.df.loc[self._current_tick - self.mom_lookback, \"Denoised_Close\"])\n",
        "            base = float(self.df.loc[max(self._current_tick - 1, 0), \"Close\"])\n",
        "            slope = (now - then) / max(self.mom_lookback, 1)\n",
        "            return float(np.tanh(10.0 * (slope / max(abs(base), 1e-6))))\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "    def step(self, action):\n",
        "        a = float(np.array(action).squeeze())\n",
        "        target_pos = float(np.clip(a, -1.0, 1.0))\n",
        "\n",
        "        r_t = self._ret_t()\n",
        "        base_ret = self.pos * r_t\n",
        "\n",
        "        changed = (\n",
        "            abs(target_pos - self.pos) >= self.min_trade_delta\n",
        "        ) and (\n",
        "            (self._current_tick - self._last_trade_step) >= self.cooldown\n",
        "        )\n",
        "\n",
        "        delta_pos = (target_pos - self.pos) if changed else 0.0\n",
        "        trade_cost = (self.cost_rate + self.slip_rate) * abs(delta_pos)\n",
        "\n",
        "        rel_alpha = base_ret - r_t\n",
        "        mom_term = self.pos * self._mom_signal()\n",
        "\n",
        "        alpha_term = self.k_alpha * rel_alpha\n",
        "\n",
        "        sent_term = 0.0\n",
        "        if ENABLE_SENTIMENT and \"SentimentScore\" in self.df.columns:\n",
        "            sent_term = self.k_sent * float(self.df.loc[self._current_tick, \"SentimentScore\"])\n",
        "\n",
        "        shaped = base_ret + alpha_term + (self.k_mom * mom_term) + sent_term - trade_cost\n",
        "        reward = float(np.clip(shaped, -self.reward_clip, self.reward_clip))\n",
        "\n",
        "\n",
        "        self.nav *= (1.0 + base_ret - trade_cost)\n",
        "        self.nav_history.append(self.nav)\n",
        "        self.peak_nav = max(self.peak_nav, self.nav)\n",
        "\n",
        "        executed_trade = False\n",
        "        if changed:\n",
        "            self.pos = target_pos\n",
        "            self._last_trade_step = self._current_tick\n",
        "            self.trade_count += 1\n",
        "            executed_trade = True\n",
        "\n",
        "        obs, terminated, truncated, info = self._step_parent_hold()\n",
        "        info = info or {}\n",
        "        info.update({\n",
        "            \"ret_t\": r_t,\n",
        "            \"nav\": self.nav,\n",
        "            \"pos\": self.pos,\n",
        "            \"trade_cost\": trade_cost,\n",
        "            \"base_ret\": base_ret,\n",
        "            \"rel_alpha\": rel_alpha,\n",
        "            \"mom\": mom_term,\n",
        "            \"changed\": bool(changed),\n",
        "            \"executed_trade\": bool(executed_trade),\n",
        "            \"trade_count\": int(self.trade_count),\n",
        "            \"delta_pos\": float(delta_pos),\n",
        "        })\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "def get_mu_sigma(model, obs):\n",
        "    \"\"\"SB3 v2-safe way to get Gaussian policy mean/std for continuous actions.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        obs_t, _ = model.policy.obs_to_tensor(obs)\n",
        "        features = model.policy.extract_features(obs_t)\n",
        "        latent_pi, _ = model.policy.mlp_extractor(features)\n",
        "        mean_actions = model.policy.action_net(latent_pi)\n",
        "        log_std = model.policy.log_std\n",
        "        mu = float(mean_actions.detach().cpu().numpy().squeeze())\n",
        "        sigma = float(log_std.exp().detach().cpu().numpy().squeeze())\n",
        "    return mu, sigma\n",
        "\n",
        "def get_walk_forward_windows(df_in, window_size=3500, step_size=500, min_len=1200):\n",
        "    return [\n",
        "        (start, start + window_size)\n",
        "        for start in range(0, len(df_in) - min_len, step_size)\n",
        "        if start + window_size < len(df_in)\n",
        "    ]\n",
        "\n",
        "def save_quantconnect_model(artifact, prefix, save_dir):\n",
        "    \"\"\"Save/copy QC-compatible artifacts into save_dir.\"\"\"\n",
        "    import shutil\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # --- Model zip: save or copy ---\n",
        "    model_dst = os.path.join(save_dir, f\"{prefix}_model.zip\")\n",
        "\n",
        "    model_obj = artifact.get(\"model\", None)\n",
        "    model_src = artifact.get(\"model_path\", None)\n",
        "\n",
        "    try:\n",
        "        if model_obj is not None:\n",
        "            # Save from in-memory SB3 model\n",
        "            if not os.path.exists(model_dst):\n",
        "                model_obj.save(model_dst)\n",
        "\n",
        "        else:\n",
        "            # Copy from an existing trained window model zip\n",
        "            if model_src and os.path.exists(model_src):\n",
        "                if os.path.abspath(model_src) != os.path.abspath(model_dst):\n",
        "                    shutil.copyfile(model_src, model_dst)\n",
        "            else:\n",
        "                # If neither provided, warn loudly\n",
        "                if not os.path.exists(model_dst):\n",
        "                    logging.warning(f\"[QC SAVE] Missing model for {prefix}: no model_obj and no valid model_path.\")\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[QC SAVE] Model handling issue for {prefix}: {e}\")\n",
        "\n",
        "    # --- VecNormalize: copy ---\n",
        "    vecnorm_src = artifact.get(\"vecnorm_path\")\n",
        "    if vecnorm_src and os.path.exists(vecnorm_src):\n",
        "        try:\n",
        "            vecnorm_dst = os.path.join(save_dir, f\"{prefix}_vecnorm.pkl\")\n",
        "            if os.path.abspath(vecnorm_src) != os.path.abspath(vecnorm_dst):\n",
        "                shutil.copyfile(vecnorm_src, vecnorm_dst)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"[QC SAVE] VecNormalize handling issue for {prefix}: {e}\")\n",
        "    else:\n",
        "        logging.warning(f\"[QC SAVE] VecNormalize missing for {prefix}: vecnorm_path not found.\")\n",
        "\n",
        "    # --- Features ---\n",
        "    try:\n",
        "        with open(os.path.join(save_dir, f\"{prefix}_features.json\"), \"w\") as f:\n",
        "            json.dump({\"features\": artifact.get(\"features\", [])}, f)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[QC SAVE] Could not write features.json for {prefix}: {e}\")\n",
        "\n",
        "    # --- Probability config ---\n",
        "    try:\n",
        "        thr = 0.2\n",
        "        try:\n",
        "            thr = float(artifact.get(\"result\", {}).get(\"Action_Threshold\", 0.2))\n",
        "        except Exception:\n",
        "            thr = 0.2\n",
        "\n",
        "        with open(os.path.join(save_dir, f\"{prefix}_probability_config.json\"), \"w\") as f:\n",
        "            json.dump(\n",
        "                {\"threshold\": thr, \"use_confidence\": True, \"inference_mode\": \"deterministic\"},\n",
        "                f\n",
        "            )\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[QC SAVE] Could not write probability_config.json for {prefix}: {e}\")\n",
        "\n",
        "\n",
        "    # --- Model info ---\n",
        "    try:\n",
        "        r = artifact.get(\"result\", {})\n",
        "        with open(os.path.join(save_dir, f\"{prefix}_model_info.json\"), \"w\") as f:\n",
        "            json.dump({\n",
        "                \"model\": \"PPO\",\n",
        "                \"ticker\": r.get(\"Ticker\"),\n",
        "                \"window\": r.get(\"Window\"),\n",
        "                \"date_trained\": datetime.today().strftime(\"%Y-%m-%d\"),\n",
        "                \"framework\": \"stable-baselines3\",\n",
        "                \"input_features\": artifact.get(\"features\", []),\n",
        "                \"final_portfolio\": r.get(\"PPO_Portfolio\"),\n",
        "                \"buy_hold\": r.get(\"BuyHold\"),\n",
        "                \"sharpe\": r.get(\"Sharpe\"),\n",
        "            }, f)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[QC SAVE] Could not write model_info.json for {prefix}: {e}\")\n",
        "\n",
        "    logging.info(f\"[QC SAVE] Saved QC artifacts for {prefix}\")\n",
        "\n",
        "def load_model_and_env(prefix):\n",
        "    \"\"\"Load a trained PPO and create a factory to build a matching env window.\"\"\"\n",
        "    model_path = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_model.zip\")\n",
        "    vec_path   = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_vecnorm.pkl\")\n",
        "    model = PPO.load(model_path, device=\"cpu\")\n",
        "\n",
        "    def make_env(df_window):\n",
        "        frame_bound = (50, len(df_window) - 3)\n",
        "        e = DummyVecEnv([lambda: ContinuousPositionEnv(\n",
        "            df=df_window, frame_bound=frame_bound, **ENV_KWARGS\n",
        "        )])\n",
        "        if os.path.exists(vec_path):\n",
        "            e = VecNormalize.load(vec_path, e)\n",
        "        e.training = False\n",
        "        e.norm_reward = False\n",
        "        return e\n",
        "\n",
        "    return model, make_env\n",
        "\n",
        "def latest_df_for_symbol(symbol, horizon_days=5, interval=\"1m\"):\n",
        "    \"\"\"Fetch fresh bars and rebuild features exactly like training.\"\"\"\n",
        "    end = datetime.utcnow()\n",
        "    start = end - timedelta(days=horizon_days)\n",
        "    df_live = yf.download(\n",
        "        symbol,\n",
        "        start=start.strftime(\"%Y-%m-%d\"),\n",
        "        end=end.strftime(\"%Y-%m-%d\"),\n",
        "        interval=interval,\n",
        "        progress=False,\n",
        "        auto_adjust=False,\n",
        "    )\n",
        "    if df_live is None or df_live.empty:\n",
        "        return None\n",
        "    df_live = df_live.reset_index()\n",
        "    df_live[\"Symbol\"] = symbol\n",
        "    df_live = compute_enhanced_features(df_live)\n",
        "    if ENABLE_WAVELET and \"Denoised_Close\" not in df_live.columns:\n",
        "        df_live[\"Denoised_Close\"] = df_live[\"Close\"]\n",
        "    return df_live\n",
        "\n",
        "def predict_latest(symbol, prefix):\n",
        "    \"\"\"Build last window, fast-forward env, call model.predict(), return a signal.\"\"\"\n",
        "    # --- load per-model threshold ---\n",
        "    cfg_path = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_probability_config.json\")\n",
        "    thr = 0.2\n",
        "    if os.path.exists(cfg_path):\n",
        "        try:\n",
        "            with open(cfg_path, \"r\") as f:\n",
        "                thr = float(json.load(f).get(\"threshold\", 0.2))\n",
        "        except Exception:\n",
        "            thr = 0.2\n",
        "\n",
        "    model, make_env = load_model_and_env(prefix)\n",
        "    live_df = latest_df_for_symbol(symbol)\n",
        "    if live_df is None or len(live_df) < 100:\n",
        "        logging.warning(\"No fresh data yet for live inference.\")\n",
        "        return None\n",
        "\n",
        "    df_window = live_df.iloc[-2500:].reset_index(drop=True) if len(live_df) > 2500 else live_df.copy()\n",
        "\n",
        "    env = make_env(df_window)\n",
        "    obs = env.reset()\n",
        "    if isinstance(obs, tuple):\n",
        "        obs, _ = obs\n",
        "\n",
        "    # fast-forward with HOLD\n",
        "    for _ in range(len(df_window) - 1):\n",
        "        obs, _, dones, _ = env.step([np.array([0.0], dtype=np.float32)])\n",
        "        if isinstance(dones, (np.ndarray, list, tuple)) and len(dones) and dones[0]:\n",
        "            break\n",
        "\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    mu, sigma = get_mu_sigma(model, obs)\n",
        "\n",
        "    a = float(np.array(action).squeeze())\n",
        "\n",
        "    # --- thresholded signal using loaded thr ---\n",
        "    if a > thr:\n",
        "        signal = \"BUY\"\n",
        "    elif a < -thr:\n",
        "        signal = \"SELL\"\n",
        "    else:\n",
        "        signal = \"HOLD\"\n",
        "\n",
        "    conf = abs(a)\n",
        "    ts = df_window[\"Datetime\"].iloc[-1] if \"Datetime\" in df_window.columns else None\n",
        "    price = float(df_window[\"Close\"].iloc[-1])\n",
        "\n",
        "    return dict(\n",
        "        signal=signal,\n",
        "        confidence=conf,\n",
        "        action=a,\n",
        "        threshold=thr,\n",
        "        ts=ts,\n",
        "        price=price,\n",
        "        mu=mu,\n",
        "        sigma=sigma,\n",
        "    )\n",
        "\n",
        "def place_order(signal, qty=1):\n",
        "    \"\"\"Stub broker router with latency simulation; logs in Colab.\"\"\"\n",
        "    if SIM_LATENCY_MS > 0:\n",
        "        time.sleep(SIM_LATENCY_MS / 1000.0)\n",
        "    if BROKER == \"log\":\n",
        "        logging.info(f\"[PAPER] {signal} x{qty}\")\n",
        "    else:\n",
        "        logging.info(f\"[BROKER={BROKER}] {signal} x{qty} (not implemented)\")\n",
        "\n",
        "def live_loop(symbol, best_prefix):\n",
        "    \"\"\"Simple polling loop—set LIVE_MODE=True to run.\"\"\"\n",
        "    while LIVE_MODE:\n",
        "        try:\n",
        "            pred = predict_latest(symbol, best_prefix)\n",
        "            if pred:\n",
        "                logging.info(\n",
        "                    f\"{symbol} {pred['ts']} | {pred['signal']} \"\n",
        "                    f\"@ {pred['price']:.2f} (conf {pred['confidence']:.2f})\"\n",
        "                )\n",
        "                place_order(pred[\"signal\"], qty=1)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Live loop error: {e}\")\n",
        "        time.sleep(60)  # Poll each minute\n",
        "\n",
        "TOP_N_WINDOWS = 3\n",
        "\n",
        "FAST = {\n",
        "    \"lr\": 8e-5,\n",
        "    \"n_steps\": 3072,\n",
        "    \"batch\": 512,\n",
        "    \"clip\": 0.2,\n",
        "    \"ent\": 0.01,\n",
        "}\n",
        "\n",
        "SLOW = {\n",
        "    \"lr\": 3e-5,\n",
        "    \"n_steps\": 3072,\n",
        "    \"batch\": 512,\n",
        "    \"clip\": 0.16,\n",
        "    \"ent\": 0.005,\n",
        "}\n",
        "\n",
        "fast_names = {\n",
        "    \"TSLA\",\"NVDA\",\"AMD\",\"AVGO\",\"AAPL\",\"MSFT\",\"AMZN\",\"GOOGL\",\"META\",\"ADBE\",\"CRM\",\n",
        "    \"INTC\",\"QCOM\",\"TXN\",\"ORCL\",\"NEE\",\"GE\",\"XOM\",\"CVX\",\"LLY\",\"NKE\",\"SBUX\"\n",
        "}\n",
        "slow_names = {\n",
        "    \"BRK-B\",\"JPM\",\"BAC\",\"JNJ\",\"UNH\",\"MRK\",\"PFE\",\"ABBV\",\"ABT\",\"AMGN\",\"PG\",\"PEP\",\"KO\",\n",
        "    \"V\",\"MA\",\"WMT\",\"MCD\",\"TMO\",\"DHR\",\"ACN\",\"IBM\",\"LIN\",\"PM\",\"RTX\",\"UPS\",\"UNP\",\"COST\",\"HD\",\"LOW\"\n",
        "}\n",
        "\n",
        "def pick_params(symbol: str):\n",
        "    return FAST if symbol in fast_names else SLOW\n",
        "\n",
        "def export_qc_top_from_existing(ticker: str, top_n: int = 3):\n",
        "    \"\"\"\n",
        "    If a ticker is fully skipped (models already exist), still populate QC_TOP_DIR.\n",
        "    Uses existing summary CSVs to pick top Sharpe windows, then copies artifacts from FINAL_MODEL_DIR.\n",
        "    Prefers using 'Prefix' from summaries (robust). Falls back to WindowIdx reconstruction.\n",
        "    \"\"\"\n",
        "    summary_files = glob.glob(os.path.join(BASE_RESULTS_DIR, \"ppo_walkforward_results_*\", \"summary*.csv\"))\n",
        "    if not summary_files:\n",
        "        logging.warning(f\"[QC_TOP] No summary files found; cannot export QC_TOP for {ticker}.\")\n",
        "        return\n",
        "\n",
        "    frames = []\n",
        "    for p in summary_files:\n",
        "        try:\n",
        "            tmp = pd.read_csv(p)\n",
        "            frames.append(tmp)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"[QC_TOP] Could not read {p}: {e}\")\n",
        "\n",
        "    if not frames:\n",
        "        logging.warning(f\"[QC_TOP] Could not read any summary files; cannot export QC_TOP for {ticker}.\")\n",
        "        return\n",
        "\n",
        "    combo = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "    if \"Ticker\" not in combo.columns:\n",
        "        logging.warning(\"[QC_TOP] Summary files missing 'Ticker' column; cannot export.\")\n",
        "        return\n",
        "\n",
        "    combo = combo[combo[\"Ticker\"] == ticker].copy()\n",
        "    if combo.empty or \"Sharpe\" not in combo.columns:\n",
        "        logging.warning(f\"[QC_TOP] No rows for {ticker} in summaries (or missing Sharpe); cannot export QC_TOP.\")\n",
        "        return\n",
        "\n",
        "    # Ensure Sharpe is numeric so sorting works reliably\n",
        "    combo[\"Sharpe\"] = pd.to_numeric(combo[\"Sharpe\"], errors=\"coerce\")\n",
        "    combo = combo.dropna(subset=[\"Sharpe\"])\n",
        "    if combo.empty:\n",
        "        logging.warning(f\"[QC_TOP] All Sharpe values were non-numeric for {ticker}; cannot export.\")\n",
        "        return\n",
        "\n",
        "    use_prefix = (\"Prefix\" in combo.columns) and combo[\"Prefix\"].notna().any()\n",
        "\n",
        "    if use_prefix:\n",
        "        # Robust path: use saved Prefix directly\n",
        "        top = combo.sort_values(\"Sharpe\", ascending=False).head(top_n).copy()\n",
        "        top[\"__prefix__\"] = top[\"Prefix\"].astype(str)\n",
        "    else:\n",
        "        # Fallback: reconstruct WindowIdx (less robust)\n",
        "        def _window_start(w):\n",
        "            try:\n",
        "                s = str(w)\n",
        "                return int(s.split(\"-\")[0]) if \"-\" in s else np.nan\n",
        "            except Exception:\n",
        "                return np.nan\n",
        "\n",
        "        combo[\"WindowStart\"] = combo[\"Window\"].apply(_window_start)\n",
        "        combo = combo.sort_values([\"WindowStart\"]).reset_index(drop=True)\n",
        "        combo[\"WindowIdx\"] = combo.groupby(\"Ticker\").cumcount() + 1\n",
        "\n",
        "        top = combo.sort_values(\"Sharpe\", ascending=False).head(top_n).copy()\n",
        "        top[\"__prefix__\"] = top[\"WindowIdx\"].apply(lambda widx: f\"ppo_{ticker}_window{int(widx)}\")\n",
        "\n",
        "    exported = 0\n",
        "\n",
        "    for _, r in top.iterrows():\n",
        "        prefix = str(r[\"__prefix__\"])\n",
        "\n",
        "        model_path = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_model.zip\")\n",
        "        vec_path   = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_vecnorm.pkl\")\n",
        "\n",
        "        if not (os.path.exists(model_path) and os.path.exists(vec_path)):\n",
        "            logging.warning(f\"[QC_TOP] Missing model/vecnorm for {prefix}; cannot export.\")\n",
        "            continue\n",
        "\n",
        "        artifact_for_save = {\n",
        "            \"model\": None,\n",
        "            \"model_path\": model_path,\n",
        "            \"vecnorm_path\": vec_path,\n",
        "            \"features\": [],         # ok if unknown; QC can load features elsewhere\n",
        "            \"result\": r.to_dict(),  # includes Sharpe, Action_Threshold, etc if present\n",
        "            \"prefix\": prefix,\n",
        "        }\n",
        "        save_quantconnect_model(artifact_for_save, prefix, QC_TOP_DIR)\n",
        "        exported += 1\n",
        "\n",
        "    logging.info(f\"[QC_TOP] Exported {exported}/{len(top)} QC artifacts for {ticker}.\")\n",
        "\n",
        "def walkforward_ppo(df_sym, ticker,\n",
        "                    window_size=3500, step_size=500,\n",
        "                    timesteps=150_000, learning_rate=1e-4,\n",
        "                    ppo_overrides=None):\n",
        "    import heapq\n",
        "\n",
        "    if ppo_overrides is None:\n",
        "        ppo_overrides = {}\n",
        "\n",
        "    if len(df_sym) < window_size:\n",
        "        logging.warning(\n",
        "            f\"Skipping {ticker}: only {len(df_sym)} rows (min required: {window_size})\"\n",
        "        )\n",
        "        return []\n",
        "\n",
        "    results = []\n",
        "    windows = get_walk_forward_windows(df_sym, window_size, step_size)\n",
        "    top_heap = []\n",
        "    skipped_windows = []\n",
        "\n",
        "    # quick check: all windows already have model+vecnorm?\n",
        "    all_done = True\n",
        "    for idx in range(len(windows)):\n",
        "        prefix = f\"ppo_{ticker}_window{idx+1}\"\n",
        "        model_ok   = os.path.exists(os.path.join(FINAL_MODEL_DIR, f\"{prefix}_model.zip\"))\n",
        "        vecnorm_ok = os.path.exists(os.path.join(FINAL_MODEL_DIR, f\"{prefix}_vecnorm.pkl\"))\n",
        "        if not (model_ok and vecnorm_ok):\n",
        "            all_done = False\n",
        "            break\n",
        "\n",
        "    if all_done:\n",
        "        logging.info(f\"Ticker {ticker} fully skipped (all {len(windows)} windows already complete).\")\n",
        "        record_skips_global(ticker, skipped_windows=[], total_windows=len(windows), fully_skipped=True)\n",
        "\n",
        "        export_qc_top_from_existing(ticker, top_n=TOP_N_WINDOWS)\n",
        "        return []\n",
        "\n",
        "\n",
        "\n",
        "    for w_idx, (start, end) in enumerate(windows):\n",
        "        window_start_time = time.time()\n",
        "        gc.collect()\n",
        "\n",
        "        prefix = f\"ppo_{ticker}_window{w_idx+1}\"\n",
        "        model_path   = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_model.zip\")\n",
        "        vecnorm_path = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_vecnorm.pkl\")\n",
        "\n",
        "        if os.path.exists(model_path) and os.path.exists(vecnorm_path):\n",
        "            logging.info(f\"Skipping {ticker} | Window {w_idx+1}, already trained.\")\n",
        "            skipped_windows.append(f\"{ticker}_window{w_idx+1}\")\n",
        "            continue\n",
        "\n",
        "        missing = []\n",
        "        if not os.path.exists(model_path):   missing.append(\"model.zip\")\n",
        "        if not os.path.exists(vecnorm_path): missing.append(\"vecnorm.pkl\")\n",
        "        logging.info(\n",
        "            f\"Will train {ticker} | Window {w_idx+1} because missing: {', '.join(missing)}\"\n",
        "        )\n",
        "\n",
        "        df_window = df_sym.iloc[start:end].reset_index(drop=True)\n",
        "        if len(df_window) <= 52 or len(df_window) % 2 != 0:\n",
        "            df_window = df_window.iloc[:-1]\n",
        "\n",
        "        frame_bound = (50, len(df_window) - 3)\n",
        "\n",
        "        env = DummyVecEnv([lambda: ContinuousPositionEnv(\n",
        "          df=df_window, frame_bound=frame_bound, **ENV_KWARGS\n",
        "        )])\n",
        "\n",
        "        env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
        "\n",
        "        try:\n",
        "            model = PPO(\n",
        "                \"MlpPolicy\",\n",
        "                env,\n",
        "                verbose=0,\n",
        "                device=(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "                learning_rate=ppo_overrides.get(\"lr\", learning_rate),\n",
        "                n_steps=ppo_overrides.get(\"n_steps\", 256),\n",
        "                batch_size=ppo_overrides.get(\"batch\", 64),\n",
        "                n_epochs=5,\n",
        "                gamma=0.99,\n",
        "                gae_lambda=0.95,\n",
        "                clip_range=ppo_overrides.get(\"clip\", 0.2),\n",
        "                ent_coef=ppo_overrides.get(\"ent\", 0.005),\n",
        "                policy_kwargs=dict(net_arch=[64, 64]),\n",
        "            )\n",
        "\n",
        "            logging.info(f\"Training {ticker} Window {w_idx+1}/{len(windows)}\")\n",
        "            model.learn(total_timesteps=timesteps)\n",
        "\n",
        "            # Evaluation pass\n",
        "            env.training = False\n",
        "            env.norm_reward = False\n",
        "            obs = env.reset()\n",
        "            if isinstance(obs, tuple):\n",
        "                obs, _ = obs\n",
        "\n",
        "            nav_track = [1.0]\n",
        "            bh_track  = [1.0]\n",
        "            step_log  = []\n",
        "            executed_trade_count = 0\n",
        "            signal_trade_count   = 0\n",
        "            signal_trade_count_dyn = 0   # dynamic-threshold diagnostic\n",
        "\n",
        "            DIAG_THR = 0.2  # diagnostic-only fixed threshold\n",
        "\n",
        "            for i in range(len(df_window) - 1):\n",
        "                action, _ = model.predict(obs, deterministic=True)\n",
        "                mu, sigma = get_mu_sigma(model, obs)\n",
        "\n",
        "                obs, rew, dones, infos = env.step(action)\n",
        "\n",
        "                if isinstance(infos, (list, tuple)):\n",
        "                    info = infos[0] if len(infos) else {}\n",
        "                elif isinstance(infos, dict):\n",
        "                    info = infos\n",
        "                else:\n",
        "                    info = {}\n",
        "\n",
        "                nav_track.append(float(info.get(\"nav\", nav_track[-1])))\n",
        "                bh_track.append(bh_track[-1] * (1.0 + float(info.get(\"ret_t\", 0.0))))\n",
        "\n",
        "                a = float(np.array(action).squeeze())\n",
        "                dt_val = df_window[\"Datetime\"].iloc[i+1] if \"Datetime\" in df_window.columns else None\n",
        "                px     = float(df_window[\"Close\"].iloc[i+1]) if \"Close\" in df_window.columns else np.nan\n",
        "\n",
        "                # “signal” trades (fixed threshold) — diagnostic only\n",
        "                if a > DIAG_THR or a < -DIAG_THR:\n",
        "                    signal_trade_count += 1\n",
        "\n",
        "                # real trades executed by env friction logic\n",
        "                if bool(info.get(\"executed_trade\", False)):\n",
        "                    executed_trade_count += 1\n",
        "\n",
        "                # next-bar return (to score BUY/SELL vs the *next* move)\n",
        "                if i + 2 < len(df_window):\n",
        "                    p0 = float(df_window[\"Close\"].iloc[i+1])\n",
        "                    p1 = float(df_window[\"Close\"].iloc[i+2])\n",
        "                    next_ret = 0.0 if p0 <= 0 else (p1 - p0) / p0\n",
        "                else:\n",
        "                    next_ret = 0.0\n",
        "\n",
        "                rew_val = float(rew[0]) if isinstance(rew, (list, tuple, np.ndarray)) else float(rew)\n",
        "\n",
        "                step_log.append({\n",
        "                    \"Index\": i+1,\n",
        "                    \"Datetime\": dt_val,\n",
        "                    \"Close\": px,\n",
        "                    \"Action\": a,\n",
        "                    \"mu\": mu,\n",
        "                    \"sigma\": sigma,\n",
        "                    \"nav\": nav_track[-1],\n",
        "                    \"ret_t\": float(info.get(\"ret_t\", 0.0)),\n",
        "                    \"next_ret\": float(next_ret),\n",
        "                    \"reward\": rew_val,\n",
        "                    \"pos\": float(info.get(\"pos\", 0.0)),\n",
        "                    \"trade_cost\": float(info.get(\"trade_cost\", 0.0)),\n",
        "                    \"base_ret\": float(info.get(\"base_ret\", 0.0)),\n",
        "                    \"rel_alpha\": float(info.get(\"rel_alpha\", 0.0)),\n",
        "                    \"mom\": float(info.get(\"mom\", 0.0)),\n",
        "                })\n",
        "\n",
        "                # done handling (VecEnv)\n",
        "                if isinstance(dones, (np.ndarray, list, tuple)):\n",
        "                    if dones[0]:\n",
        "                        break\n",
        "                elif dones:\n",
        "                    break\n",
        "\n",
        "\n",
        "            # --- Metrics ---\n",
        "            final_value = float(nav_track[-1]) * 100_000.0\n",
        "            hold_value  = float(bh_track[-1])  * 100_000.0\n",
        "\n",
        "            #dynamic action threshold for this window (prevents “no signals” windows)\n",
        "            abs_actions = np.array([abs(float(r[\"Action\"])) for r in step_log], dtype=float)\n",
        "            if len(abs_actions) > 0:\n",
        "                thr = float(np.quantile(abs_actions, 0.70))  # 70th percentile\n",
        "                thr = float(np.clip(thr, 0.08, 0.30))\n",
        "            else:\n",
        "                thr = 0.2\n",
        "\n",
        "            # Dynamic signal trade count (post-hoc diagnostic)\n",
        "            signal_trade_count_dyn = int(np.sum(abs_actions > thr)) if len(abs_actions) > 0 else 0\n",
        "\n",
        "\n",
        "            returns = pd.Series(nav_track).pct_change().fillna(0.0)\n",
        "            sharpe  = float((returns.mean() / (returns.std() + 1e-9)) * _annualization_factor(df_window))\n",
        "            drawdown = float(\n",
        "                ((pd.Series(nav_track).cummax() - pd.Series(nav_track)) /\n",
        "                pd.Series(nav_track).cummax()).max() * 100.0\n",
        "            )\n",
        "\n",
        "            # Classification stats (now using thr)\n",
        "            correct = 0\n",
        "            total   = 0\n",
        "            tp_buy = fp_buy = 0\n",
        "            tp_sell = fp_sell = 0\n",
        "\n",
        "            for r in step_log:\n",
        "                a = float(r[\"Action\"])\n",
        "                ret_t = float(r.get(\"next_ret\", 0.0))\n",
        "\n",
        "                if a > thr:\n",
        "                    sig = \"BUY\"\n",
        "                elif a < -thr:\n",
        "                    sig = \"SELL\"\n",
        "                else:\n",
        "                    sig = \"HOLD\"\n",
        "\n",
        "                if sig == \"BUY\":\n",
        "                    if ret_t > 0:\n",
        "                        tp_buy += 1; correct += 1\n",
        "                    else:\n",
        "                        fp_buy += 1\n",
        "                    total += 1\n",
        "                elif sig == \"SELL\":\n",
        "                    if ret_t < 0:\n",
        "                        tp_sell += 1; correct += 1\n",
        "                    else:\n",
        "                        fp_sell += 1\n",
        "                    total += 1\n",
        "                # HOLD not counted\n",
        "\n",
        "            precision_long  = tp_buy  / (tp_buy  + fp_buy  + 1e-9)\n",
        "            precision_short = tp_sell / (tp_sell + fp_sell + 1e-9)\n",
        "            precision_trades = (tp_buy + tp_sell) / (\n",
        "                (tp_buy + tp_sell) + (fp_buy + fp_sell) + 1e-9\n",
        "            )\n",
        "            step_accuracy = round(correct / total, 4) if total > 0 else 0.0\n",
        "            #Trade_count reflect REAL executed trades (cooldown/min_trade_delta)\n",
        "            trade_count = int(executed_trade_count)\n",
        "\n",
        "            # Save VecNormalize\n",
        "            try:\n",
        "                env.save(vecnorm_path)\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"Could not save VecNormalize for {ticker} {start}-{end}: {e}\")\n",
        "                vecnorm_path = None\n",
        "\n",
        "            # Save model\n",
        "            model.save(model_path)\n",
        "\n",
        "            # Save detailed predictions\n",
        "            pred_path = os.path.join(RUN_RESULTS_DIR, f\"{prefix}_predictions.csv\")\n",
        "            pd.DataFrame(step_log).to_csv(pred_path, index=False)\n",
        "            logging.info(f\"Saved predictions to {pred_path}\")\n",
        "\n",
        "            # Save compat predictions with same thresholds as metrics\n",
        "            compat_rows = []\n",
        "            for r in step_log:\n",
        "                a = r[\"Action\"]\n",
        "\n",
        "                if a > thr:\n",
        "                    signal = \"BUY\"\n",
        "                elif a < -thr:\n",
        "                    signal = \"SELL\"\n",
        "                else:\n",
        "                    signal = \"HOLD\"\n",
        "                compat_rows.append({\n",
        "                    \"Index\": r[\"Index\"],\n",
        "                    \"Datetime\": r[\"Datetime\"],\n",
        "                    \"Close\": r[\"Close\"],\n",
        "                    \"Action\": a,\n",
        "                    \"Signal\": signal,\n",
        "                    \"PortfolioValue\": r[\"nav\"],\n",
        "                    \"Reward\": r.get(\"reward\", np.nan),\n",
        "                })\n",
        "            compat_path = os.path.join(RUN_RESULTS_DIR, f\"{prefix}_predictions_compat.csv\")\n",
        "            pd.DataFrame(compat_rows).to_csv(compat_path, index=False)\n",
        "            logging.info(f\"Saved compatibility predictions to {compat_path}\")\n",
        "\n",
        "            # Summary row\n",
        "            result_row = {\n",
        "                \"Ticker\": ticker,\n",
        "                \"Window\": f\"{start}-{end}\",\n",
        "                \"WindowIdx\": int(w_idx + 1),\n",
        "                \"Prefix\": prefix,\n",
        "                \"PPO_Portfolio\": round(final_value, 2),\n",
        "                \"BuyHold\": round(hold_value, 2),\n",
        "                \"Sharpe\": round(sharpe, 3),\n",
        "                \"Drawdown_%\": round(drawdown, 2),\n",
        "                \"Winner\": \"PPO\" if final_value > hold_value else \"Buy & Hold\",\n",
        "                \"Action_Threshold\": round(thr, 4),\n",
        "                \"Accuracy\": step_accuracy,\n",
        "                \"Trade_Count\": trade_count,\n",
        "                \"Signal_Trade_Count\": int(signal_trade_count),\n",
        "                \"Signal_Trade_Count_Dyn\": int(signal_trade_count_dyn),\n",
        "                \"Executed_Trade_Count\": int(executed_trade_count),\n",
        "                \"Precision_Long\": round(precision_long, 4),\n",
        "                \"Precision_Short\": round(precision_short, 4),\n",
        "                \"Precision_Trades\": round(precision_trades, 4),\n",
        "            }\n",
        "\n",
        "            results.append(result_row)\n",
        "\n",
        "            meta = {\n",
        "                \"result\": result_row,\n",
        "                \"features\": df_window.columns.tolist(),\n",
        "                \"prefix\": prefix,\n",
        "                \"model_path\": model_path,\n",
        "                \"vecnorm_path\": vecnorm_path,\n",
        "            }\n",
        "\n",
        "            item = (result_row[\"Sharpe\"], prefix, meta)\n",
        "            if len(top_heap) < TOP_N_WINDOWS:\n",
        "                heapq.heappush(top_heap, item)\n",
        "            else:\n",
        "                if item[0] > top_heap[0][0]:\n",
        "                    heapq.heapreplace(top_heap, item)\n",
        "\n",
        "            logging.info(\n",
        "                f\"{ticker} | Window {w_idx+1} runtime: \"\n",
        "                f\"{round(time.time() - window_start_time, 2)}s\"\n",
        "            )\n",
        "        finally:\n",
        "            try:\n",
        "                env.close()\n",
        "            except Exception:\n",
        "                pass\n",
        "            del env\n",
        "            try:\n",
        "                del model\n",
        "            except Exception:\n",
        "                pass\n",
        "            gc.collect()\n",
        "            try:\n",
        "                torch.cuda.empty_cache()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    if skipped_windows:\n",
        "        logging.info(\n",
        "            f\"{ticker} skipped windows (already complete): {', '.join(skipped_windows)}\"\n",
        "        )\n",
        "        record_skips_global(\n",
        "            ticker,\n",
        "            skipped_windows=skipped_windows,\n",
        "            total_windows=len(windows),\n",
        "            fully_skipped=False,\n",
        "        )\n",
        "\n",
        "    # Save top-N QC-compatible\n",
        "    top_list = sorted(top_heap, key=lambda t: t[0], reverse=True)\n",
        "    for _, _, meta in top_list:\n",
        "        artifact_for_save = {\n",
        "            \"model\": None,  # we're copying from disk, not re-saving an in-memory model\n",
        "            \"model_path\": meta[\"model_path\"],\n",
        "            \"vecnorm_path\": meta[\"vecnorm_path\"],\n",
        "            \"features\": meta[\"features\"],\n",
        "            \"result\": meta[\"result\"],\n",
        "            \"prefix\": meta[\"prefix\"],\n",
        "        }\n",
        "        save_quantconnect_model(artifact_for_save, meta[\"prefix\"], QC_TOP_DIR)\n",
        "\n",
        "    return results\n",
        "\n",
        "def process_ticker(ticker):\n",
        "    try:\n",
        "        hp = pick_params(ticker)\n",
        "        return walkforward_ppo(\n",
        "            df[df[\"Symbol\"] == ticker].copy(),\n",
        "            ticker,\n",
        "            window_size=WINDOW_SIZE,\n",
        "            step_size=STEP_SIZE,\n",
        "            timesteps=TIMESTEPS,\n",
        "            learning_rate=hp[\"lr\"],\n",
        "            ppo_overrides=hp,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logging.error(f\"{ticker}: training failed with {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def run_parallel_tickers(tickers,\n",
        "                         out_path=os.path.join(RUN_RESULTS_DIR, \"summary.csv\"),\n",
        "                         max_workers=8):\n",
        "    results = []\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
        "        for res in ex.map(process_ticker, tickers):\n",
        "            if res:\n",
        "                results.extend(res)\n",
        "\n",
        "    if results:\n",
        "        pd.DataFrame(results).to_csv(out_path, index=False)\n",
        "        logging.info(f\"Saved summary to {out_path}\")\n",
        "    else:\n",
        "        logging.warning(\"No results produced; summary not written.\")\n",
        "\n",
        "    logging.info(\"All tickers processed.\")\n",
        "    return results\n",
        "\n",
        "def build_ppo_selector():\n",
        "    \"\"\"Aggregate all summary*.csv across runs and build selector JSON.\"\"\"\n",
        "    summary_files = glob.glob(\n",
        "        os.path.join(BASE_RESULTS_DIR, \"ppo_walkforward_results_*\", \"summary*.csv\")\n",
        "    )\n",
        "    all_summaries = []\n",
        "\n",
        "    for p in summary_files:\n",
        "        try:\n",
        "            tmp = pd.read_csv(p)\n",
        "            tmp[\"RunFolder\"] = os.path.dirname(p)\n",
        "            all_summaries.append(tmp)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Skipping {p} due to error: {e}\")\n",
        "\n",
        "    if not all_summaries:\n",
        "        logging.warning(\"No PPO summaries found across walkforward results folders.\")\n",
        "        return\n",
        "\n",
        "    combo = pd.concat(all_summaries, ignore_index=True)\n",
        "\n",
        "    # Robust Sharpe handling (avoid string sorting / bad rows from older runs)\n",
        "    if \"Sharpe\" in combo.columns:\n",
        "        combo[\"Sharpe\"] = pd.to_numeric(combo[\"Sharpe\"], errors=\"coerce\")\n",
        "        combo = combo.dropna(subset=[\"Sharpe\"])\n",
        "    else:\n",
        "        logging.warning(\"No 'Sharpe' column found in combined summaries; selector may be empty.\")\n",
        "\n",
        "    # Ensure key columns exist for robust ratios\n",
        "    if \"BuyHold\" not in combo.columns:\n",
        "        combo[\"BuyHold\"] = np.nan\n",
        "    if \"PPO_Portfolio\" not in combo.columns:\n",
        "        combo[\"PPO_Portfolio\"] = np.nan\n",
        "\n",
        "    # parse Window \"start-end\" to WindowStart\n",
        "    def _parse_window_start(w):\n",
        "        if pd.isna(w):\n",
        "            return None\n",
        "        if isinstance(w, (int, float)):\n",
        "            return int(w)\n",
        "        parts = str(w).split(\"-\")\n",
        "        try:\n",
        "            return int(parts[0])\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    combo[\"WindowStart\"] = combo[\"Window\"].apply(_parse_window_start)\n",
        "\n",
        "    combo = combo.sort_values([\"Ticker\", \"WindowStart\"]).reset_index(drop=True)\n",
        "    combo[\"WindowIdx\"] = combo.groupby(\"Ticker\").cumcount() + 1\n",
        "\n",
        "    combo = combo.drop_duplicates(subset=[\"Ticker\", \"WindowIdx\"], keep=\"last\")\n",
        "\n",
        "    best_by_symbol = (\n",
        "        combo\n",
        "        .sort_values(\"Sharpe\", ascending=False)\n",
        "        .groupby(\"Ticker\")\n",
        "        .first()\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # If Drawdown_% missing (older runs), create it so rename won't break\n",
        "    if \"Drawdown_%\" not in best_by_symbol.columns:\n",
        "        best_by_symbol[\"Drawdown_%\"] = np.nan\n",
        "\n",
        "    # Ensure precision cols exist\n",
        "    for col in [\"Precision_Long\", \"Precision_Short\", \"Precision_Trades\"]:\n",
        "        if col not in best_by_symbol.columns:\n",
        "            best_by_symbol[col] = None  # or np.nan if you prefer\n",
        "\n",
        "    # Rename columns so everything downstream uses consistent names\n",
        "    best_by_symbol = best_by_symbol.rename(columns={\n",
        "        \"Drawdown_%\": \"Drawdown\",\n",
        "        \"PPO_Portfolio\": \"Final_Portfolio\",\n",
        "    })\n",
        "\n",
        "    # Ensure Accuracy / Trade_Count exist\n",
        "    if \"Accuracy\" not in best_by_symbol.columns:\n",
        "        best_by_symbol[\"Accuracy\"] = 0.0\n",
        "    if \"Trade_Count\" not in best_by_symbol.columns:\n",
        "        best_by_symbol[\"Trade_Count\"] = None\n",
        "\n",
        "    best_by_symbol[\"Model\"] = MODEL_NAME\n",
        "\n",
        "    # PPO vs Buy & Hold ratio (safe division)\n",
        "    best_by_symbol[\"Rel_vs_BH\"] = best_by_symbol.apply(\n",
        "        lambda r: (r[\"Final_Portfolio\"] / r[\"BuyHold\"])\n",
        "        if (pd.notna(r[\"BuyHold\"]) and r[\"BuyHold\"] not in (0, 0.0)) else np.nan,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Save flat CSV for debugging\n",
        "    best_by_symbol.to_csv(SELECTOR_FULL_PATH, index=False)\n",
        "    print(f\"Aggregated PPO selector saved to → {SELECTOR_FULL_PATH}\")\n",
        "\n",
        "    # Safety filters (tune as needed)\n",
        "    df_sel = best_by_symbol.copy()\n",
        "    gates = (\n",
        "        (df_sel[\"Sharpe\"].fillna(-999) > 0.0) &\n",
        "        (df_sel[\"Drawdown\"].fillna(999) < 50.0) &\n",
        "        (df_sel[\"Final_Portfolio\"].fillna(0) > 80_000) &\n",
        "        (df_sel[\"Rel_vs_BH\"].fillna(0) >= 0.95)   # PPO ≥ 95% of B&H; change to >1.0 to enforce beat\n",
        "    )\n",
        "    df_sel = df_sel[gates].copy()\n",
        "\n",
        "    df_sel[\"prefix\"] = (\n",
        "        \"ppo_\"\n",
        "        + df_sel[\"Ticker\"].astype(str)\n",
        "        + \"_window\"\n",
        "        + df_sel[\"WindowIdx\"].astype(int).astype(str)\n",
        "    )\n",
        "\n",
        "    df_sel[\"artifact_path\"] = df_sel[\"prefix\"].apply(\n",
        "        lambda p: os.path.join(FINAL_MODEL_DIR, f\"{p}_model.zip\")\n",
        "    )\n",
        "    df_sel[\"vecnorm_path\"] = df_sel[\"prefix\"].apply(\n",
        "        lambda p: os.path.join(FINAL_MODEL_DIR, f\"{p}_vecnorm.pkl\")\n",
        "    )\n",
        "\n",
        "    EPS = 0.03  # 3% of top-sharpe for \"close enough\"\n",
        "    selected_models = {}\n",
        "\n",
        "    def safe_int(v, default=0):\n",
        "        if v is None:\n",
        "            return int(default)\n",
        "        try:\n",
        "            import math\n",
        "            if isinstance(v, float) and math.isnan(v):\n",
        "                return int(default)\n",
        "        except TypeError:\n",
        "            pass\n",
        "        try:\n",
        "            return int(v)\n",
        "        except (ValueError, TypeError):\n",
        "            return int(default)\n",
        "\n",
        "    def safe_float(v, default=0.0):\n",
        "        if v is None:\n",
        "            return float(default)\n",
        "        try:\n",
        "            import math\n",
        "            if isinstance(v, float) and math.isnan(v):\n",
        "                return float(default)\n",
        "        except TypeError:\n",
        "            pass\n",
        "        try:\n",
        "            return float(v)\n",
        "        except (ValueError, TypeError):\n",
        "            return float(default)\n",
        "\n",
        "    for ticker, group in df_sel.groupby(\"Ticker\"):\n",
        "        group_sorted = group.sort_values(\"Sharpe\", ascending=False)\n",
        "        top = group_sorted.iloc[0]\n",
        "        second = group_sorted.iloc[1] if len(group_sorted) > 1 else None\n",
        "\n",
        "        if (second is not None) and (\n",
        "            abs(top[\"Sharpe\"] - second[\"Sharpe\"]) <= abs(top[\"Sharpe\"]) * EPS\n",
        "        ):\n",
        "            mode = \"ensemble\"\n",
        "            primary, secondary = top[\"Model\"], second[\"Model\"]\n",
        "        else:\n",
        "            mode = \"single\"\n",
        "            primary, secondary = top[\"Model\"], None\n",
        "\n",
        "        selected_models[ticker] = {\n",
        "            \"model\": MODEL_NAME,\n",
        "            \"score\": round(safe_float(top[\"Sharpe\"]), 4),\n",
        "            \"return\": round(safe_float(top[\"Final_Portfolio\"]), 2),\n",
        "            \"sharpe\": round(safe_float(top[\"Sharpe\"]), 3),\n",
        "            \"drawdown\": round(safe_float(top[\"Drawdown\"]), 2),\n",
        "            \"sortino\": None,\n",
        "            \"turnover\": None,\n",
        "            \"trade_count\": safe_int(top.get(\"Trade_Count\", 0)),\n",
        "            \"precision\": {\n",
        "                \"long\":   safe_float(top.get(\"Precision_Long\", 0.0)),\n",
        "                \"short\":  safe_float(top.get(\"Precision_Short\", 0.0)),\n",
        "                \"trades\": safe_float(top.get(\"Precision_Trades\", 0.0)),\n",
        "            },\n",
        "            \"stability\": {},\n",
        "            \"regime\": \"unknown\",\n",
        "            \"rl_profile\": \"fast\",\n",
        "            \"artifact\": {\n",
        "                \"path\": top[\"artifact_path\"],\n",
        "                \"vecnorm\": top[\"vecnorm_path\"],\n",
        "                \"features\": None,\n",
        "                \"load_ms\": 180,\n",
        "                \"mem_mb\": 512,\n",
        "                \"exists\": os.path.exists(top[\"artifact_path\"]),\n",
        "            },\n",
        "            \"selection\": {\n",
        "                \"mode\": mode,\n",
        "                \"primary\": primary,\n",
        "                \"secondary\": secondary,\n",
        "            },\n",
        "        }\n",
        "\n",
        "    with open(SELECTOR_JSON_PATH, \"w\") as f:\n",
        "        json.dump(selected_models, f, indent=2)\n",
        "\n",
        "    print(f\"Final enhanced PPO selector JSON saved to → {SELECTOR_JSON_PATH}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.info(f\"RUN_RESULTS_DIR   = {RUN_RESULTS_DIR}\")\n",
        "    logging.info(f\"FINAL_MODEL_DIR  = {FINAL_MODEL_DIR}\")\n",
        "    logging.info(f\"BASE_RESULTS_DIR = {BASE_RESULTS_DIR}\")\n",
        "\n",
        "    min_rows = WINDOW_SIZE + 50  # small buffer so we have at least one window\n",
        "    all_symbols = df[\"Symbol\"].value_counts()\n",
        "    candidate_symbols = []\n",
        "\n",
        "    for sym, n in all_symbols.items():\n",
        "        if n >= min_rows:\n",
        "            candidate_symbols.append(sym)\n",
        "        else:\n",
        "            logging.warning(f\"Skipping {sym}: only {n} rows (< {min_rows} required)\")\n",
        "\n",
        "    if not candidate_symbols:\n",
        "        logging.error(\"No symbols have enough rows for the current WINDOW_SIZE. Nothing to train.\")\n",
        "    else:\n",
        "        logging.info(f\"Training candidate symbols: {candidate_symbols}\")\n",
        "\n",
        "    needed_cols = [\"Close\", \"Datetime\"]\n",
        "    if ENABLE_WAVELET:\n",
        "        needed_cols.append(\"Denoised_Close\")\n",
        "    if ENABLE_SENTIMENT:\n",
        "        needed_cols.append(\"SentimentScore\")\n",
        "\n",
        "    valid_symbols = []\n",
        "    for sym in candidate_symbols:\n",
        "        cols = set(df.loc[df[\"Symbol\"] == sym].columns)\n",
        "        missing = [c for c in needed_cols if c not in cols]\n",
        "        if missing:\n",
        "            logging.warning(f\"Skipping {sym}: missing required cols {missing}\")\n",
        "        else:\n",
        "            valid_symbols.append(sym)\n",
        "\n",
        "    if not valid_symbols:\n",
        "        logging.error(\"No symbols passed the feature/column checks. Nothing to train.\")\n",
        "    else:\n",
        "        logging.info(f\"Final training universe: {valid_symbols}\")\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    if test_mode:\n",
        "        # Optional: shrink timesteps and/or window size in test mode\n",
        "        TIMESTEPS = 100_000   # lighter test\n",
        "        # WINDOW_SIZE = 2000  # uncomment if you want faster test runs\n",
        "        # STEP_SIZE   = 500\n",
        "\n",
        "        test_stocks = [\"AAPL\", \"NVDA\", \"MSFT\"]\n",
        "        present = [s for s in test_stocks if s in valid_symbols]\n",
        "        if not present:\n",
        "            logging.warning(\"Test mode: none of ['AAPL','NVDA','MSFT'] present after filters.\")\n",
        "        else:\n",
        "            logging.info(f\"Test mode: running on {present}\")\n",
        "\n",
        "        for sym in present:\n",
        "            logging.info(f\">>> [TEST_MODE] Processing {sym}\")\n",
        "            res = process_ticker(sym)\n",
        "            logging.info(f\"{sym}: produced {len(res)} window summaries\")\n",
        "            if res:\n",
        "                all_results.extend(res)\n",
        "\n",
        "        summary_path = os.path.join(RUN_RESULTS_DIR, \"summary_test_mode.csv\")\n",
        "        if all_results:\n",
        "            pd.DataFrame(all_results).to_csv(summary_path, index=False)\n",
        "            logging.info(f\"Test-mode summary saved to {summary_path}\")\n",
        "        else:\n",
        "            logging.warning(\"Test mode finished but no results were generated (no windows, or all skipped).\")\n",
        "\n",
        "    else:\n",
        "        logging.info(\"Starting full parallel PPO walkforward run...\")\n",
        "        summary_results = run_parallel_tickers(valid_symbols)\n",
        "        if not summary_results:\n",
        "            logging.warning(\"No results generated in full run (check logs for skips/length issues).\")\n",
        "        else:\n",
        "            summary_path = os.path.join(RUN_RESULTS_DIR, \"summary.csv\")\n",
        "            pd.DataFrame(summary_results).to_csv(summary_path, index=False)\n",
        "            logging.info(f\"Summary saved to {summary_path}\")\n",
        "\n",
        "    try:\n",
        "        build_ppo_selector()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"build_ppo_selector failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9FCqXbnzHQU",
        "outputId": "b5c480aa-48e3-414a-cb54-ddb8a0cd1751"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "2025-12-26 00:44:30,958 - INFO - RUN_RESULTS_DIR   = /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044\n",
            "2025-12-26 00:44:30,960 - INFO - FINAL_MODEL_DIR  = /content/drive/MyDrive/Results_May_2025/ppo_models_master\n",
            "2025-12-26 00:44:30,961 - INFO - BASE_RESULTS_DIR = /content/drive/MyDrive/Results_May_2025\n",
            "2025-12-26 00:44:31,078 - INFO - Training candidate symbols: ['AAPL', 'QCOM', 'COST', 'RTX', 'BRK-B', 'SBUX', 'TMO', 'BAC', 'TSLA', 'CRM', 'AVGO', 'TXN', 'UNH', 'ADBE', 'AMGN', 'UNP', 'AMD', 'ACN', 'AMZN', 'ABT', 'WMT', 'ABBV', 'XOM', 'PFE', 'PM', 'CSCO', 'PG', 'LLY', 'MA', 'MCD', 'META', 'MRK', 'JPM', 'MSFT', 'NVDA', 'NKE', 'INTC', 'LOW', 'CVX', 'PEP', 'HD', 'GOOGL', 'ORCL', 'GE', 'BMY', 'JNJ', 'NEE', 'LIN', 'KO', 'V', 'UPS', 'DHR', 'IBM']\n",
            "2025-12-26 00:44:36,459 - INFO - Final training universe: ['AAPL', 'QCOM', 'COST', 'RTX', 'BRK-B', 'SBUX', 'TMO', 'BAC', 'TSLA', 'CRM', 'AVGO', 'TXN', 'UNH', 'ADBE', 'AMGN', 'UNP', 'AMD', 'ACN', 'AMZN', 'ABT', 'WMT', 'ABBV', 'XOM', 'PFE', 'PM', 'CSCO', 'PG', 'LLY', 'MA', 'MCD', 'META', 'MRK', 'JPM', 'MSFT', 'NVDA', 'NKE', 'INTC', 'LOW', 'CVX', 'PEP', 'HD', 'GOOGL', 'ORCL', 'GE', 'BMY', 'JNJ', 'NEE', 'LIN', 'KO', 'V', 'UPS', 'DHR', 'IBM']\n",
            "2025-12-26 00:44:36,461 - INFO - Test mode: running on ['AAPL', 'NVDA', 'MSFT']\n",
            "2025-12-26 00:44:36,462 - INFO - >>> [TEST_MODE] Processing AAPL\n",
            "2025-12-26 00:44:37,024 - INFO - Will train AAPL | Window 1 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:44:48,713 - INFO - Training AAPL Window 1/3\n",
            "2025-12-26 00:47:22,129 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window1_predictions.csv\n",
            "2025-12-26 00:47:22,205 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window1_predictions_compat.csv\n",
            "2025-12-26 00:47:22,206 - INFO - AAPL | Window 1 runtime: 165.69s\n",
            "2025-12-26 00:47:23,014 - INFO - Will train AAPL | Window 2 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:47:23,024 - INFO - Training AAPL Window 2/3\n",
            "2025-12-26 00:49:38,624 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window2_predictions.csv\n",
            "2025-12-26 00:49:38,718 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window2_predictions_compat.csv\n",
            "2025-12-26 00:49:38,719 - INFO - AAPL | Window 2 runtime: 136.07s\n",
            "2025-12-26 00:49:39,501 - INFO - Will train AAPL | Window 3 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:49:39,509 - INFO - Training AAPL Window 3/3\n",
            "2025-12-26 00:51:55,863 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window3_predictions.csv\n",
            "2025-12-26 00:51:55,987 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window3_predictions_compat.csv\n",
            "2025-12-26 00:51:55,990 - INFO - AAPL | Window 3 runtime: 136.86s\n",
            "2025-12-26 00:51:56,549 - INFO - [QC SAVE] Saved QC artifacts for ppo_AAPL_window3\n",
            "2025-12-26 00:51:56,576 - INFO - [QC SAVE] Saved QC artifacts for ppo_AAPL_window2\n",
            "2025-12-26 00:51:56,603 - INFO - [QC SAVE] Saved QC artifacts for ppo_AAPL_window1\n",
            "2025-12-26 00:51:56,610 - INFO - AAPL: produced 3 window summaries\n",
            "2025-12-26 00:51:56,612 - INFO - >>> [TEST_MODE] Processing NVDA\n",
            "2025-12-26 00:51:57,183 - INFO - Will train NVDA | Window 1 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:51:57,193 - INFO - Training NVDA Window 1/3\n",
            "2025-12-26 00:54:20,438 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window1_predictions.csv\n",
            "2025-12-26 00:54:20,643 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window1_predictions_compat.csv\n",
            "2025-12-26 00:54:20,645 - INFO - NVDA | Window 1 runtime: 143.94s\n",
            "2025-12-26 00:54:22,250 - INFO - Will train NVDA | Window 2 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:54:22,263 - INFO - Training NVDA Window 2/3\n",
            "2025-12-26 00:56:48,511 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window2_predictions.csv\n",
            "2025-12-26 00:56:48,590 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window2_predictions_compat.csv\n",
            "2025-12-26 00:56:48,591 - INFO - NVDA | Window 2 runtime: 147.14s\n",
            "2025-12-26 00:56:49,383 - INFO - Will train NVDA | Window 3 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:56:49,391 - INFO - Training NVDA Window 3/3\n",
            "2025-12-26 00:59:11,463 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window3_predictions.csv\n",
            "2025-12-26 00:59:11,547 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window3_predictions_compat.csv\n",
            "2025-12-26 00:59:11,548 - INFO - NVDA | Window 3 runtime: 142.54s\n",
            "2025-12-26 00:59:12,044 - INFO - [QC SAVE] Saved QC artifacts for ppo_NVDA_window1\n",
            "2025-12-26 00:59:12,081 - INFO - [QC SAVE] Saved QC artifacts for ppo_NVDA_window2\n",
            "2025-12-26 00:59:12,120 - INFO - [QC SAVE] Saved QC artifacts for ppo_NVDA_window3\n",
            "2025-12-26 00:59:12,127 - INFO - NVDA: produced 3 window summaries\n",
            "2025-12-26 00:59:12,127 - INFO - >>> [TEST_MODE] Processing MSFT\n",
            "2025-12-26 00:59:12,587 - INFO - Will train MSFT | Window 1 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:59:12,597 - INFO - Training MSFT Window 1/3\n",
            "2025-12-26 01:01:29,748 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window1_predictions.csv\n",
            "2025-12-26 01:01:29,862 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window1_predictions_compat.csv\n",
            "2025-12-26 01:01:29,865 - INFO - MSFT | Window 1 runtime: 137.7s\n",
            "2025-12-26 01:01:30,841 - INFO - Will train MSFT | Window 2 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 01:01:30,852 - INFO - Training MSFT Window 2/3\n",
            "2025-12-26 01:03:53,747 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window2_predictions.csv\n",
            "2025-12-26 01:03:53,877 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window2_predictions_compat.csv\n",
            "2025-12-26 01:03:53,881 - INFO - MSFT | Window 2 runtime: 143.51s\n",
            "2025-12-26 01:03:55,045 - INFO - Will train MSFT | Window 3 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 01:03:55,057 - INFO - Training MSFT Window 3/3\n",
            "2025-12-26 01:06:09,439 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window3_predictions.csv\n",
            "2025-12-26 01:06:09,522 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window3_predictions_compat.csv\n",
            "2025-12-26 01:06:09,523 - INFO - MSFT | Window 3 runtime: 134.92s\n",
            "2025-12-26 01:06:09,985 - INFO - [QC SAVE] Saved QC artifacts for ppo_MSFT_window2\n",
            "2025-12-26 01:06:10,025 - INFO - [QC SAVE] Saved QC artifacts for ppo_MSFT_window1\n",
            "2025-12-26 01:06:10,086 - INFO - [QC SAVE] Saved QC artifacts for ppo_MSFT_window3\n",
            "2025-12-26 01:06:10,093 - INFO - MSFT: produced 3 window summaries\n",
            "2025-12-26 01:06:10,108 - INFO - Test-mode summary saved to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/summary_test_mode.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aggregated PPO selector saved to → /content/drive/MyDrive/Results_May_2025/ppo_model_selector_FULL.csv\n",
            "Final enhanced PPO selector JSON saved to → /content/drive/MyDrive/Results_May_2025/ppo_model_selector_final.json\n"
          ]
        }
      ],
      "source": [
        "# PPO walkforward training + selector\n",
        "import os, gc, time, json, logging, glob\n",
        "import shutil\n",
        "from threading import Lock\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt  # optional for ad-hoc plots\n",
        "\n",
        "import torch\n",
        "import gymnasium as gym\n",
        "from gymnasium.spaces import Box as GBox\n",
        "\n",
        "import yfinance as yf\n",
        "from gym_anytrading.envs import StocksEnv\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"gymnasium\")\n",
        "# ---- Sharpe annualization helper (intraday heuristic: 6.5 hrs * 252) ----\n",
        "def _annualization_factor(_df_like=None) -> float:\n",
        "    \"\"\"Annualization factor for intraday bars (6.5 trading hours × 252 days).\"\"\"\n",
        "    return np.sqrt(252 * 6.5)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"jupyter_client.session\")\n",
        "warnings.filterwarnings(\"ignore\", message=\".*Gym has been unmaintained.*\")\n",
        "\n",
        "try:\n",
        "    compute_enhanced_features  # type: ignore\n",
        "except NameError:\n",
        "    def compute_enhanced_features(df_in: pd.DataFrame) -> pd.DataFrame:\n",
        "        df_out = df_in.copy()\n",
        "        if \"Datetime\" in df_out.columns:\n",
        "            df_out[\"Datetime\"] = pd.to_datetime(df_out[\"Datetime\"])\n",
        "            df_out = df_out.sort_values(\"Datetime\").reset_index(drop=True)\n",
        "        if \"Close\" not in df_out.columns:\n",
        "            raise ValueError(\"compute_enhanced_features: missing required column 'Close'\")\n",
        "        return df_out\n",
        "\n",
        "set_random_seed(42)\n",
        "\n",
        "BASE_RESULTS_DIR = \"/content/drive/MyDrive/Results_May_2025\"\n",
        "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "RUN_RESULTS_DIR = os.path.join(BASE_RESULTS_DIR, f\"ppo_walkforward_results_{RUN_TAG}\")\n",
        "FINAL_MODEL_DIR = os.path.join(BASE_RESULTS_DIR, \"ppo_models_master\")\n",
        "QC_TOP_DIR      = os.path.join(BASE_RESULTS_DIR, \"ppo_models_QC_TOP\")\n",
        "\n",
        "os.makedirs(QC_TOP_DIR, exist_ok=True)\n",
        "os.makedirs(RUN_RESULTS_DIR, exist_ok=True)\n",
        "os.makedirs(FINAL_MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# Aggregated selector outputs\n",
        "SELECTOR_FULL_PATH = os.path.join(BASE_RESULTS_DIR, \"ppo_model_selector_FULL.csv\")\n",
        "SELECTOR_JSON_PATH = os.path.join(BASE_RESULTS_DIR, \"ppo_model_selector_final.json\")\n",
        "MODEL_NAME = \"PPO\"\n",
        "\n",
        "# Global skip aggregation (thread-safe)\n",
        "SKIP_AGG_PATH = os.path.join(RUN_RESULTS_DIR, \"skipped_windows_global.csv\")\n",
        "SKIP_LOCK = Lock()\n",
        "\n",
        "# Logging Setup\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    force=True\n",
        ")\n",
        "\n",
        "# Flags\n",
        "ENABLE_SENTIMENT = False\n",
        "ENABLE_SLO       = True\n",
        "ENABLE_WAVELET   = True\n",
        "test_mode        = True            # set False for full universe\n",
        "ENABLE_PLOTS     = False\n",
        "LIVE_MODE        = False           # set True to run simple live/paper loop\n",
        "SIM_LATENCY_MS   = 0               # broker latency simulation; 0 = off\n",
        "BROKER           = \"log\"           # \"log\" = do not place orders, just log\n",
        "\n",
        "# Global training settings\n",
        "WINDOW_SIZE = 3500\n",
        "STEP_SIZE   = 500\n",
        "TIMESTEPS   = 150_000  # overridden in test_mode block to smaller value\n",
        "\n",
        "\n",
        "DATA_PATH = \"multi_stock_feature_engineered_dataset.csv\"\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    raise FileNotFoundError(\"Required feature-engineered dataset not found!\")\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "df[\"Datetime\"] = pd.to_datetime(df[\"Datetime\"])\n",
        "\n",
        "# Wavelet fallback\n",
        "if ENABLE_WAVELET and \"Denoised_Close\" not in df.columns:\n",
        "    logging.warning(\"ENABLE_WAVELET=True but 'Denoised_Close' missing; \"\n",
        "                    \"falling back to Close->Denoised_Close.\")\n",
        "    df[\"Denoised_Close\"] = df[\"Close\"]\n",
        "\n",
        "\n",
        "def record_skips_global(ticker: str, skipped_windows: list,\n",
        "                        total_windows: int = None, fully_skipped: bool = False):\n",
        "    \"\"\"Append skipped windows to the global skip log.\"\"\"\n",
        "    if not skipped_windows and not fully_skipped:\n",
        "        return\n",
        "    import csv\n",
        "    with SKIP_LOCK:\n",
        "        new_file = not os.path.exists(SKIP_AGG_PATH)\n",
        "        with open(SKIP_AGG_PATH, \"a\", newline=\"\") as f:\n",
        "            w = csv.writer(f)\n",
        "            if new_file:\n",
        "                w.writerow([\"Ticker\", \"Window\", \"FullySkipped\", \"TotalWindows\"])\n",
        "            if fully_skipped:\n",
        "                w.writerow([ticker, \"ALL\", True, total_windows if total_windows is not None else \"\"])\n",
        "            else:\n",
        "                for wname in skipped_windows:\n",
        "                    try:\n",
        "                        _, win_str = wname.split(\"_window\")\n",
        "                        win = int(win_str)\n",
        "                    except Exception:\n",
        "                        win = \"\"\n",
        "                    w.writerow([ticker, win, False, total_windows if total_windows is not None else \"\"])\n",
        "\n",
        "\n",
        "ENV_KWARGS = dict(\n",
        "    window_size=10,\n",
        "    cost_rate=0.0002,\n",
        "    slip_rate=0.0003,\n",
        "\n",
        "    k_alpha=0.0,\n",
        "    k_mom=0.15,\n",
        "    k_sent=(0.01 if ENABLE_SENTIMENT else 0.0),\n",
        "    mom_source=\"denoised\",\n",
        "    mom_lookback=20,\n",
        "\n",
        "    min_trade_delta=0.08,\n",
        "    cooldown=10,\n",
        "\n",
        "    reward_clip=0.05,\n",
        "    k_vol=0.00,\n",
        "    k_dd=0.00,\n",
        ")\n",
        "\n",
        "\n",
        "class ContinuousPositionEnv(StocksEnv):\n",
        "    def __init__(self, df, frame_bound, **kwargs):\n",
        "        # Require window_size from ENV_KWARGS\n",
        "        if \"window_size\" not in kwargs:\n",
        "            raise ValueError(\"ContinuousPositionEnv requires window_size (pass via ENV_KWARGS).\")\n",
        "\n",
        "        window_size = int(kwargs.pop(\"window_size\"))\n",
        "\n",
        "        # Pull params (all defaults live in ENV_KWARGS; these are just safety fallbacks)\n",
        "        cost_rate       = float(kwargs.pop(\"cost_rate\", 0.0002))\n",
        "        slip_rate       = float(kwargs.pop(\"slip_rate\", 0.0003))\n",
        "        k_alpha         = float(kwargs.pop(\"k_alpha\", 0.0))\n",
        "        k_mom           = float(kwargs.pop(\"k_mom\", 0.15))\n",
        "        k_sent          = float(kwargs.pop(\"k_sent\", 0.0))\n",
        "        mom_source      = str(kwargs.pop(\"mom_source\", \"denoised\"))\n",
        "        mom_lookback    = int(kwargs.pop(\"mom_lookback\", 20))\n",
        "        min_trade_delta = float(kwargs.pop(\"min_trade_delta\", 0.04))\n",
        "        cooldown        = int(kwargs.pop(\"cooldown\", 6))\n",
        "        reward_clip     = float(kwargs.pop(\"reward_clip\", 0.05))\n",
        "        k_vol           = float(kwargs.pop(\"k_vol\", 0.0))\n",
        "        k_dd            = float(kwargs.pop(\"k_dd\", 0.0))\n",
        "\n",
        "        # Fail fast on unexpected env kwargs\n",
        "        if kwargs:\n",
        "            raise ValueError(f\"Unexpected env kwargs: {list(kwargs.keys())}\")\n",
        "\n",
        "        super().__init__(\n",
        "            df=df.reset_index(drop=True),\n",
        "            frame_bound=frame_bound,\n",
        "            window_size=window_size\n",
        "        )\n",
        "\n",
        "        if isinstance(self.observation_space, gym.spaces.Box):\n",
        "            self.observation_space = GBox(\n",
        "                low=self.observation_space.low,\n",
        "                high=self.observation_space.high,\n",
        "                shape=self.observation_space.shape,\n",
        "                dtype=self.observation_space.dtype,\n",
        "            )\n",
        "\n",
        "        self.k_vol = k_vol\n",
        "        self.k_dd  = k_dd\n",
        "\n",
        "        self.ret_history = []\n",
        "        self.nav_history = []\n",
        "        self.peak_nav    = 1.0\n",
        "        self.trade_count = 0\n",
        "\n",
        "        self.action_space = GBox(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)\n",
        "\n",
        "\n",
        "        self.cost_rate       = cost_rate\n",
        "        self.slip_rate       = slip_rate\n",
        "        self.k_alpha         = k_alpha\n",
        "        self.k_mom           = k_mom\n",
        "        self.k_sent          = k_sent\n",
        "        self.mom_source      = mom_source\n",
        "        self.mom_lookback    = mom_lookback\n",
        "        self.min_trade_delta = min_trade_delta\n",
        "        self.cooldown        = cooldown\n",
        "        self.reward_clip     = reward_clip\n",
        "\n",
        "        self.nav = 1.0\n",
        "        self.pos = 0.0\n",
        "        self._last_trade_step = -self.cooldown\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        out = super().reset(**kwargs)\n",
        "        if isinstance(out, tuple):\n",
        "            obs, info = out\n",
        "        else:\n",
        "            obs, info = out, {}\n",
        "\n",
        "        self.nav = 1.0\n",
        "        self.pos = 0.0\n",
        "        self._last_trade_step = -self.cooldown\n",
        "\n",
        "        self.trade_count = 0\n",
        "        self.ret_history = []\n",
        "        self.nav_history = [self.nav]\n",
        "        self.peak_nav    = self.nav\n",
        "\n",
        "        info = info or {}\n",
        "        info.update({\n",
        "            \"nav\": self.nav,\n",
        "            \"pos\": self.pos,\n",
        "            \"trade_count\": int(self.trade_count),\n",
        "\n",
        "        })\n",
        "        return obs, info\n",
        "\n",
        "    def _step_parent_hold(self):\n",
        "        step_result = super().step(2)\n",
        "        if len(step_result) == 5:\n",
        "            obs, _env_rew, terminated, truncated, info = step_result\n",
        "        else:\n",
        "            obs, _env_rew, done, info = step_result\n",
        "            terminated, truncated = bool(done), False\n",
        "        return obs, terminated, truncated, info\n",
        "\n",
        "    def _ret_t(self):\n",
        "        cur  = float(self.df.loc[self._current_tick, \"Close\"])\n",
        "        prev = float(self.df.loc[max(self._current_tick - 1, 0), \"Close\"])\n",
        "        return 0.0 if prev <= 0 else (cur - prev) / prev\n",
        "\n",
        "    def _mom_signal(self):\n",
        "        if self.mom_source == \"macd\" and \"MACD_Line\" in self.df.columns:\n",
        "            recent = self.df[\"MACD_Line\"].iloc[max(self._current_tick - 200, 0):self._current_tick + 1]\n",
        "            return float(np.tanh(\n",
        "                float(self.df.loc[self._current_tick, \"MACD_Line\"]) /\n",
        "                (1e-6 + float(recent.std()))\n",
        "            ))\n",
        "\n",
        "        if \"Denoised_Close\" in self.df.columns and self._current_tick - self.mom_lookback >= 0:\n",
        "            now  = float(self.df.loc[self._current_tick, \"Denoised_Close\"])\n",
        "            then = float(self.df.loc[self._current_tick - self.mom_lookback, \"Denoised_Close\"])\n",
        "            base = float(self.df.loc[max(self._current_tick - 1, 0), \"Close\"])\n",
        "            slope = (now - then) / max(self.mom_lookback, 1)\n",
        "            return float(np.tanh(10.0 * (slope / max(abs(base), 1e-6))))\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "    def step(self, action):\n",
        "        a = float(np.array(action).squeeze())\n",
        "        target_pos = float(np.clip(a, -1.0, 1.0))\n",
        "\n",
        "        r_t = self._ret_t()\n",
        "        base_ret = self.pos * r_t\n",
        "\n",
        "        changed = (\n",
        "            abs(target_pos - self.pos) >= self.min_trade_delta\n",
        "        ) and (\n",
        "            (self._current_tick - self._last_trade_step) >= self.cooldown\n",
        "        )\n",
        "\n",
        "        delta_pos = (target_pos - self.pos) if changed else 0.0\n",
        "        trade_cost = (self.cost_rate + self.slip_rate) * abs(delta_pos)\n",
        "\n",
        "        rel_alpha = base_ret - r_t\n",
        "        mom_term = self.pos * self._mom_signal()\n",
        "\n",
        "        alpha_term = self.k_alpha * rel_alpha\n",
        "\n",
        "        sent_term = 0.0\n",
        "        if ENABLE_SENTIMENT and \"SentimentScore\" in self.df.columns:\n",
        "            sent_term = self.k_sent * float(self.df.loc[self._current_tick, \"SentimentScore\"])\n",
        "\n",
        "        shaped = base_ret + alpha_term + (self.k_mom * mom_term) + sent_term - trade_cost\n",
        "        reward = float(np.clip(shaped, -self.reward_clip, self.reward_clip))\n",
        "\n",
        "\n",
        "        self.nav *= (1.0 + base_ret - trade_cost)\n",
        "        self.nav_history.append(self.nav)\n",
        "        self.peak_nav = max(self.peak_nav, self.nav)\n",
        "\n",
        "        executed_trade = False\n",
        "        if changed:\n",
        "            self.pos = target_pos\n",
        "            self._last_trade_step = self._current_tick\n",
        "            self.trade_count += 1\n",
        "            executed_trade = True\n",
        "\n",
        "        obs, terminated, truncated, info = self._step_parent_hold()\n",
        "        info = info or {}\n",
        "        info.update({\n",
        "            \"ret_t\": r_t,\n",
        "            \"nav\": self.nav,\n",
        "            \"pos\": self.pos,\n",
        "            \"trade_cost\": trade_cost,\n",
        "            \"base_ret\": base_ret,\n",
        "            \"rel_alpha\": rel_alpha,\n",
        "            \"mom\": mom_term,\n",
        "            \"changed\": bool(changed),\n",
        "            \"executed_trade\": bool(executed_trade),\n",
        "            \"trade_count\": int(self.trade_count),\n",
        "            \"delta_pos\": float(delta_pos),\n",
        "        })\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "def get_mu_sigma(model, obs):\n",
        "    \"\"\"SB3 v2-safe way to get Gaussian policy mean/std for continuous actions.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        obs_t, _ = model.policy.obs_to_tensor(obs)\n",
        "        features = model.policy.extract_features(obs_t)\n",
        "        latent_pi, _ = model.policy.mlp_extractor(features)\n",
        "        mean_actions = model.policy.action_net(latent_pi)\n",
        "        log_std = model.policy.log_std\n",
        "        mu = float(mean_actions.detach().cpu().numpy().squeeze())\n",
        "        sigma = float(log_std.exp().detach().cpu().numpy().squeeze())\n",
        "    return mu, sigma\n",
        "\n",
        "def get_walk_forward_windows(df_in, window_size=3500, step_size=500, min_len=1200):\n",
        "    return [\n",
        "        (start, start + window_size)\n",
        "        for start in range(0, len(df_in) - min_len, step_size)\n",
        "        if start + window_size < len(df_in)\n",
        "    ]\n",
        "\n",
        "def save_quantconnect_model(artifact, prefix, save_dir):\n",
        "    \"\"\"Save/copy QC-compatible artifacts into save_dir.\"\"\"\n",
        "    import shutil\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # --- Model zip: save or copy ---\n",
        "    model_dst = os.path.join(save_dir, f\"{prefix}_model.zip\")\n",
        "\n",
        "    model_obj = artifact.get(\"model\", None)\n",
        "    model_src = artifact.get(\"model_path\", None)\n",
        "\n",
        "    try:\n",
        "        if model_obj is not None:\n",
        "            # Save from in-memory SB3 model\n",
        "            if not os.path.exists(model_dst):\n",
        "                model_obj.save(model_dst)\n",
        "\n",
        "        else:\n",
        "            # Copy from an existing trained window model zip\n",
        "            if model_src and os.path.exists(model_src):\n",
        "                if os.path.abspath(model_src) != os.path.abspath(model_dst):\n",
        "                    shutil.copyfile(model_src, model_dst)\n",
        "            else:\n",
        "                # If neither provided, warn loudly\n",
        "                if not os.path.exists(model_dst):\n",
        "                    logging.warning(f\"[QC SAVE] Missing model for {prefix}: no model_obj and no valid model_path.\")\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[QC SAVE] Model handling issue for {prefix}: {e}\")\n",
        "\n",
        "    # --- VecNormalize: copy ---\n",
        "    vecnorm_src = artifact.get(\"vecnorm_path\")\n",
        "    if vecnorm_src and os.path.exists(vecnorm_src):\n",
        "        try:\n",
        "            vecnorm_dst = os.path.join(save_dir, f\"{prefix}_vecnorm.pkl\")\n",
        "            if os.path.abspath(vecnorm_src) != os.path.abspath(vecnorm_dst):\n",
        "                shutil.copyfile(vecnorm_src, vecnorm_dst)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"[QC SAVE] VecNormalize handling issue for {prefix}: {e}\")\n",
        "    else:\n",
        "        logging.warning(f\"[QC SAVE] VecNormalize missing for {prefix}: vecnorm_path not found.\")\n",
        "\n",
        "    # --- Features ---\n",
        "    try:\n",
        "        with open(os.path.join(save_dir, f\"{prefix}_features.json\"), \"w\") as f:\n",
        "            json.dump({\"features\": artifact.get(\"features\", [])}, f)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[QC SAVE] Could not write features.json for {prefix}: {e}\")\n",
        "\n",
        "    # --- Probability config ---\n",
        "    try:\n",
        "        thr = 0.2\n",
        "        try:\n",
        "            thr = float(artifact.get(\"result\", {}).get(\"Action_Threshold\", 0.2))\n",
        "        except Exception:\n",
        "            thr = 0.2\n",
        "\n",
        "        with open(os.path.join(save_dir, f\"{prefix}_probability_config.json\"), \"w\") as f:\n",
        "            json.dump(\n",
        "                {\"threshold\": thr, \"use_confidence\": True, \"inference_mode\": \"deterministic\"},\n",
        "                f\n",
        "            )\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[QC SAVE] Could not write probability_config.json for {prefix}: {e}\")\n",
        "\n",
        "\n",
        "    # --- Model info ---\n",
        "    try:\n",
        "        r = artifact.get(\"result\", {})\n",
        "        with open(os.path.join(save_dir, f\"{prefix}_model_info.json\"), \"w\") as f:\n",
        "            json.dump({\n",
        "                \"model\": \"PPO\",\n",
        "                \"ticker\": r.get(\"Ticker\"),\n",
        "                \"window\": r.get(\"Window\"),\n",
        "                \"date_trained\": datetime.today().strftime(\"%Y-%m-%d\"),\n",
        "                \"framework\": \"stable-baselines3\",\n",
        "                \"input_features\": artifact.get(\"features\", []),\n",
        "                \"final_portfolio\": r.get(\"PPO_Portfolio\"),\n",
        "                \"buy_hold\": r.get(\"BuyHold\"),\n",
        "                \"sharpe\": r.get(\"Sharpe\"),\n",
        "            }, f)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[QC SAVE] Could not write model_info.json for {prefix}: {e}\")\n",
        "\n",
        "    logging.info(f\"[QC SAVE] Saved QC artifacts for {prefix}\")\n",
        "\n",
        "def load_model_and_env(prefix):\n",
        "    \"\"\"Load a trained PPO and create a factory to build a matching env window.\"\"\"\n",
        "    model_path = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_model.zip\")\n",
        "    vec_path   = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_vecnorm.pkl\")\n",
        "    model = PPO.load(model_path, device=\"cpu\")\n",
        "\n",
        "    def make_env(df_window):\n",
        "        frame_bound = (50, len(df_window) - 3)\n",
        "        e = DummyVecEnv([lambda: ContinuousPositionEnv(\n",
        "            df=df_window, frame_bound=frame_bound, **ENV_KWARGS\n",
        "        )])\n",
        "        if os.path.exists(vec_path):\n",
        "            e = VecNormalize.load(vec_path, e)\n",
        "        e.training = False\n",
        "        e.norm_reward = False\n",
        "        return e\n",
        "\n",
        "    return model, make_env\n",
        "\n",
        "def latest_df_for_symbol(symbol, horizon_days=5, interval=\"1m\"):\n",
        "    \"\"\"Fetch fresh bars and rebuild features exactly like training.\"\"\"\n",
        "    end = datetime.utcnow()\n",
        "    start = end - timedelta(days=horizon_days)\n",
        "    df_live = yf.download(\n",
        "        symbol,\n",
        "        start=start.strftime(\"%Y-%m-%d\"),\n",
        "        end=end.strftime(\"%Y-%m-%d\"),\n",
        "        interval=interval,\n",
        "        progress=False,\n",
        "        auto_adjust=False,\n",
        "    )\n",
        "    if df_live is None or df_live.empty:\n",
        "        return None\n",
        "    df_live = df_live.reset_index()\n",
        "    df_live[\"Symbol\"] = symbol\n",
        "    df_live = compute_enhanced_features(df_live)\n",
        "    if ENABLE_WAVELET and \"Denoised_Close\" not in df_live.columns:\n",
        "        df_live[\"Denoised_Close\"] = df_live[\"Close\"]\n",
        "    return df_live\n",
        "\n",
        "def predict_latest(symbol, prefix):\n",
        "    \"\"\"Build last window, fast-forward env, call model.predict(), return a signal.\"\"\"\n",
        "    # --- load per-model threshold ---\n",
        "    cfg_path = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_probability_config.json\")\n",
        "    thr = 0.2\n",
        "    if os.path.exists(cfg_path):\n",
        "        try:\n",
        "            with open(cfg_path, \"r\") as f:\n",
        "                thr = float(json.load(f).get(\"threshold\", 0.2))\n",
        "        except Exception:\n",
        "            thr = 0.2\n",
        "\n",
        "    model, make_env = load_model_and_env(prefix)\n",
        "    live_df = latest_df_for_symbol(symbol)\n",
        "    if live_df is None or len(live_df) < 100:\n",
        "        logging.warning(\"No fresh data yet for live inference.\")\n",
        "        return None\n",
        "\n",
        "    df_window = live_df.iloc[-2500:].reset_index(drop=True) if len(live_df) > 2500 else live_df.copy()\n",
        "\n",
        "    env = make_env(df_window)\n",
        "    obs = env.reset()\n",
        "    if isinstance(obs, tuple):\n",
        "        obs, _ = obs\n",
        "\n",
        "    # fast-forward with HOLD\n",
        "    for _ in range(len(df_window) - 1):\n",
        "        obs, _, dones, _ = env.step([np.array([0.0], dtype=np.float32)])\n",
        "        if isinstance(dones, (np.ndarray, list, tuple)) and len(dones) and dones[0]:\n",
        "            break\n",
        "\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    mu, sigma = get_mu_sigma(model, obs)\n",
        "\n",
        "    a = float(np.array(action).squeeze())\n",
        "\n",
        "    # --- thresholded signal using loaded thr ---\n",
        "    if a > thr:\n",
        "        signal = \"BUY\"\n",
        "    elif a < -thr:\n",
        "        signal = \"SELL\"\n",
        "    else:\n",
        "        signal = \"HOLD\"\n",
        "\n",
        "    conf = abs(a)\n",
        "    ts = df_window[\"Datetime\"].iloc[-1] if \"Datetime\" in df_window.columns else None\n",
        "    price = float(df_window[\"Close\"].iloc[-1])\n",
        "\n",
        "    return dict(\n",
        "        signal=signal,\n",
        "        confidence=conf,\n",
        "        action=a,\n",
        "        threshold=thr,\n",
        "        ts=ts,\n",
        "        price=price,\n",
        "        mu=mu,\n",
        "        sigma=sigma,\n",
        "    )\n",
        "\n",
        "def place_order(signal, qty=1):\n",
        "    \"\"\"Stub broker router with latency simulation; logs in Colab.\"\"\"\n",
        "    if SIM_LATENCY_MS > 0:\n",
        "        time.sleep(SIM_LATENCY_MS / 1000.0)\n",
        "    if BROKER == \"log\":\n",
        "        logging.info(f\"[PAPER] {signal} x{qty}\")\n",
        "    else:\n",
        "        logging.info(f\"[BROKER={BROKER}] {signal} x{qty} (not implemented)\")\n",
        "\n",
        "def live_loop(symbol, best_prefix):\n",
        "    \"\"\"Simple polling loop—set LIVE_MODE=True to run.\"\"\"\n",
        "    while LIVE_MODE:\n",
        "        try:\n",
        "            pred = predict_latest(symbol, best_prefix)\n",
        "            if pred:\n",
        "                logging.info(\n",
        "                    f\"{symbol} {pred['ts']} | {pred['signal']} \"\n",
        "                    f\"@ {pred['price']:.2f} (conf {pred['confidence']:.2f})\"\n",
        "                )\n",
        "                place_order(pred[\"signal\"], qty=1)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Live loop error: {e}\")\n",
        "        time.sleep(60)  # Poll each minute\n",
        "\n",
        "TOP_N_WINDOWS = 3\n",
        "\n",
        "FAST = {\n",
        "    \"lr\": 8e-5,\n",
        "    \"n_steps\": 3072,\n",
        "    \"batch\": 512,\n",
        "    \"clip\": 0.2,\n",
        "    \"ent\": 0.01,\n",
        "}\n",
        "\n",
        "SLOW = {\n",
        "    \"lr\": 3e-5,\n",
        "    \"n_steps\": 3072,\n",
        "    \"batch\": 512,\n",
        "    \"clip\": 0.16,\n",
        "    \"ent\": 0.005,\n",
        "}\n",
        "\n",
        "fast_names = {\n",
        "    \"TSLA\",\"NVDA\",\"AMD\",\"AVGO\",\"AAPL\",\"MSFT\",\"AMZN\",\"GOOGL\",\"META\",\"ADBE\",\"CRM\",\n",
        "    \"INTC\",\"QCOM\",\"TXN\",\"ORCL\",\"NEE\",\"GE\",\"XOM\",\"CVX\",\"LLY\",\"NKE\",\"SBUX\"\n",
        "}\n",
        "slow_names = {\n",
        "    \"BRK-B\",\"JPM\",\"BAC\",\"JNJ\",\"UNH\",\"MRK\",\"PFE\",\"ABBV\",\"ABT\",\"AMGN\",\"PG\",\"PEP\",\"KO\",\n",
        "    \"V\",\"MA\",\"WMT\",\"MCD\",\"TMO\",\"DHR\",\"ACN\",\"IBM\",\"LIN\",\"PM\",\"RTX\",\"UPS\",\"UNP\",\"COST\",\"HD\",\"LOW\"\n",
        "}\n",
        "\n",
        "def pick_params(symbol: str):\n",
        "    return FAST if symbol in fast_names else SLOW\n",
        "\n",
        "def export_qc_top_from_existing(ticker: str, top_n: int = 3):\n",
        "    \"\"\"\n",
        "    If a ticker is fully skipped (models already exist), still populate QC_TOP_DIR.\n",
        "    Uses existing summary CSVs to pick top Sharpe windows, then copies artifacts from FINAL_MODEL_DIR.\n",
        "    Prefers using 'Prefix' from summaries (robust). Falls back to WindowIdx reconstruction.\n",
        "    \"\"\"\n",
        "    summary_files = glob.glob(os.path.join(BASE_RESULTS_DIR, \"ppo_walkforward_results_*\", \"summary*.csv\"))\n",
        "    if not summary_files:\n",
        "        logging.warning(f\"[QC_TOP] No summary files found; cannot export QC_TOP for {ticker}.\")\n",
        "        return\n",
        "\n",
        "    frames = []\n",
        "    for p in summary_files:\n",
        "        try:\n",
        "            tmp = pd.read_csv(p)\n",
        "            frames.append(tmp)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"[QC_TOP] Could not read {p}: {e}\")\n",
        "\n",
        "    if not frames:\n",
        "        logging.warning(f\"[QC_TOP] Could not read any summary files; cannot export QC_TOP for {ticker}.\")\n",
        "        return\n",
        "\n",
        "    combo = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "    if \"Ticker\" not in combo.columns:\n",
        "        logging.warning(\"[QC_TOP] Summary files missing 'Ticker' column; cannot export.\")\n",
        "        return\n",
        "\n",
        "    combo = combo[combo[\"Ticker\"] == ticker].copy()\n",
        "    if combo.empty or \"Sharpe\" not in combo.columns:\n",
        "        logging.warning(f\"[QC_TOP] No rows for {ticker} in summaries (or missing Sharpe); cannot export QC_TOP.\")\n",
        "        return\n",
        "\n",
        "    # Ensure Sharpe is numeric so sorting works reliably\n",
        "    combo[\"Sharpe\"] = pd.to_numeric(combo[\"Sharpe\"], errors=\"coerce\")\n",
        "    combo = combo.dropna(subset=[\"Sharpe\"])\n",
        "    if combo.empty:\n",
        "        logging.warning(f\"[QC_TOP] All Sharpe values were non-numeric for {ticker}; cannot export.\")\n",
        "        return\n",
        "\n",
        "    use_prefix = (\"Prefix\" in combo.columns) and combo[\"Prefix\"].notna().any()\n",
        "\n",
        "    if use_prefix:\n",
        "        # Robust path: use saved Prefix directly\n",
        "        top = combo.sort_values(\"Sharpe\", ascending=False).head(top_n).copy()\n",
        "        top[\"__prefix__\"] = top[\"Prefix\"].astype(str)\n",
        "    else:\n",
        "        # Fallback: reconstruct WindowIdx (less robust)\n",
        "        def _window_start(w):\n",
        "            try:\n",
        "                s = str(w)\n",
        "                return int(s.split(\"-\")[0]) if \"-\" in s else np.nan\n",
        "            except Exception:\n",
        "                return np.nan\n",
        "\n",
        "        combo[\"WindowStart\"] = combo[\"Window\"].apply(_window_start)\n",
        "        combo = combo.sort_values([\"WindowStart\"]).reset_index(drop=True)\n",
        "        combo[\"WindowIdx\"] = combo.groupby(\"Ticker\").cumcount() + 1\n",
        "\n",
        "        top = combo.sort_values(\"Sharpe\", ascending=False).head(top_n).copy()\n",
        "        top[\"__prefix__\"] = top[\"WindowIdx\"].apply(lambda widx: f\"ppo_{ticker}_window{int(widx)}\")\n",
        "\n",
        "    exported = 0\n",
        "\n",
        "    for _, r in top.iterrows():\n",
        "        prefix = str(r[\"__prefix__\"])\n",
        "\n",
        "        model_path = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_model.zip\")\n",
        "        vec_path   = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_vecnorm.pkl\")\n",
        "\n",
        "        if not (os.path.exists(model_path) and os.path.exists(vec_path)):\n",
        "            logging.warning(f\"[QC_TOP] Missing model/vecnorm for {prefix}; cannot export.\")\n",
        "            continue\n",
        "\n",
        "        artifact_for_save = {\n",
        "            \"model\": None,\n",
        "            \"model_path\": model_path,\n",
        "            \"vecnorm_path\": vec_path,\n",
        "            \"features\": [],         # ok if unknown; QC can load features elsewhere\n",
        "            \"result\": r.to_dict(),  # includes Sharpe, Action_Threshold, etc if present\n",
        "            \"prefix\": prefix,\n",
        "        }\n",
        "        save_quantconnect_model(artifact_for_save, prefix, QC_TOP_DIR)\n",
        "        exported += 1\n",
        "\n",
        "    logging.info(f\"[QC_TOP] Exported {exported}/{len(top)} QC artifacts for {ticker}.\")\n",
        "\n",
        "def walkforward_ppo(df_sym, ticker,\n",
        "                    window_size=3500, step_size=500,\n",
        "                    timesteps=150_000, learning_rate=1e-4,\n",
        "                    ppo_overrides=None):\n",
        "    import heapq\n",
        "\n",
        "    if ppo_overrides is None:\n",
        "        ppo_overrides = {}\n",
        "\n",
        "    if len(df_sym) < window_size:\n",
        "        logging.warning(\n",
        "            f\"Skipping {ticker}: only {len(df_sym)} rows (min required: {window_size})\"\n",
        "        )\n",
        "        return []\n",
        "\n",
        "    results = []\n",
        "    windows = get_walk_forward_windows(df_sym, window_size, step_size)\n",
        "    top_heap = []\n",
        "    skipped_windows = []\n",
        "\n",
        "    # quick check: all windows already have model+vecnorm?\n",
        "    all_done = True\n",
        "    for idx in range(len(windows)):\n",
        "        prefix = f\"ppo_{ticker}_window{idx+1}\"\n",
        "        model_ok   = os.path.exists(os.path.join(FINAL_MODEL_DIR, f\"{prefix}_model.zip\"))\n",
        "        vecnorm_ok = os.path.exists(os.path.join(FINAL_MODEL_DIR, f\"{prefix}_vecnorm.pkl\"))\n",
        "        if not (model_ok and vecnorm_ok):\n",
        "            all_done = False\n",
        "            break\n",
        "\n",
        "    if all_done:\n",
        "        logging.info(f\"Ticker {ticker} fully skipped (all {len(windows)} windows already complete).\")\n",
        "        record_skips_global(ticker, skipped_windows=[], total_windows=len(windows), fully_skipped=True)\n",
        "\n",
        "        export_qc_top_from_existing(ticker, top_n=TOP_N_WINDOWS)\n",
        "        return []\n",
        "\n",
        "\n",
        "\n",
        "    for w_idx, (start, end) in enumerate(windows):\n",
        "        window_start_time = time.time()\n",
        "        gc.collect()\n",
        "\n",
        "        prefix = f\"ppo_{ticker}_window{w_idx+1}\"\n",
        "        model_path   = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_model.zip\")\n",
        "        vecnorm_path = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_vecnorm.pkl\")\n",
        "\n",
        "        if os.path.exists(model_path) and os.path.exists(vecnorm_path):\n",
        "            logging.info(f\"Skipping {ticker} | Window {w_idx+1}, already trained.\")\n",
        "            skipped_windows.append(f\"{ticker}_window{w_idx+1}\")\n",
        "            continue\n",
        "\n",
        "        missing = []\n",
        "        if not os.path.exists(model_path):   missing.append(\"model.zip\")\n",
        "        if not os.path.exists(vecnorm_path): missing.append(\"vecnorm.pkl\")\n",
        "        logging.info(\n",
        "            f\"Will train {ticker} | Window {w_idx+1} because missing: {', '.join(missing)}\"\n",
        "        )\n",
        "\n",
        "        df_window = df_sym.iloc[start:end].reset_index(drop=True)\n",
        "        if len(df_window) <= 52 or len(df_window) % 2 != 0:\n",
        "            df_window = df_window.iloc[:-1]\n",
        "\n",
        "        frame_bound = (50, len(df_window) - 3)\n",
        "\n",
        "        env = DummyVecEnv([lambda: ContinuousPositionEnv(\n",
        "          df=df_window, frame_bound=frame_bound, **ENV_KWARGS\n",
        "        )])\n",
        "\n",
        "        env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
        "\n",
        "        try:\n",
        "            model = PPO(\n",
        "                \"MlpPolicy\",\n",
        "                env,\n",
        "                verbose=0,\n",
        "                device=(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "                learning_rate=ppo_overrides.get(\"lr\", learning_rate),\n",
        "                n_steps=ppo_overrides.get(\"n_steps\", 256),\n",
        "                batch_size=ppo_overrides.get(\"batch\", 64),\n",
        "                n_epochs=5,\n",
        "                gamma=0.99,\n",
        "                gae_lambda=0.95,\n",
        "                clip_range=ppo_overrides.get(\"clip\", 0.2),\n",
        "                ent_coef=ppo_overrides.get(\"ent\", 0.005),\n",
        "                policy_kwargs=dict(net_arch=[64, 64]),\n",
        "            )\n",
        "\n",
        "            logging.info(f\"Training {ticker} Window {w_idx+1}/{len(windows)}\")\n",
        "            model.learn(total_timesteps=timesteps)\n",
        "\n",
        "            # Evaluation pass\n",
        "            env.training = False\n",
        "            env.norm_reward = False\n",
        "            obs = env.reset()\n",
        "            if isinstance(obs, tuple):\n",
        "                obs, _ = obs\n",
        "\n",
        "            nav_track = [1.0]\n",
        "            bh_track  = [1.0]\n",
        "            step_log  = []\n",
        "            executed_trade_count = 0\n",
        "            signal_trade_count   = 0\n",
        "            signal_trade_count_dyn   = 0   # dynamic-threshold diagnostic\n",
        "\n",
        "            DIAG_THR = 0.2\n",
        "            for i in range(len(df_window) - 1):\n",
        "                action, _ = model.predict(obs, deterministic=True)\n",
        "                mu, sigma = get_mu_sigma(model, obs)\n",
        "\n",
        "                obs, rew, dones, infos = env.step(action)\n",
        "                # VecEnv returns list/tuple of infos; otherwise it may be a dict\n",
        "                if isinstance(infos, (list, tuple)):\n",
        "                    info = infos[0] if len(infos) else {}\n",
        "                elif isinstance(infos, dict):\n",
        "                    info = infos\n",
        "                else:\n",
        "                    info = {}\n",
        "\n",
        "\n",
        "                nav_track.append(float(info.get(\"nav\", nav_track[-1])))\n",
        "                bh_track.append(\n",
        "                    bh_track[-1] * (1.0 + float(info.get(\"ret_t\", 0.0)))\n",
        "                )\n",
        "\n",
        "                a = float(np.array(action).squeeze())\n",
        "                dt_val = df_window[\"Datetime\"].iloc[i+1] if \"Datetime\" in df_window.columns else None\n",
        "                px     = float(df_window[\"Close\"].iloc[i+1]) if \"Close\" in df_window.columns else np.nan\n",
        "                #“signal” trades (threshold-based) — diagnostic only\n",
        "                if a > DIAG_THR or a < -DIAG_THR:\n",
        "                    signal_trade_count += 1\n",
        "                #real trades executed by env friction logic\n",
        "                if bool(info.get(\"executed_trade\", False)):\n",
        "                    executed_trade_count += 1\n",
        "\n",
        "                # next-bar return (to score BUY/SELL vs the *next* move)\n",
        "                if i + 2 < len(df_window):\n",
        "                    p0 = float(df_window[\"Close\"].iloc[i+1])\n",
        "                    p1 = float(df_window[\"Close\"].iloc[i+2])\n",
        "                    next_ret = 0.0 if p0 <= 0 else (p1 - p0) / p0\n",
        "                else:\n",
        "                    next_ret = 0.0\n",
        "\n",
        "                # reward scalar (VecEnv returns arrays)\n",
        "                rew_val = float(rew[0]) if isinstance(rew, (list, tuple, np.ndarray)) else float(rew)\n",
        "\n",
        "                step_log.append({\n",
        "                    \"Index\": i+1,\n",
        "                    \"Datetime\": dt_val,\n",
        "                    \"Close\": px,\n",
        "                    \"Action\": a,\n",
        "                    \"mu\": mu,\n",
        "                    \"sigma\": sigma,\n",
        "                    \"nav\": nav_track[-1],\n",
        "                    \"ret_t\": float(info.get(\"ret_t\", 0.0)),\n",
        "                    \"next_ret\": float(next_ret),\n",
        "                    \"reward\": rew_val,\n",
        "                    \"pos\": float(info.get(\"pos\", 0.0)),\n",
        "                    \"trade_cost\": float(info.get(\"trade_cost\", 0.0)),\n",
        "                    \"base_ret\": float(info.get(\"base_ret\", 0.0)),\n",
        "                    \"rel_alpha\": float(info.get(\"rel_alpha\", 0.0)),\n",
        "                    \"mom\": float(info.get(\"mom\", 0.0)),\n",
        "                })\n",
        "\n",
        "                # done handling (VecEnv)\n",
        "                if isinstance(dones, (np.ndarray, list, tuple)):\n",
        "                    if dones[0]:\n",
        "                        break\n",
        "                elif dones:\n",
        "                    break\n",
        "\n",
        "\n",
        "            # --- Metrics ---\n",
        "            final_value = float(nav_track[-1]) * 100_000.0\n",
        "            hold_value  = float(bh_track[-1])  * 100_000.0\n",
        "\n",
        "            #dynamic action threshold for this window (prevents “no signals” windows)\n",
        "            abs_actions = np.array([abs(float(r[\"Action\"])) for r in step_log], dtype=float)\n",
        "            if len(abs_actions) > 0:\n",
        "                thr = float(np.quantile(abs_actions, 0.70))  # 70th percentile\n",
        "                thr = float(np.clip(thr, 0.08, 0.30))\n",
        "            else:\n",
        "                thr = 0.2\n",
        "\n",
        "            # Dynamic signal trade count (post-hoc diagnostic)\n",
        "            signal_trade_count_dyn = int(np.sum(abs_actions > thr)) if len(abs_actions) > 0 else 0\n",
        "\n",
        "\n",
        "            returns = pd.Series(nav_track).pct_change().fillna(0.0)\n",
        "            sharpe  = float((returns.mean() / (returns.std() + 1e-9)) * _annualization_factor(df_window))\n",
        "            drawdown = float(\n",
        "                ((pd.Series(nav_track).cummax() - pd.Series(nav_track)) /\n",
        "                pd.Series(nav_track).cummax()).max() * 100.0\n",
        "            )\n",
        "\n",
        "            # Classification stats (now using thr)\n",
        "            correct = 0\n",
        "            total   = 0\n",
        "            tp_buy = fp_buy = 0\n",
        "            tp_sell = fp_sell = 0\n",
        "\n",
        "            for r in step_log:\n",
        "                a = float(r[\"Action\"])\n",
        "                ret_t = float(r.get(\"next_ret\", 0.0))\n",
        "\n",
        "                if a > thr:\n",
        "                    sig = \"BUY\"\n",
        "                elif a < -thr:\n",
        "                    sig = \"SELL\"\n",
        "                else:\n",
        "                    sig = \"HOLD\"\n",
        "\n",
        "                if sig == \"BUY\":\n",
        "                    if ret_t > 0:\n",
        "                        tp_buy += 1; correct += 1\n",
        "                    else:\n",
        "                        fp_buy += 1\n",
        "                    total += 1\n",
        "                elif sig == \"SELL\":\n",
        "                    if ret_t < 0:\n",
        "                        tp_sell += 1; correct += 1\n",
        "                    else:\n",
        "                        fp_sell += 1\n",
        "                    total += 1\n",
        "                # HOLD not counted\n",
        "\n",
        "            precision_long  = tp_buy  / (tp_buy  + fp_buy  + 1e-9)\n",
        "            precision_short = tp_sell / (tp_sell + fp_sell + 1e-9)\n",
        "            precision_trades = (tp_buy + tp_sell) / (\n",
        "                (tp_buy + tp_sell) + (fp_buy + fp_sell) + 1e-9\n",
        "            )\n",
        "            step_accuracy = round(correct / total, 4) if total > 0 else 0.0\n",
        "            #Trade_count reflect REAL executed trades (cooldown/min_trade_delta)\n",
        "            trade_count = int(executed_trade_count)\n",
        "\n",
        "            # Save VecNormalize\n",
        "            try:\n",
        "                env.save(vecnorm_path)\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"Could not save VecNormalize for {ticker} {start}-{end}: {e}\")\n",
        "                vecnorm_path = None\n",
        "\n",
        "            # Save model\n",
        "            model.save(model_path)\n",
        "\n",
        "            # Save detailed predictions\n",
        "            pred_path = os.path.join(RUN_RESULTS_DIR, f\"{prefix}_predictions.csv\")\n",
        "            pd.DataFrame(step_log).to_csv(pred_path, index=False)\n",
        "            logging.info(f\"Saved predictions to {pred_path}\")\n",
        "\n",
        "            # Save compat predictions with same thresholds as metrics\n",
        "            compat_rows = []\n",
        "            for r in step_log:\n",
        "                a = r[\"Action\"]\n",
        "\n",
        "                if a > thr:\n",
        "                    signal = \"BUY\"\n",
        "                elif a < -thr:\n",
        "                    signal = \"SELL\"\n",
        "                else:\n",
        "                    signal = \"HOLD\"\n",
        "                compat_rows.append({\n",
        "                    \"Index\": r[\"Index\"],\n",
        "                    \"Datetime\": r[\"Datetime\"],\n",
        "                    \"Close\": r[\"Close\"],\n",
        "                    \"Action\": a,\n",
        "                    \"Signal\": signal,\n",
        "                    \"PortfolioValue\": r[\"nav\"],\n",
        "                    \"Reward\": r.get(\"reward\", np.nan),\n",
        "                })\n",
        "            compat_path = os.path.join(RUN_RESULTS_DIR, f\"{prefix}_predictions_compat.csv\")\n",
        "            pd.DataFrame(compat_rows).to_csv(compat_path, index=False)\n",
        "            logging.info(f\"Saved compatibility predictions to {compat_path}\")\n",
        "\n",
        "            # Summary row\n",
        "            result_row = {\n",
        "                \"Ticker\": ticker,\n",
        "                \"Window\": f\"{start}-{end}\",\n",
        "                \"WindowIdx\": int(w_idx + 1),\n",
        "                \"Prefix\": prefix,\n",
        "                \"PPO_Portfolio\": round(final_value, 2),\n",
        "                \"BuyHold\": round(hold_value, 2),\n",
        "                \"Sharpe\": round(sharpe, 3),\n",
        "                \"Drawdown_%\": round(drawdown, 2),\n",
        "                \"Winner\": \"PPO\" if final_value > hold_value else \"Buy & Hold\",\n",
        "                \"Action_Threshold\": round(thr, 4),\n",
        "                \"Accuracy\": step_accuracy,\n",
        "                \"Trade_Count\": trade_count,\n",
        "                \"Signal_Trade_Count\": int(signal_trade_count),\n",
        "                \"Signal_Trade_Count_Dyn\": int(signal_trade_count_dyn),\n",
        "                \"Executed_Trade_Count\": int(executed_trade_count),\n",
        "                \"Precision_Long\": round(precision_long, 4),\n",
        "                \"Precision_Short\": round(precision_short, 4),\n",
        "                \"Precision_Trades\": round(precision_trades, 4),\n",
        "            }\n",
        "\n",
        "            results.append(result_row)\n",
        "\n",
        "            meta = {\n",
        "                \"result\": result_row,\n",
        "                \"features\": df_window.columns.tolist(),\n",
        "                \"prefix\": prefix,\n",
        "                \"model_path\": model_path,\n",
        "                \"vecnorm_path\": vecnorm_path,\n",
        "            }\n",
        "\n",
        "            item = (result_row[\"Sharpe\"], prefix, meta)\n",
        "            if len(top_heap) < TOP_N_WINDOWS:\n",
        "                heapq.heappush(top_heap, item)\n",
        "            else:\n",
        "                if item[0] > top_heap[0][0]:\n",
        "                    heapq.heapreplace(top_heap, item)\n",
        "\n",
        "            logging.info(\n",
        "                f\"{ticker} | Window {w_idx+1} runtime: \"\n",
        "                f\"{round(time.time() - window_start_time, 2)}s\"\n",
        "            )\n",
        "        finally:\n",
        "            try:\n",
        "                env.close()\n",
        "            except Exception:\n",
        "                pass\n",
        "            del env\n",
        "            try:\n",
        "                del model\n",
        "            except Exception:\n",
        "                pass\n",
        "            gc.collect()\n",
        "            try:\n",
        "                torch.cuda.empty_cache()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    if skipped_windows:\n",
        "        logging.info(\n",
        "            f\"{ticker} skipped windows (already complete): {', '.join(skipped_windows)}\"\n",
        "        )\n",
        "        record_skips_global(\n",
        "            ticker,\n",
        "            skipped_windows=skipped_windows,\n",
        "            total_windows=len(windows),\n",
        "            fully_skipped=False,\n",
        "        )\n",
        "\n",
        "    # Save top-N QC-compatible\n",
        "    top_list = sorted(top_heap, key=lambda t: t[0], reverse=True)\n",
        "    for _, _, meta in top_list:\n",
        "        artifact_for_save = {\n",
        "            \"model\": None,  # we're copying from disk, not re-saving an in-memory model\n",
        "            \"model_path\": meta[\"model_path\"],\n",
        "            \"vecnorm_path\": meta[\"vecnorm_path\"],\n",
        "            \"features\": meta[\"features\"],\n",
        "            \"result\": meta[\"result\"],\n",
        "            \"prefix\": meta[\"prefix\"],\n",
        "        }\n",
        "        save_quantconnect_model(artifact_for_save, meta[\"prefix\"], QC_TOP_DIR)\n",
        "\n",
        "    return results\n",
        "\n",
        "def process_ticker(ticker):\n",
        "    try:\n",
        "        hp = pick_params(ticker)\n",
        "        return walkforward_ppo(\n",
        "            df[df[\"Symbol\"] == ticker].copy(),\n",
        "            ticker,\n",
        "            window_size=WINDOW_SIZE,\n",
        "            step_size=STEP_SIZE,\n",
        "            timesteps=TIMESTEPS,\n",
        "            learning_rate=hp[\"lr\"],\n",
        "            ppo_overrides=hp,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logging.error(f\"{ticker}: training failed with {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def run_parallel_tickers(tickers,\n",
        "                         out_path=os.path.join(RUN_RESULTS_DIR, \"summary.csv\"),\n",
        "                         max_workers=8):\n",
        "    results = []\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
        "        for res in ex.map(process_ticker, tickers):\n",
        "            if res:\n",
        "                results.extend(res)\n",
        "\n",
        "    if results:\n",
        "        pd.DataFrame(results).to_csv(out_path, index=False)\n",
        "        logging.info(f\"Saved summary to {out_path}\")\n",
        "    else:\n",
        "        logging.warning(\"No results produced; summary not written.\")\n",
        "\n",
        "    logging.info(\"All tickers processed.\")\n",
        "    return results\n",
        "\n",
        "def build_ppo_selector():\n",
        "    \"\"\"Aggregate all summary*.csv across runs and build selector JSON.\"\"\"\n",
        "    summary_files = glob.glob(\n",
        "        os.path.join(BASE_RESULTS_DIR, \"ppo_walkforward_results_*\", \"summary*.csv\")\n",
        "    )\n",
        "    all_summaries = []\n",
        "\n",
        "    for p in summary_files:\n",
        "        try:\n",
        "            tmp = pd.read_csv(p)\n",
        "            tmp[\"RunFolder\"] = os.path.dirname(p)\n",
        "            all_summaries.append(tmp)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Skipping {p} due to error: {e}\")\n",
        "\n",
        "    if not all_summaries:\n",
        "        logging.warning(\"No PPO summaries found across walkforward results folders.\")\n",
        "        return\n",
        "\n",
        "    combo = pd.concat(all_summaries, ignore_index=True)\n",
        "    if \"Sharpe\" in combo.columns:\n",
        "    combo[\"Sharpe\"] = pd.to_numeric(combo[\"Sharpe\"], errors=\"coerce\")\n",
        "    combo = combo.dropna(subset=[\"Sharpe\"])\n",
        "\n",
        "    # Ensure key columns exist for robust ratios\n",
        "    if \"BuyHold\" not in combo.columns:\n",
        "        combo[\"BuyHold\"] = np.nan\n",
        "    if \"PPO_Portfolio\" not in combo.columns:\n",
        "        combo[\"PPO_Portfolio\"] = np.nan\n",
        "\n",
        "    # parse Window \"start-end\" to WindowStart\n",
        "    def _parse_window_start(w):\n",
        "        if pd.isna(w):\n",
        "            return None\n",
        "        if isinstance(w, (int, float)):\n",
        "            return int(w)\n",
        "        parts = str(w).split(\"-\")\n",
        "        try:\n",
        "            return int(parts[0])\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    combo[\"WindowStart\"] = combo[\"Window\"].apply(_parse_window_start)\n",
        "\n",
        "    combo = combo.sort_values([\"Ticker\", \"WindowStart\"]).reset_index(drop=True)\n",
        "    combo[\"WindowIdx\"] = combo.groupby(\"Ticker\").cumcount() + 1\n",
        "\n",
        "    combo = combo.drop_duplicates(subset=[\"Ticker\", \"WindowIdx\"], keep=\"last\")\n",
        "\n",
        "    best_by_symbol = (\n",
        "        combo\n",
        "        .sort_values(\"Sharpe\", ascending=False)\n",
        "        .groupby(\"Ticker\")\n",
        "        .first()\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # If Drawdown_% missing (older runs), create it so rename won't break\n",
        "    if \"Drawdown_%\" not in best_by_symbol.columns:\n",
        "        best_by_symbol[\"Drawdown_%\"] = np.nan\n",
        "\n",
        "    # Ensure precision cols exist\n",
        "    for col in [\"Precision_Long\", \"Precision_Short\", \"Precision_Trades\"]:\n",
        "        if col not in best_by_symbol.columns:\n",
        "            best_by_symbol[col] = None  # or np.nan if you prefer\n",
        "\n",
        "    # Rename columns so everything downstream uses consistent names\n",
        "    best_by_symbol = best_by_symbol.rename(columns={\n",
        "        \"Drawdown_%\": \"Drawdown\",\n",
        "        \"PPO_Portfolio\": \"Final_Portfolio\",\n",
        "    })\n",
        "\n",
        "    # Ensure Accuracy / Trade_Count exist\n",
        "    if \"Accuracy\" not in best_by_symbol.columns:\n",
        "        best_by_symbol[\"Accuracy\"] = 0.0\n",
        "    if \"Trade_Count\" not in best_by_symbol.columns:\n",
        "        best_by_symbol[\"Trade_Count\"] = None\n",
        "\n",
        "    best_by_symbol[\"Model\"] = MODEL_NAME\n",
        "\n",
        "    # PPO vs Buy & Hold ratio (safe division)\n",
        "    best_by_symbol[\"Rel_vs_BH\"] = best_by_symbol.apply(\n",
        "        lambda r: (r[\"Final_Portfolio\"] / r[\"BuyHold\"])\n",
        "        if (pd.notna(r[\"BuyHold\"]) and r[\"BuyHold\"] not in (0, 0.0)) else np.nan,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Save flat CSV for debugging\n",
        "    best_by_symbol.to_csv(SELECTOR_FULL_PATH, index=False)\n",
        "    print(f\"Aggregated PPO selector saved to → {SELECTOR_FULL_PATH}\")\n",
        "\n",
        "    # Safety filters (tune as needed)\n",
        "    df_sel = best_by_symbol.copy()\n",
        "    gates = (\n",
        "        (df_sel[\"Sharpe\"].fillna(-999) > 0.0) &\n",
        "        (df_sel[\"Drawdown\"].fillna(999) < 50.0) &\n",
        "        (df_sel[\"Final_Portfolio\"].fillna(0) > 80_000) &\n",
        "        (df_sel[\"Rel_vs_BH\"].fillna(0) >= 0.95)   # PPO ≥ 95% of B&H; change to >1.0 to enforce beat\n",
        "    )\n",
        "    df_sel = df_sel[gates].copy()\n",
        "\n",
        "    df_sel[\"prefix\"] = (\n",
        "        \"ppo_\"\n",
        "        + df_sel[\"Ticker\"].astype(str)\n",
        "        + \"_window\"\n",
        "        + df_sel[\"WindowIdx\"].astype(int).astype(str)\n",
        "    )\n",
        "\n",
        "    df_sel[\"artifact_path\"] = df_sel[\"prefix\"].apply(\n",
        "        lambda p: os.path.join(FINAL_MODEL_DIR, f\"{p}_model.zip\")\n",
        "    )\n",
        "    df_sel[\"vecnorm_path\"] = df_sel[\"prefix\"].apply(\n",
        "        lambda p: os.path.join(FINAL_MODEL_DIR, f\"{p}_vecnorm.pkl\")\n",
        "    )\n",
        "\n",
        "    EPS = 0.03  # 3% of top-sharpe for \"close enough\"\n",
        "    selected_models = {}\n",
        "\n",
        "    def safe_int(v, default=0):\n",
        "        if v is None:\n",
        "            return int(default)\n",
        "        try:\n",
        "            import math\n",
        "            if isinstance(v, float) and math.isnan(v):\n",
        "                return int(default)\n",
        "        except TypeError:\n",
        "            pass\n",
        "        try:\n",
        "            return int(v)\n",
        "        except (ValueError, TypeError):\n",
        "            return int(default)\n",
        "\n",
        "    def safe_float(v, default=0.0):\n",
        "        if v is None:\n",
        "            return float(default)\n",
        "        try:\n",
        "            import math\n",
        "            if isinstance(v, float) and math.isnan(v):\n",
        "                return float(default)\n",
        "        except TypeError:\n",
        "            pass\n",
        "        try:\n",
        "            return float(v)\n",
        "        except (ValueError, TypeError):\n",
        "            return float(default)\n",
        "\n",
        "    for ticker, group in df_sel.groupby(\"Ticker\"):\n",
        "        group_sorted = group.sort_values(\"Sharpe\", ascending=False)\n",
        "        top = group_sorted.iloc[0]\n",
        "        second = group_sorted.iloc[1] if len(group_sorted) > 1 else None\n",
        "\n",
        "        if (second is not None) and (\n",
        "            abs(top[\"Sharpe\"] - second[\"Sharpe\"]) <= abs(top[\"Sharpe\"]) * EPS\n",
        "        ):\n",
        "            mode = \"ensemble\"\n",
        "            primary, secondary = top[\"Model\"], second[\"Model\"]\n",
        "        else:\n",
        "            mode = \"single\"\n",
        "            primary, secondary = top[\"Model\"], None\n",
        "\n",
        "        selected_models[ticker] = {\n",
        "            \"model\": MODEL_NAME,\n",
        "            \"score\": round(safe_float(top[\"Sharpe\"]), 4),\n",
        "            \"return\": round(safe_float(top[\"Final_Portfolio\"]), 2),\n",
        "            \"sharpe\": round(safe_float(top[\"Sharpe\"]), 3),\n",
        "            \"drawdown\": round(safe_float(top[\"Drawdown\"]), 2),\n",
        "            \"sortino\": None,\n",
        "            \"turnover\": None,\n",
        "            \"trade_count\": safe_int(top.get(\"Trade_Count\", 0)),\n",
        "            \"precision\": {\n",
        "                \"long\":   safe_float(top.get(\"Precision_Long\", 0.0)),\n",
        "                \"short\":  safe_float(top.get(\"Precision_Short\", 0.0)),\n",
        "                \"trades\": safe_float(top.get(\"Precision_Trades\", 0.0)),\n",
        "            },\n",
        "            \"stability\": {},\n",
        "            \"regime\": \"unknown\",\n",
        "            \"rl_profile\": \"fast\",\n",
        "            \"artifact\": {\n",
        "                \"path\": top[\"artifact_path\"],\n",
        "                \"vecnorm\": top[\"vecnorm_path\"],\n",
        "                \"features\": None,\n",
        "                \"load_ms\": 180,\n",
        "                \"mem_mb\": 512,\n",
        "                \"exists\": os.path.exists(top[\"artifact_path\"]),\n",
        "            },\n",
        "            \"selection\": {\n",
        "                \"mode\": mode,\n",
        "                \"primary\": primary,\n",
        "                \"secondary\": secondary,\n",
        "            },\n",
        "        }\n",
        "\n",
        "    with open(SELECTOR_JSON_PATH, \"w\") as f:\n",
        "        json.dump(selected_models, f, indent=2)\n",
        "\n",
        "    print(f\"Final enhanced PPO selector JSON saved to → {SELECTOR_JSON_PATH}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.info(f\"RUN_RESULTS_DIR   = {RUN_RESULTS_DIR}\")\n",
        "    logging.info(f\"FINAL_MODEL_DIR  = {FINAL_MODEL_DIR}\")\n",
        "    logging.info(f\"BASE_RESULTS_DIR = {BASE_RESULTS_DIR}\")\n",
        "\n",
        "    min_rows = WINDOW_SIZE + 50  # small buffer so we have at least one window\n",
        "    all_symbols = df[\"Symbol\"].value_counts()\n",
        "    candidate_symbols = []\n",
        "\n",
        "    for sym, n in all_symbols.items():\n",
        "        if n >= min_rows:\n",
        "            candidate_symbols.append(sym)\n",
        "        else:\n",
        "            logging.warning(f\"Skipping {sym}: only {n} rows (< {min_rows} required)\")\n",
        "\n",
        "    if not candidate_symbols:\n",
        "        logging.error(\"No symbols have enough rows for the current WINDOW_SIZE. Nothing to train.\")\n",
        "    else:\n",
        "        logging.info(f\"Training candidate symbols: {candidate_symbols}\")\n",
        "\n",
        "    needed_cols = [\"Close\", \"Datetime\"]\n",
        "    if ENABLE_WAVELET:\n",
        "        needed_cols.append(\"Denoised_Close\")\n",
        "    if ENABLE_SENTIMENT:\n",
        "        needed_cols.append(\"SentimentScore\")\n",
        "\n",
        "    valid_symbols = []\n",
        "    for sym in candidate_symbols:\n",
        "        cols = set(df.loc[df[\"Symbol\"] == sym].columns)\n",
        "        missing = [c for c in needed_cols if c not in cols]\n",
        "        if missing:\n",
        "            logging.warning(f\"Skipping {sym}: missing required cols {missing}\")\n",
        "        else:\n",
        "            valid_symbols.append(sym)\n",
        "\n",
        "    if not valid_symbols:\n",
        "        logging.error(\"No symbols passed the feature/column checks. Nothing to train.\")\n",
        "    else:\n",
        "        logging.info(f\"Final training universe: {valid_symbols}\")\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    if test_mode:\n",
        "        # Optional: shrink timesteps and/or window size in test mode\n",
        "        TIMESTEPS = 100_000   # lighter test\n",
        "        # WINDOW_SIZE = 2000  # uncomment if you want faster test runs\n",
        "        # STEP_SIZE   = 500\n",
        "\n",
        "        test_stocks = [\"AAPL\", \"NVDA\", \"MSFT\"]\n",
        "        present = [s for s in test_stocks if s in valid_symbols]\n",
        "        if not present:\n",
        "            logging.warning(\"Test mode: none of ['AAPL','NVDA','MSFT'] present after filters.\")\n",
        "        else:\n",
        "            logging.info(f\"Test mode: running on {present}\")\n",
        "\n",
        "        for sym in present:\n",
        "            logging.info(f\">>> [TEST_MODE] Processing {sym}\")\n",
        "            res = process_ticker(sym)\n",
        "            logging.info(f\"{sym}: produced {len(res)} window summaries\")\n",
        "            if res:\n",
        "                all_results.extend(res)\n",
        "\n",
        "        summary_path = os.path.join(RUN_RESULTS_DIR, \"summary_test_mode.csv\")\n",
        "        if all_results:\n",
        "            pd.DataFrame(all_results).to_csv(summary_path, index=False)\n",
        "            logging.info(f\"Test-mode summary saved to {summary_path}\")\n",
        "        else:\n",
        "            logging.warning(\"Test mode finished but no results were generated (no windows, or all skipped).\")\n",
        "\n",
        "    else:\n",
        "        logging.info(\"Starting full parallel PPO walkforward run...\")\n",
        "        summary_results = run_parallel_tickers(valid_symbols)\n",
        "        if not summary_results:\n",
        "            logging.warning(\"No results generated in full run (check logs for skips/length issues).\")\n",
        "        else:\n",
        "            summary_path = os.path.join(RUN_RESULTS_DIR, \"summary.csv\")\n",
        "            pd.DataFrame(summary_results).to_csv(summary_path, index=False)\n",
        "            logging.info(f\"Summary saved to {summary_path}\")\n",
        "\n",
        "    try:\n",
        "        build_ppo_selector()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"build_ppo_selector failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5c480aa-48e3-414a-cb54-ddb8a0cd1751",
        "id": "nuQLTzY7iqna"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "2025-12-26 00:44:30,958 - INFO - RUN_RESULTS_DIR   = /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044\n",
            "2025-12-26 00:44:30,960 - INFO - FINAL_MODEL_DIR  = /content/drive/MyDrive/Results_May_2025/ppo_models_master\n",
            "2025-12-26 00:44:30,961 - INFO - BASE_RESULTS_DIR = /content/drive/MyDrive/Results_May_2025\n",
            "2025-12-26 00:44:31,078 - INFO - Training candidate symbols: ['AAPL', 'QCOM', 'COST', 'RTX', 'BRK-B', 'SBUX', 'TMO', 'BAC', 'TSLA', 'CRM', 'AVGO', 'TXN', 'UNH', 'ADBE', 'AMGN', 'UNP', 'AMD', 'ACN', 'AMZN', 'ABT', 'WMT', 'ABBV', 'XOM', 'PFE', 'PM', 'CSCO', 'PG', 'LLY', 'MA', 'MCD', 'META', 'MRK', 'JPM', 'MSFT', 'NVDA', 'NKE', 'INTC', 'LOW', 'CVX', 'PEP', 'HD', 'GOOGL', 'ORCL', 'GE', 'BMY', 'JNJ', 'NEE', 'LIN', 'KO', 'V', 'UPS', 'DHR', 'IBM']\n",
            "2025-12-26 00:44:36,459 - INFO - Final training universe: ['AAPL', 'QCOM', 'COST', 'RTX', 'BRK-B', 'SBUX', 'TMO', 'BAC', 'TSLA', 'CRM', 'AVGO', 'TXN', 'UNH', 'ADBE', 'AMGN', 'UNP', 'AMD', 'ACN', 'AMZN', 'ABT', 'WMT', 'ABBV', 'XOM', 'PFE', 'PM', 'CSCO', 'PG', 'LLY', 'MA', 'MCD', 'META', 'MRK', 'JPM', 'MSFT', 'NVDA', 'NKE', 'INTC', 'LOW', 'CVX', 'PEP', 'HD', 'GOOGL', 'ORCL', 'GE', 'BMY', 'JNJ', 'NEE', 'LIN', 'KO', 'V', 'UPS', 'DHR', 'IBM']\n",
            "2025-12-26 00:44:36,461 - INFO - Test mode: running on ['AAPL', 'NVDA', 'MSFT']\n",
            "2025-12-26 00:44:36,462 - INFO - >>> [TEST_MODE] Processing AAPL\n",
            "2025-12-26 00:44:37,024 - INFO - Will train AAPL | Window 1 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:44:48,713 - INFO - Training AAPL Window 1/3\n",
            "2025-12-26 00:47:22,129 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window1_predictions.csv\n",
            "2025-12-26 00:47:22,205 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window1_predictions_compat.csv\n",
            "2025-12-26 00:47:22,206 - INFO - AAPL | Window 1 runtime: 165.69s\n",
            "2025-12-26 00:47:23,014 - INFO - Will train AAPL | Window 2 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:47:23,024 - INFO - Training AAPL Window 2/3\n",
            "2025-12-26 00:49:38,624 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window2_predictions.csv\n",
            "2025-12-26 00:49:38,718 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window2_predictions_compat.csv\n",
            "2025-12-26 00:49:38,719 - INFO - AAPL | Window 2 runtime: 136.07s\n",
            "2025-12-26 00:49:39,501 - INFO - Will train AAPL | Window 3 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:49:39,509 - INFO - Training AAPL Window 3/3\n",
            "2025-12-26 00:51:55,863 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window3_predictions.csv\n",
            "2025-12-26 00:51:55,987 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window3_predictions_compat.csv\n",
            "2025-12-26 00:51:55,990 - INFO - AAPL | Window 3 runtime: 136.86s\n",
            "2025-12-26 00:51:56,549 - INFO - [QC SAVE] Saved QC artifacts for ppo_AAPL_window3\n",
            "2025-12-26 00:51:56,576 - INFO - [QC SAVE] Saved QC artifacts for ppo_AAPL_window2\n",
            "2025-12-26 00:51:56,603 - INFO - [QC SAVE] Saved QC artifacts for ppo_AAPL_window1\n",
            "2025-12-26 00:51:56,610 - INFO - AAPL: produced 3 window summaries\n",
            "2025-12-26 00:51:56,612 - INFO - >>> [TEST_MODE] Processing NVDA\n",
            "2025-12-26 00:51:57,183 - INFO - Will train NVDA | Window 1 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:51:57,193 - INFO - Training NVDA Window 1/3\n",
            "2025-12-26 00:54:20,438 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window1_predictions.csv\n",
            "2025-12-26 00:54:20,643 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window1_predictions_compat.csv\n",
            "2025-12-26 00:54:20,645 - INFO - NVDA | Window 1 runtime: 143.94s\n",
            "2025-12-26 00:54:22,250 - INFO - Will train NVDA | Window 2 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:54:22,263 - INFO - Training NVDA Window 2/3\n",
            "2025-12-26 00:56:48,511 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window2_predictions.csv\n",
            "2025-12-26 00:56:48,590 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window2_predictions_compat.csv\n",
            "2025-12-26 00:56:48,591 - INFO - NVDA | Window 2 runtime: 147.14s\n",
            "2025-12-26 00:56:49,383 - INFO - Will train NVDA | Window 3 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:56:49,391 - INFO - Training NVDA Window 3/3\n",
            "2025-12-26 00:59:11,463 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window3_predictions.csv\n",
            "2025-12-26 00:59:11,547 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window3_predictions_compat.csv\n",
            "2025-12-26 00:59:11,548 - INFO - NVDA | Window 3 runtime: 142.54s\n",
            "2025-12-26 00:59:12,044 - INFO - [QC SAVE] Saved QC artifacts for ppo_NVDA_window1\n",
            "2025-12-26 00:59:12,081 - INFO - [QC SAVE] Saved QC artifacts for ppo_NVDA_window2\n",
            "2025-12-26 00:59:12,120 - INFO - [QC SAVE] Saved QC artifacts for ppo_NVDA_window3\n",
            "2025-12-26 00:59:12,127 - INFO - NVDA: produced 3 window summaries\n",
            "2025-12-26 00:59:12,127 - INFO - >>> [TEST_MODE] Processing MSFT\n",
            "2025-12-26 00:59:12,587 - INFO - Will train MSFT | Window 1 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:59:12,597 - INFO - Training MSFT Window 1/3\n",
            "2025-12-26 01:01:29,748 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window1_predictions.csv\n",
            "2025-12-26 01:01:29,862 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window1_predictions_compat.csv\n",
            "2025-12-26 01:01:29,865 - INFO - MSFT | Window 1 runtime: 137.7s\n",
            "2025-12-26 01:01:30,841 - INFO - Will train MSFT | Window 2 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 01:01:30,852 - INFO - Training MSFT Window 2/3\n",
            "2025-12-26 01:03:53,747 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window2_predictions.csv\n",
            "2025-12-26 01:03:53,877 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window2_predictions_compat.csv\n",
            "2025-12-26 01:03:53,881 - INFO - MSFT | Window 2 runtime: 143.51s\n",
            "2025-12-26 01:03:55,045 - INFO - Will train MSFT | Window 3 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 01:03:55,057 - INFO - Training MSFT Window 3/3\n",
            "2025-12-26 01:06:09,439 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window3_predictions.csv\n",
            "2025-12-26 01:06:09,522 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window3_predictions_compat.csv\n",
            "2025-12-26 01:06:09,523 - INFO - MSFT | Window 3 runtime: 134.92s\n",
            "2025-12-26 01:06:09,985 - INFO - [QC SAVE] Saved QC artifacts for ppo_MSFT_window2\n",
            "2025-12-26 01:06:10,025 - INFO - [QC SAVE] Saved QC artifacts for ppo_MSFT_window1\n",
            "2025-12-26 01:06:10,086 - INFO - [QC SAVE] Saved QC artifacts for ppo_MSFT_window3\n",
            "2025-12-26 01:06:10,093 - INFO - MSFT: produced 3 window summaries\n",
            "2025-12-26 01:06:10,108 - INFO - Test-mode summary saved to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/summary_test_mode.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aggregated PPO selector saved to → /content/drive/MyDrive/Results_May_2025/ppo_model_selector_FULL.csv\n",
            "Final enhanced PPO selector JSON saved to → /content/drive/MyDrive/Results_May_2025/ppo_model_selector_final.json\n"
          ]
        }
      ],
      "source": [
        "# PPO walkforward training + selector\n",
        "import os, gc, time, json, logging, glob\n",
        "import shutil\n",
        "from threading import Lock\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt  # optional for ad-hoc plots\n",
        "\n",
        "import torch\n",
        "import gymnasium as gym\n",
        "from gymnasium.spaces import Box as GBox\n",
        "\n",
        "import yfinance as yf\n",
        "from gym_anytrading.envs import StocksEnv\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"gymnasium\")\n",
        "# ---- Sharpe annualization helper (intraday heuristic: 6.5 hrs * 252) ----\n",
        "def _annualization_factor(_df_like=None) -> float:\n",
        "    \"\"\"Annualization factor for intraday bars (6.5 trading hours × 252 days).\"\"\"\n",
        "    return np.sqrt(252 * 6.5)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"jupyter_client.session\")\n",
        "warnings.filterwarnings(\"ignore\", message=\".*Gym has been unmaintained.*\")\n",
        "\n",
        "try:\n",
        "    compute_enhanced_features  # type: ignore\n",
        "except NameError:\n",
        "    def compute_enhanced_features(df_in: pd.DataFrame) -> pd.DataFrame:\n",
        "        df_out = df_in.copy()\n",
        "        if \"Datetime\" in df_out.columns:\n",
        "            df_out[\"Datetime\"] = pd.to_datetime(df_out[\"Datetime\"])\n",
        "            df_out = df_out.sort_values(\"Datetime\").reset_index(drop=True)\n",
        "        if \"Close\" not in df_out.columns:\n",
        "            raise ValueError(\"compute_enhanced_features: missing required column 'Close'\")\n",
        "        return df_out\n",
        "\n",
        "set_random_seed(42)\n",
        "\n",
        "BASE_RESULTS_DIR = \"/content/drive/MyDrive/Results_May_2025\"\n",
        "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "RUN_RESULTS_DIR = os.path.join(BASE_RESULTS_DIR, f\"ppo_walkforward_results_{RUN_TAG}\")\n",
        "FINAL_MODEL_DIR = os.path.join(BASE_RESULTS_DIR, \"ppo_models_master\")\n",
        "QC_TOP_DIR      = os.path.join(BASE_RESULTS_DIR, \"ppo_models_QC_TOP\")\n",
        "\n",
        "os.makedirs(QC_TOP_DIR, exist_ok=True)\n",
        "os.makedirs(RUN_RESULTS_DIR, exist_ok=True)\n",
        "os.makedirs(FINAL_MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# Aggregated selector outputs\n",
        "SELECTOR_FULL_PATH = os.path.join(BASE_RESULTS_DIR, \"ppo_model_selector_FULL.csv\")\n",
        "SELECTOR_JSON_PATH = os.path.join(BASE_RESULTS_DIR, \"ppo_model_selector_final.json\")\n",
        "MODEL_NAME = \"PPO\"\n",
        "\n",
        "# Global skip aggregation (thread-safe)\n",
        "SKIP_AGG_PATH = os.path.join(RUN_RESULTS_DIR, \"skipped_windows_global.csv\")\n",
        "SKIP_LOCK = Lock()\n",
        "\n",
        "# Logging Setup\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    force=True\n",
        ")\n",
        "\n",
        "# Flags\n",
        "ENABLE_SENTIMENT = False\n",
        "ENABLE_SLO       = True\n",
        "ENABLE_WAVELET   = True\n",
        "test_mode        = True            # set False for full universe\n",
        "ENABLE_PLOTS     = False\n",
        "LIVE_MODE        = False           # set True to run simple live/paper loop\n",
        "SIM_LATENCY_MS   = 0               # broker latency simulation; 0 = off\n",
        "BROKER           = \"log\"           # \"log\" = do not place orders, just log\n",
        "\n",
        "# Global training settings\n",
        "WINDOW_SIZE = 3500\n",
        "STEP_SIZE   = 500\n",
        "TIMESTEPS   = 150_000  # overridden in test_mode block to smaller value\n",
        "\n",
        "\n",
        "DATA_PATH = \"multi_stock_feature_engineered_dataset.csv\"\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    raise FileNotFoundError(\"Required feature-engineered dataset not found!\")\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "df[\"Datetime\"] = pd.to_datetime(df[\"Datetime\"])\n",
        "\n",
        "# Wavelet fallback\n",
        "if ENABLE_WAVELET and \"Denoised_Close\" not in df.columns:\n",
        "    logging.warning(\"ENABLE_WAVELET=True but 'Denoised_Close' missing; \"\n",
        "                    \"falling back to Close->Denoised_Close.\")\n",
        "    df[\"Denoised_Close\"] = df[\"Close\"]\n",
        "\n",
        "\n",
        "def record_skips_global(ticker: str, skipped_windows: list,\n",
        "                        total_windows: int = None, fully_skipped: bool = False):\n",
        "    \"\"\"Append skipped windows to the global skip log.\"\"\"\n",
        "    if not skipped_windows and not fully_skipped:\n",
        "        return\n",
        "    import csv\n",
        "    with SKIP_LOCK:\n",
        "        new_file = not os.path.exists(SKIP_AGG_PATH)\n",
        "        with open(SKIP_AGG_PATH, \"a\", newline=\"\") as f:\n",
        "            w = csv.writer(f)\n",
        "            if new_file:\n",
        "                w.writerow([\"Ticker\", \"Window\", \"FullySkipped\", \"TotalWindows\"])\n",
        "            if fully_skipped:\n",
        "                w.writerow([ticker, \"ALL\", True, total_windows if total_windows is not None else \"\"])\n",
        "            else:\n",
        "                for wname in skipped_windows:\n",
        "                    try:\n",
        "                        _, win_str = wname.split(\"_window\")\n",
        "                        win = int(win_str)\n",
        "                    except Exception:\n",
        "                        win = \"\"\n",
        "                    w.writerow([ticker, win, False, total_windows if total_windows is not None else \"\"])\n",
        "\n",
        "\n",
        "ENV_KWARGS = dict(\n",
        "    window_size=10,\n",
        "    cost_rate=0.0002,\n",
        "    slip_rate=0.0003,\n",
        "\n",
        "    k_alpha=0.0,\n",
        "    k_mom=0.15,\n",
        "    k_sent=(0.01 if ENABLE_SENTIMENT else 0.0),\n",
        "    mom_source=\"denoised\",\n",
        "    mom_lookback=20,\n",
        "\n",
        "    min_trade_delta=0.08,\n",
        "    cooldown=10,\n",
        "\n",
        "    reward_clip=0.05,\n",
        "    k_vol=0.00,\n",
        "    k_dd=0.00,\n",
        ")\n",
        "\n",
        "\n",
        "class ContinuousPositionEnv(StocksEnv):\n",
        "    def __init__(self, df, frame_bound, **kwargs):\n",
        "        # Require window_size from ENV_KWARGS\n",
        "        if \"window_size\" not in kwargs:\n",
        "            raise ValueError(\"ContinuousPositionEnv requires window_size (pass via ENV_KWARGS).\")\n",
        "\n",
        "        window_size = int(kwargs.pop(\"window_size\"))\n",
        "\n",
        "        # Pull params (all defaults live in ENV_KWARGS; these are just safety fallbacks)\n",
        "        cost_rate       = float(kwargs.pop(\"cost_rate\", 0.0002))\n",
        "        slip_rate       = float(kwargs.pop(\"slip_rate\", 0.0003))\n",
        "        k_alpha         = float(kwargs.pop(\"k_alpha\", 0.0))\n",
        "        k_mom           = float(kwargs.pop(\"k_mom\", 0.15))\n",
        "        k_sent          = float(kwargs.pop(\"k_sent\", 0.0))\n",
        "        mom_source      = str(kwargs.pop(\"mom_source\", \"denoised\"))\n",
        "        mom_lookback    = int(kwargs.pop(\"mom_lookback\", 20))\n",
        "        min_trade_delta = float(kwargs.pop(\"min_trade_delta\", 0.04))\n",
        "        cooldown        = int(kwargs.pop(\"cooldown\", 6))\n",
        "        reward_clip     = float(kwargs.pop(\"reward_clip\", 0.05))\n",
        "        k_vol           = float(kwargs.pop(\"k_vol\", 0.0))\n",
        "        k_dd            = float(kwargs.pop(\"k_dd\", 0.0))\n",
        "\n",
        "        # Fail fast on unexpected env kwargs\n",
        "        if kwargs:\n",
        "            raise ValueError(f\"Unexpected env kwargs: {list(kwargs.keys())}\")\n",
        "\n",
        "        super().__init__(\n",
        "            df=df.reset_index(drop=True),\n",
        "            frame_bound=frame_bound,\n",
        "            window_size=window_size\n",
        "        )\n",
        "\n",
        "        if isinstance(self.observation_space, gym.spaces.Box):\n",
        "            self.observation_space = GBox(\n",
        "                low=self.observation_space.low,\n",
        "                high=self.observation_space.high,\n",
        "                shape=self.observation_space.shape,\n",
        "                dtype=self.observation_space.dtype,\n",
        "            )\n",
        "\n",
        "        self.k_vol = k_vol\n",
        "        self.k_dd  = k_dd\n",
        "\n",
        "        self.ret_history = []\n",
        "        self.nav_history = []\n",
        "        self.peak_nav    = 1.0\n",
        "        self.trade_count = 0\n",
        "\n",
        "        self.action_space = GBox(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)\n",
        "\n",
        "\n",
        "        self.cost_rate       = cost_rate\n",
        "        self.slip_rate       = slip_rate\n",
        "        self.k_alpha         = k_alpha\n",
        "        self.k_mom           = k_mom\n",
        "        self.k_sent          = k_sent\n",
        "        self.mom_source      = mom_source\n",
        "        self.mom_lookback    = mom_lookback\n",
        "        self.min_trade_delta = min_trade_delta\n",
        "        self.cooldown        = cooldown\n",
        "        self.reward_clip     = reward_clip\n",
        "\n",
        "        self.nav = 1.0\n",
        "        self.pos = 0.0\n",
        "        self._last_trade_step = -self.cooldown\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        out = super().reset(**kwargs)\n",
        "        if isinstance(out, tuple):\n",
        "            obs, info = out\n",
        "        else:\n",
        "            obs, info = out, {}\n",
        "\n",
        "        self.nav = 1.0\n",
        "        self.pos = 0.0\n",
        "        self._last_trade_step = -self.cooldown\n",
        "\n",
        "        self.trade_count = 0\n",
        "        self.ret_history = []\n",
        "        self.nav_history = [self.nav]\n",
        "        self.peak_nav    = self.nav\n",
        "\n",
        "        info = info or {}\n",
        "        info.update({\n",
        "            \"nav\": self.nav,\n",
        "            \"pos\": self.pos,\n",
        "            \"trade_count\": int(self.trade_count),\n",
        "\n",
        "        })\n",
        "        return obs, info\n",
        "\n",
        "    def _step_parent_hold(self):\n",
        "        step_result = super().step(2)\n",
        "        if len(step_result) == 5:\n",
        "            obs, _env_rew, terminated, truncated, info = step_result\n",
        "        else:\n",
        "            obs, _env_rew, done, info = step_result\n",
        "            terminated, truncated = bool(done), False\n",
        "        return obs, terminated, truncated, info\n",
        "\n",
        "    def _ret_t(self):\n",
        "        cur  = float(self.df.loc[self._current_tick, \"Close\"])\n",
        "        prev = float(self.df.loc[max(self._current_tick - 1, 0), \"Close\"])\n",
        "        return 0.0 if prev <= 0 else (cur - prev) / prev\n",
        "\n",
        "    def _mom_signal(self):\n",
        "        if self.mom_source == \"macd\" and \"MACD_Line\" in self.df.columns:\n",
        "            recent = self.df[\"MACD_Line\"].iloc[max(self._current_tick - 200, 0):self._current_tick + 1]\n",
        "            return float(np.tanh(\n",
        "                float(self.df.loc[self._current_tick, \"MACD_Line\"]) /\n",
        "                (1e-6 + float(recent.std()))\n",
        "            ))\n",
        "\n",
        "        if \"Denoised_Close\" in self.df.columns and self._current_tick - self.mom_lookback >= 0:\n",
        "            now  = float(self.df.loc[self._current_tick, \"Denoised_Close\"])\n",
        "            then = float(self.df.loc[self._current_tick - self.mom_lookback, \"Denoised_Close\"])\n",
        "            base = float(self.df.loc[max(self._current_tick - 1, 0), \"Close\"])\n",
        "            slope = (now - then) / max(self.mom_lookback, 1)\n",
        "            return float(np.tanh(10.0 * (slope / max(abs(base), 1e-6))))\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "    def step(self, action):\n",
        "        a = float(np.array(action).squeeze())\n",
        "        target_pos = float(np.clip(a, -1.0, 1.0))\n",
        "\n",
        "        r_t = self._ret_t()\n",
        "        base_ret = self.pos * r_t\n",
        "\n",
        "        changed = (\n",
        "            abs(target_pos - self.pos) >= self.min_trade_delta\n",
        "        ) and (\n",
        "            (self._current_tick - self._last_trade_step) >= self.cooldown\n",
        "        )\n",
        "\n",
        "        delta_pos = (target_pos - self.pos) if changed else 0.0\n",
        "        trade_cost = (self.cost_rate + self.slip_rate) * abs(delta_pos)\n",
        "\n",
        "        rel_alpha = base_ret - r_t\n",
        "        mom_term = self.pos * self._mom_signal()\n",
        "\n",
        "        alpha_term = self.k_alpha * rel_alpha\n",
        "\n",
        "        sent_term = 0.0\n",
        "        if ENABLE_SENTIMENT and \"SentimentScore\" in self.df.columns:\n",
        "            sent_term = self.k_sent * float(self.df.loc[self._current_tick, \"SentimentScore\"])\n",
        "\n",
        "        shaped = base_ret + alpha_term + (self.k_mom * mom_term) + sent_term - trade_cost\n",
        "        reward = float(np.clip(shaped, -self.reward_clip, self.reward_clip))\n",
        "\n",
        "\n",
        "        self.nav *= (1.0 + base_ret - trade_cost)\n",
        "        self.nav_history.append(self.nav)\n",
        "        self.peak_nav = max(self.peak_nav, self.nav)\n",
        "\n",
        "        executed_trade = False\n",
        "        if changed:\n",
        "            self.pos = target_pos\n",
        "            self._last_trade_step = self._current_tick\n",
        "            self.trade_count += 1\n",
        "            executed_trade = True\n",
        "\n",
        "        obs, terminated, truncated, info = self._step_parent_hold()\n",
        "        info = info or {}\n",
        "        info.update({\n",
        "            \"ret_t\": r_t,\n",
        "            \"nav\": self.nav,\n",
        "            \"pos\": self.pos,\n",
        "            \"trade_cost\": trade_cost,\n",
        "            \"base_ret\": base_ret,\n",
        "            \"rel_alpha\": rel_alpha,\n",
        "            \"mom\": mom_term,\n",
        "            \"changed\": bool(changed),\n",
        "            \"executed_trade\": bool(executed_trade),\n",
        "            \"trade_count\": int(self.trade_count),\n",
        "            \"delta_pos\": float(delta_pos),\n",
        "        })\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "def get_mu_sigma(model, obs):\n",
        "    \"\"\"SB3 v2-safe way to get Gaussian policy mean/std for continuous actions.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        obs_t, _ = model.policy.obs_to_tensor(obs)\n",
        "        features = model.policy.extract_features(obs_t)\n",
        "        latent_pi, _ = model.policy.mlp_extractor(features)\n",
        "        mean_actions = model.policy.action_net(latent_pi)\n",
        "        log_std = model.policy.log_std\n",
        "        mu = float(mean_actions.detach().cpu().numpy().squeeze())\n",
        "        sigma = float(log_std.exp().detach().cpu().numpy().squeeze())\n",
        "    return mu, sigma\n",
        "\n",
        "def get_walk_forward_windows(df_in, window_size=3500, step_size=500, min_len=1200):\n",
        "    return [\n",
        "        (start, start + window_size)\n",
        "        for start in range(0, len(df_in) - min_len, step_size)\n",
        "        if start + window_size < len(df_in)\n",
        "    ]\n",
        "\n",
        "def save_quantconnect_model(artifact, prefix, save_dir):\n",
        "    \"\"\"Save/copy QC-compatible artifacts into save_dir.\"\"\"\n",
        "    import shutil\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # --- Model zip: save or copy ---\n",
        "    model_dst = os.path.join(save_dir, f\"{prefix}_model.zip\")\n",
        "\n",
        "    model_obj = artifact.get(\"model\", None)\n",
        "    model_src = artifact.get(\"model_path\", None)\n",
        "\n",
        "    try:\n",
        "        if model_obj is not None:\n",
        "            # Save from in-memory SB3 model\n",
        "            if not os.path.exists(model_dst):\n",
        "                model_obj.save(model_dst)\n",
        "\n",
        "        else:\n",
        "            # Copy from an existing trained window model zip\n",
        "            if model_src and os.path.exists(model_src):\n",
        "                if os.path.abspath(model_src) != os.path.abspath(model_dst):\n",
        "                    shutil.copyfile(model_src, model_dst)\n",
        "            else:\n",
        "                # If neither provided, warn loudly\n",
        "                if not os.path.exists(model_dst):\n",
        "                    logging.warning(f\"[QC SAVE] Missing model for {prefix}: no model_obj and no valid model_path.\")\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[QC SAVE] Model handling issue for {prefix}: {e}\")\n",
        "\n",
        "    # --- VecNormalize: copy ---\n",
        "    vecnorm_src = artifact.get(\"vecnorm_path\")\n",
        "    if vecnorm_src and os.path.exists(vecnorm_src):\n",
        "        try:\n",
        "            vecnorm_dst = os.path.join(save_dir, f\"{prefix}_vecnorm.pkl\")\n",
        "            if os.path.abspath(vecnorm_src) != os.path.abspath(vecnorm_dst):\n",
        "                shutil.copyfile(vecnorm_src, vecnorm_dst)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"[QC SAVE] VecNormalize handling issue for {prefix}: {e}\")\n",
        "    else:\n",
        "        logging.warning(f\"[QC SAVE] VecNormalize missing for {prefix}: vecnorm_path not found.\")\n",
        "\n",
        "    # --- Features ---\n",
        "    try:\n",
        "        with open(os.path.join(save_dir, f\"{prefix}_features.json\"), \"w\") as f:\n",
        "            json.dump({\"features\": artifact.get(\"features\", [])}, f)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[QC SAVE] Could not write features.json for {prefix}: {e}\")\n",
        "\n",
        "    # --- Probability config ---\n",
        "    try:\n",
        "        thr = 0.2\n",
        "        try:\n",
        "            thr = float(artifact.get(\"result\", {}).get(\"Action_Threshold\", 0.2))\n",
        "        except Exception:\n",
        "            thr = 0.2\n",
        "\n",
        "        with open(os.path.join(save_dir, f\"{prefix}_probability_config.json\"), \"w\") as f:\n",
        "            json.dump(\n",
        "                {\"threshold\": thr, \"use_confidence\": True, \"inference_mode\": \"deterministic\"},\n",
        "                f\n",
        "            )\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[QC SAVE] Could not write probability_config.json for {prefix}: {e}\")\n",
        "\n",
        "\n",
        "    # --- Model info ---\n",
        "    try:\n",
        "        r = artifact.get(\"result\", {})\n",
        "        with open(os.path.join(save_dir, f\"{prefix}_model_info.json\"), \"w\") as f:\n",
        "            json.dump({\n",
        "                \"model\": \"PPO\",\n",
        "                \"ticker\": r.get(\"Ticker\"),\n",
        "                \"window\": r.get(\"Window\"),\n",
        "                \"date_trained\": datetime.today().strftime(\"%Y-%m-%d\"),\n",
        "                \"framework\": \"stable-baselines3\",\n",
        "                \"input_features\": artifact.get(\"features\", []),\n",
        "                \"final_portfolio\": r.get(\"PPO_Portfolio\"),\n",
        "                \"buy_hold\": r.get(\"BuyHold\"),\n",
        "                \"sharpe\": r.get(\"Sharpe\"),\n",
        "            }, f)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[QC SAVE] Could not write model_info.json for {prefix}: {e}\")\n",
        "\n",
        "    logging.info(f\"[QC SAVE] Saved QC artifacts for {prefix}\")\n",
        "\n",
        "def load_model_and_env(prefix):\n",
        "    \"\"\"Load a trained PPO and create a factory to build a matching env window.\"\"\"\n",
        "    model_path = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_model.zip\")\n",
        "    vec_path   = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_vecnorm.pkl\")\n",
        "    model = PPO.load(model_path, device=\"cpu\")\n",
        "\n",
        "    def make_env(df_window):\n",
        "        frame_bound = (50, len(df_window) - 3)\n",
        "        e = DummyVecEnv([lambda: ContinuousPositionEnv(\n",
        "            df=df_window, frame_bound=frame_bound, **ENV_KWARGS\n",
        "        )])\n",
        "        if os.path.exists(vec_path):\n",
        "            e = VecNormalize.load(vec_path, e)\n",
        "        e.training = False\n",
        "        e.norm_reward = False\n",
        "        return e\n",
        "\n",
        "    return model, make_env\n",
        "\n",
        "def latest_df_for_symbol(symbol, horizon_days=5, interval=\"1m\"):\n",
        "    \"\"\"Fetch fresh bars and rebuild features exactly like training.\"\"\"\n",
        "    end = datetime.utcnow()\n",
        "    start = end - timedelta(days=horizon_days)\n",
        "    df_live = yf.download(\n",
        "        symbol,\n",
        "        start=start.strftime(\"%Y-%m-%d\"),\n",
        "        end=end.strftime(\"%Y-%m-%d\"),\n",
        "        interval=interval,\n",
        "        progress=False,\n",
        "        auto_adjust=False,\n",
        "    )\n",
        "    if df_live is None or df_live.empty:\n",
        "        return None\n",
        "    df_live = df_live.reset_index()\n",
        "    df_live[\"Symbol\"] = symbol\n",
        "    df_live = compute_enhanced_features(df_live)\n",
        "    if ENABLE_WAVELET and \"Denoised_Close\" not in df_live.columns:\n",
        "        df_live[\"Denoised_Close\"] = df_live[\"Close\"]\n",
        "    return df_live\n",
        "\n",
        "def predict_latest(symbol, prefix):\n",
        "    \"\"\"Build last window, fast-forward env, call model.predict(), return a signal.\"\"\"\n",
        "    # --- load per-model threshold ---\n",
        "    cfg_path = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_probability_config.json\")\n",
        "    thr = 0.2\n",
        "    if os.path.exists(cfg_path):\n",
        "        try:\n",
        "            with open(cfg_path, \"r\") as f:\n",
        "                thr = float(json.load(f).get(\"threshold\", 0.2))\n",
        "        except Exception:\n",
        "            thr = 0.2\n",
        "\n",
        "    model, make_env = load_model_and_env(prefix)\n",
        "    live_df = latest_df_for_symbol(symbol)\n",
        "    if live_df is None or len(live_df) < 100:\n",
        "        logging.warning(\"No fresh data yet for live inference.\")\n",
        "        return None\n",
        "\n",
        "    df_window = live_df.iloc[-2500:].reset_index(drop=True) if len(live_df) > 2500 else live_df.copy()\n",
        "\n",
        "    env = make_env(df_window)\n",
        "    obs = env.reset()\n",
        "    if isinstance(obs, tuple):\n",
        "        obs, _ = obs\n",
        "\n",
        "    # fast-forward with HOLD\n",
        "    for _ in range(len(df_window) - 1):\n",
        "        obs, _, dones, _ = env.step([np.array([0.0], dtype=np.float32)])\n",
        "        if isinstance(dones, (np.ndarray, list, tuple)) and len(dones) and dones[0]:\n",
        "            break\n",
        "\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    mu, sigma = get_mu_sigma(model, obs)\n",
        "\n",
        "    a = float(np.array(action).squeeze())\n",
        "\n",
        "    # --- thresholded signal using loaded thr ---\n",
        "    if a > thr:\n",
        "        signal = \"BUY\"\n",
        "    elif a < -thr:\n",
        "        signal = \"SELL\"\n",
        "    else:\n",
        "        signal = \"HOLD\"\n",
        "\n",
        "    conf = abs(a)\n",
        "    ts = df_window[\"Datetime\"].iloc[-1] if \"Datetime\" in df_window.columns else None\n",
        "    price = float(df_window[\"Close\"].iloc[-1])\n",
        "\n",
        "    return dict(\n",
        "        signal=signal,\n",
        "        confidence=conf,\n",
        "        action=a,\n",
        "        threshold=thr,\n",
        "        ts=ts,\n",
        "        price=price,\n",
        "        mu=mu,\n",
        "        sigma=sigma,\n",
        "    )\n",
        "\n",
        "def place_order(signal, qty=1):\n",
        "    \"\"\"Stub broker router with latency simulation; logs in Colab.\"\"\"\n",
        "    if SIM_LATENCY_MS > 0:\n",
        "        time.sleep(SIM_LATENCY_MS / 1000.0)\n",
        "    if BROKER == \"log\":\n",
        "        logging.info(f\"[PAPER] {signal} x{qty}\")\n",
        "    else:\n",
        "        logging.info(f\"[BROKER={BROKER}] {signal} x{qty} (not implemented)\")\n",
        "\n",
        "def live_loop(symbol, best_prefix):\n",
        "    \"\"\"Simple polling loop—set LIVE_MODE=True to run.\"\"\"\n",
        "    while LIVE_MODE:\n",
        "        try:\n",
        "            pred = predict_latest(symbol, best_prefix)\n",
        "            if pred:\n",
        "                logging.info(\n",
        "                    f\"{symbol} {pred['ts']} | {pred['signal']} \"\n",
        "                    f\"@ {pred['price']:.2f} (conf {pred['confidence']:.2f})\"\n",
        "                )\n",
        "                place_order(pred[\"signal\"], qty=1)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Live loop error: {e}\")\n",
        "        time.sleep(60)  # Poll each minute\n",
        "\n",
        "TOP_N_WINDOWS = 3\n",
        "\n",
        "FAST = {\n",
        "    \"lr\": 8e-5,\n",
        "    \"n_steps\": 3072,\n",
        "    \"batch\": 512,\n",
        "    \"clip\": 0.2,\n",
        "    \"ent\": 0.01,\n",
        "}\n",
        "\n",
        "SLOW = {\n",
        "    \"lr\": 3e-5,\n",
        "    \"n_steps\": 3072,\n",
        "    \"batch\": 512,\n",
        "    \"clip\": 0.16,\n",
        "    \"ent\": 0.005,\n",
        "}\n",
        "\n",
        "fast_names = {\n",
        "    \"TSLA\",\"NVDA\",\"AMD\",\"AVGO\",\"AAPL\",\"MSFT\",\"AMZN\",\"GOOGL\",\"META\",\"ADBE\",\"CRM\",\n",
        "    \"INTC\",\"QCOM\",\"TXN\",\"ORCL\",\"NEE\",\"GE\",\"XOM\",\"CVX\",\"LLY\",\"NKE\",\"SBUX\"\n",
        "}\n",
        "slow_names = {\n",
        "    \"BRK-B\",\"JPM\",\"BAC\",\"JNJ\",\"UNH\",\"MRK\",\"PFE\",\"ABBV\",\"ABT\",\"AMGN\",\"PG\",\"PEP\",\"KO\",\n",
        "    \"V\",\"MA\",\"WMT\",\"MCD\",\"TMO\",\"DHR\",\"ACN\",\"IBM\",\"LIN\",\"PM\",\"RTX\",\"UPS\",\"UNP\",\"COST\",\"HD\",\"LOW\"\n",
        "}\n",
        "\n",
        "def pick_params(symbol: str):\n",
        "    return FAST if symbol in fast_names else SLOW\n",
        "\n",
        "def export_qc_top_from_existing(ticker: str, top_n: int = 3):\n",
        "    \"\"\"\n",
        "    If a ticker is fully skipped (models already exist), still populate QC_TOP_DIR.\n",
        "    Uses existing summary CSVs to pick top Sharpe windows, then copies artifacts from FINAL_MODEL_DIR.\n",
        "    Prefers using 'Prefix' from summaries (robust). Falls back to WindowIdx reconstruction.\n",
        "    \"\"\"\n",
        "    summary_files = glob.glob(os.path.join(BASE_RESULTS_DIR, \"ppo_walkforward_results_*\", \"summary*.csv\"))\n",
        "    if not summary_files:\n",
        "        logging.warning(f\"[QC_TOP] No summary files found; cannot export QC_TOP for {ticker}.\")\n",
        "        return\n",
        "\n",
        "    frames = []\n",
        "    for p in summary_files:\n",
        "        try:\n",
        "            tmp = pd.read_csv(p)\n",
        "            frames.append(tmp)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"[QC_TOP] Could not read {p}: {e}\")\n",
        "\n",
        "    if not frames:\n",
        "        logging.warning(f\"[QC_TOP] Could not read any summary files; cannot export QC_TOP for {ticker}.\")\n",
        "        return\n",
        "\n",
        "    combo = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "    if \"Ticker\" not in combo.columns:\n",
        "        logging.warning(\"[QC_TOP] Summary files missing 'Ticker' column; cannot export.\")\n",
        "        return\n",
        "\n",
        "    combo = combo[combo[\"Ticker\"] == ticker].copy()\n",
        "    if combo.empty or \"Sharpe\" not in combo.columns:\n",
        "        logging.warning(f\"[QC_TOP] No rows for {ticker} in summaries (or missing Sharpe); cannot export QC_TOP.\")\n",
        "        return\n",
        "\n",
        "    # Ensure Sharpe is numeric so sorting works reliably\n",
        "    combo[\"Sharpe\"] = pd.to_numeric(combo[\"Sharpe\"], errors=\"coerce\")\n",
        "    combo = combo.dropna(subset=[\"Sharpe\"])\n",
        "    if combo.empty:\n",
        "        logging.warning(f\"[QC_TOP] All Sharpe values were non-numeric for {ticker}; cannot export.\")\n",
        "        return\n",
        "\n",
        "    use_prefix = (\"Prefix\" in combo.columns) and combo[\"Prefix\"].notna().any()\n",
        "\n",
        "    if use_prefix:\n",
        "        # Robust path: use saved Prefix directly\n",
        "        top = combo.sort_values(\"Sharpe\", ascending=False).head(top_n).copy()\n",
        "        top[\"__prefix__\"] = top[\"Prefix\"].astype(str)\n",
        "    else:\n",
        "        # Fallback: reconstruct WindowIdx (less robust)\n",
        "        def _window_start(w):\n",
        "            try:\n",
        "                s = str(w)\n",
        "                return int(s.split(\"-\")[0]) if \"-\" in s else np.nan\n",
        "            except Exception:\n",
        "                return np.nan\n",
        "\n",
        "        combo[\"WindowStart\"] = combo[\"Window\"].apply(_window_start)\n",
        "        combo = combo.sort_values([\"WindowStart\"]).reset_index(drop=True)\n",
        "        combo[\"WindowIdx\"] = combo.groupby(\"Ticker\").cumcount() + 1\n",
        "\n",
        "        top = combo.sort_values(\"Sharpe\", ascending=False).head(top_n).copy()\n",
        "        top[\"__prefix__\"] = top[\"WindowIdx\"].apply(lambda widx: f\"ppo_{ticker}_window{int(widx)}\")\n",
        "\n",
        "    exported = 0\n",
        "\n",
        "    for _, r in top.iterrows():\n",
        "        prefix = str(r[\"__prefix__\"])\n",
        "\n",
        "        model_path = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_model.zip\")\n",
        "        vec_path   = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_vecnorm.pkl\")\n",
        "\n",
        "        if not (os.path.exists(model_path) and os.path.exists(vec_path)):\n",
        "            logging.warning(f\"[QC_TOP] Missing model/vecnorm for {prefix}; cannot export.\")\n",
        "            continue\n",
        "\n",
        "        artifact_for_save = {\n",
        "            \"model\": None,\n",
        "            \"model_path\": model_path,\n",
        "            \"vecnorm_path\": vec_path,\n",
        "            \"features\": [],         # ok if unknown; QC can load features elsewhere\n",
        "            \"result\": r.to_dict(),  # includes Sharpe, Action_Threshold, etc if present\n",
        "            \"prefix\": prefix,\n",
        "        }\n",
        "        save_quantconnect_model(artifact_for_save, prefix, QC_TOP_DIR)\n",
        "        exported += 1\n",
        "\n",
        "    logging.info(f\"[QC_TOP] Exported {exported}/{len(top)} QC artifacts for {ticker}.\")\n",
        "\n",
        "def walkforward_ppo(df_sym, ticker,\n",
        "                    window_size=3500, step_size=500,\n",
        "                    timesteps=150_000, learning_rate=1e-4,\n",
        "                    ppo_overrides=None):\n",
        "    import heapq\n",
        "\n",
        "    if ppo_overrides is None:\n",
        "        ppo_overrides = {}\n",
        "\n",
        "    if len(df_sym) < window_size:\n",
        "        logging.warning(\n",
        "            f\"Skipping {ticker}: only {len(df_sym)} rows (min required: {window_size})\"\n",
        "        )\n",
        "        return []\n",
        "\n",
        "    results = []\n",
        "    windows = get_walk_forward_windows(df_sym, window_size, step_size)\n",
        "    top_heap = []\n",
        "    skipped_windows = []\n",
        "\n",
        "    # quick check: all windows already have model+vecnorm?\n",
        "    all_done = True\n",
        "    for idx in range(len(windows)):\n",
        "        prefix = f\"ppo_{ticker}_window{idx+1}\"\n",
        "        model_ok   = os.path.exists(os.path.join(FINAL_MODEL_DIR, f\"{prefix}_model.zip\"))\n",
        "        vecnorm_ok = os.path.exists(os.path.join(FINAL_MODEL_DIR, f\"{prefix}_vecnorm.pkl\"))\n",
        "        if not (model_ok and vecnorm_ok):\n",
        "            all_done = False\n",
        "            break\n",
        "\n",
        "    if all_done:\n",
        "        logging.info(f\"Ticker {ticker} fully skipped (all {len(windows)} windows already complete).\")\n",
        "        record_skips_global(ticker, skipped_windows=[], total_windows=len(windows), fully_skipped=True)\n",
        "\n",
        "        export_qc_top_from_existing(ticker, top_n=TOP_N_WINDOWS)\n",
        "        return []\n",
        "\n",
        "\n",
        "\n",
        "    for w_idx, (start, end) in enumerate(windows):\n",
        "        window_start_time = time.time()\n",
        "        gc.collect()\n",
        "\n",
        "        prefix = f\"ppo_{ticker}_window{w_idx+1}\"\n",
        "        model_path   = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_model.zip\")\n",
        "        vecnorm_path = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_vecnorm.pkl\")\n",
        "\n",
        "        if os.path.exists(model_path) and os.path.exists(vecnorm_path):\n",
        "            logging.info(f\"Skipping {ticker} | Window {w_idx+1}, already trained.\")\n",
        "            skipped_windows.append(f\"{ticker}_window{w_idx+1}\")\n",
        "            continue\n",
        "\n",
        "        missing = []\n",
        "        if not os.path.exists(model_path):   missing.append(\"model.zip\")\n",
        "        if not os.path.exists(vecnorm_path): missing.append(\"vecnorm.pkl\")\n",
        "        logging.info(\n",
        "            f\"Will train {ticker} | Window {w_idx+1} because missing: {', '.join(missing)}\"\n",
        "        )\n",
        "\n",
        "        df_window = df_sym.iloc[start:end].reset_index(drop=True)\n",
        "        if len(df_window) <= 52 or len(df_window) % 2 != 0:\n",
        "            df_window = df_window.iloc[:-1]\n",
        "\n",
        "        frame_bound = (50, len(df_window) - 3)\n",
        "\n",
        "        env = DummyVecEnv([lambda: ContinuousPositionEnv(\n",
        "          df=df_window, frame_bound=frame_bound, **ENV_KWARGS\n",
        "        )])\n",
        "\n",
        "        env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
        "\n",
        "        try:\n",
        "            model = PPO(\n",
        "                \"MlpPolicy\",\n",
        "                env,\n",
        "                verbose=0,\n",
        "                device=(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "                learning_rate=ppo_overrides.get(\"lr\", learning_rate),\n",
        "                n_steps=ppo_overrides.get(\"n_steps\", 256),\n",
        "                batch_size=ppo_overrides.get(\"batch\", 64),\n",
        "                n_epochs=5,\n",
        "                gamma=0.99,\n",
        "                gae_lambda=0.95,\n",
        "                clip_range=ppo_overrides.get(\"clip\", 0.2),\n",
        "                ent_coef=ppo_overrides.get(\"ent\", 0.005),\n",
        "                policy_kwargs=dict(net_arch=[64, 64]),\n",
        "            )\n",
        "\n",
        "            logging.info(f\"Training {ticker} Window {w_idx+1}/{len(windows)}\")\n",
        "            model.learn(total_timesteps=timesteps)\n",
        "\n",
        "            # Evaluation pass\n",
        "            env.training = False\n",
        "            env.norm_reward = False\n",
        "            obs = env.reset()\n",
        "            if isinstance(obs, tuple):\n",
        "                obs, _ = obs\n",
        "\n",
        "            nav_track = [1.0]\n",
        "            bh_track  = [1.0]\n",
        "            step_log  = []\n",
        "            executed_trade_count = 0\n",
        "            signal_trade_count   = 0\n",
        "            signal_trade_count_dyn   = 0   # dynamic-threshold diagnostic\n",
        "\n",
        "            DIAG_THR = 0.2\n",
        "            for i in range(len(df_window) - 1):\n",
        "                action, _ = model.predict(obs, deterministic=True)\n",
        "                mu, sigma = get_mu_sigma(model, obs)\n",
        "\n",
        "                obs, rew, dones, infos = env.step(action)\n",
        "                # VecEnv returns list/tuple of infos; otherwise it may be a dict\n",
        "                if isinstance(infos, (list, tuple)):\n",
        "                    info = infos[0] if len(infos) else {}\n",
        "                elif isinstance(infos, dict):\n",
        "                    info = infos\n",
        "                else:\n",
        "                    info = {}\n",
        "\n",
        "\n",
        "                nav_track.append(float(info.get(\"nav\", nav_track[-1])))\n",
        "                bh_track.append(\n",
        "                    bh_track[-1] * (1.0 + float(info.get(\"ret_t\", 0.0)))\n",
        "                )\n",
        "\n",
        "                a = float(np.array(action).squeeze())\n",
        "                dt_val = df_window[\"Datetime\"].iloc[i+1] if \"Datetime\" in df_window.columns else None\n",
        "                px     = float(df_window[\"Close\"].iloc[i+1]) if \"Close\" in df_window.columns else np.nan\n",
        "                #“signal” trades (threshold-based) — diagnostic only\n",
        "                if a > DIAG_THR or a < -DIAG_THR:\n",
        "                    signal_trade_count += 1\n",
        "                #real trades executed by env friction logic\n",
        "                if bool(info.get(\"executed_trade\", False)):\n",
        "                    executed_trade_count += 1\n",
        "\n",
        "                # next-bar return (to score BUY/SELL vs the *next* move)\n",
        "                if i + 2 < len(df_window):\n",
        "                    p0 = float(df_window[\"Close\"].iloc[i+1])\n",
        "                    p1 = float(df_window[\"Close\"].iloc[i+2])\n",
        "                    next_ret = 0.0 if p0 <= 0 else (p1 - p0) / p0\n",
        "                else:\n",
        "                    next_ret = 0.0\n",
        "\n",
        "                # reward scalar (VecEnv returns arrays)\n",
        "                rew_val = float(rew[0]) if isinstance(rew, (list, tuple, np.ndarray)) else float(rew)\n",
        "\n",
        "                step_log.append({\n",
        "                    \"Index\": i+1,\n",
        "                    \"Datetime\": dt_val,\n",
        "                    \"Close\": px,\n",
        "                    \"Action\": a,\n",
        "                    \"mu\": mu,\n",
        "                    \"sigma\": sigma,\n",
        "                    \"nav\": nav_track[-1],\n",
        "                    \"ret_t\": float(info.get(\"ret_t\", 0.0)),\n",
        "                    \"next_ret\": float(next_ret),\n",
        "                    \"reward\": rew_val,\n",
        "                    \"pos\": float(info.get(\"pos\", 0.0)),\n",
        "                    \"trade_cost\": float(info.get(\"trade_cost\", 0.0)),\n",
        "                    \"base_ret\": float(info.get(\"base_ret\", 0.0)),\n",
        "                    \"rel_alpha\": float(info.get(\"rel_alpha\", 0.0)),\n",
        "                    \"mom\": float(info.get(\"mom\", 0.0)),\n",
        "                })\n",
        "\n",
        "                # done handling (VecEnv)\n",
        "                if isinstance(dones, (np.ndarray, list, tuple)):\n",
        "                    if dones[0]:\n",
        "                        break\n",
        "                elif dones:\n",
        "                    break\n",
        "\n",
        "\n",
        "            # --- Metrics ---\n",
        "            final_value = float(nav_track[-1]) * 100_000.0\n",
        "            hold_value  = float(bh_track[-1])  * 100_000.0\n",
        "\n",
        "            #dynamic action threshold for this window (prevents “no signals” windows)\n",
        "            abs_actions = np.array([abs(float(r[\"Action\"])) for r in step_log], dtype=float)\n",
        "            if len(abs_actions) > 0:\n",
        "                thr = float(np.quantile(abs_actions, 0.70))  # 70th percentile\n",
        "                thr = float(np.clip(thr, 0.08, 0.30))\n",
        "            else:\n",
        "                thr = 0.2\n",
        "\n",
        "            # Dynamic signal trade count (post-hoc diagnostic)\n",
        "            signal_trade_count_dyn = int(np.sum(abs_actions > thr)) if len(abs_actions) > 0 else 0\n",
        "\n",
        "\n",
        "            returns = pd.Series(nav_track).pct_change().fillna(0.0)\n",
        "            sharpe  = float((returns.mean() / (returns.std() + 1e-9)) * _annualization_factor(df_window))\n",
        "            drawdown = float(\n",
        "                ((pd.Series(nav_track).cummax() - pd.Series(nav_track)) /\n",
        "                pd.Series(nav_track).cummax()).max() * 100.0\n",
        "            )\n",
        "\n",
        "            # Classification stats (now using thr)\n",
        "            correct = 0\n",
        "            total   = 0\n",
        "            tp_buy = fp_buy = 0\n",
        "            tp_sell = fp_sell = 0\n",
        "\n",
        "            for r in step_log:\n",
        "                a = float(r[\"Action\"])\n",
        "                ret_t = float(r.get(\"next_ret\", 0.0))\n",
        "\n",
        "                if a > thr:\n",
        "                    sig = \"BUY\"\n",
        "                elif a < -thr:\n",
        "                    sig = \"SELL\"\n",
        "                else:\n",
        "                    sig = \"HOLD\"\n",
        "\n",
        "                if sig == \"BUY\":\n",
        "                    if ret_t > 0:\n",
        "                        tp_buy += 1; correct += 1\n",
        "                    else:\n",
        "                        fp_buy += 1\n",
        "                    total += 1\n",
        "                elif sig == \"SELL\":\n",
        "                    if ret_t < 0:\n",
        "                        tp_sell += 1; correct += 1\n",
        "                    else:\n",
        "                        fp_sell += 1\n",
        "                    total += 1\n",
        "                # HOLD not counted\n",
        "\n",
        "            precision_long  = tp_buy  / (tp_buy  + fp_buy  + 1e-9)\n",
        "            precision_short = tp_sell / (tp_sell + fp_sell + 1e-9)\n",
        "            precision_trades = (tp_buy + tp_sell) / (\n",
        "                (tp_buy + tp_sell) + (fp_buy + fp_sell) + 1e-9\n",
        "            )\n",
        "            step_accuracy = round(correct / total, 4) if total > 0 else 0.0\n",
        "            #Trade_count reflect REAL executed trades (cooldown/min_trade_delta)\n",
        "            trade_count = int(executed_trade_count)\n",
        "\n",
        "            # Save VecNormalize\n",
        "            try:\n",
        "                env.save(vecnorm_path)\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"Could not save VecNormalize for {ticker} {start}-{end}: {e}\")\n",
        "                vecnorm_path = None\n",
        "\n",
        "            # Save model\n",
        "            model.save(model_path)\n",
        "\n",
        "            # Save detailed predictions\n",
        "            pred_path = os.path.join(RUN_RESULTS_DIR, f\"{prefix}_predictions.csv\")\n",
        "            pd.DataFrame(step_log).to_csv(pred_path, index=False)\n",
        "            logging.info(f\"Saved predictions to {pred_path}\")\n",
        "\n",
        "            # Save compat predictions with same thresholds as metrics\n",
        "            compat_rows = []\n",
        "            for r in step_log:\n",
        "                a = r[\"Action\"]\n",
        "\n",
        "                if a > thr:\n",
        "                    signal = \"BUY\"\n",
        "                elif a < -thr:\n",
        "                    signal = \"SELL\"\n",
        "                else:\n",
        "                    signal = \"HOLD\"\n",
        "                compat_rows.append({\n",
        "                    \"Index\": r[\"Index\"],\n",
        "                    \"Datetime\": r[\"Datetime\"],\n",
        "                    \"Close\": r[\"Close\"],\n",
        "                    \"Action\": a,\n",
        "                    \"Signal\": signal,\n",
        "                    \"PortfolioValue\": r[\"nav\"],\n",
        "                    \"Reward\": r.get(\"reward\", np.nan),\n",
        "                })\n",
        "            compat_path = os.path.join(RUN_RESULTS_DIR, f\"{prefix}_predictions_compat.csv\")\n",
        "            pd.DataFrame(compat_rows).to_csv(compat_path, index=False)\n",
        "            logging.info(f\"Saved compatibility predictions to {compat_path}\")\n",
        "\n",
        "            # Summary row\n",
        "            result_row = {\n",
        "                \"Ticker\": ticker,\n",
        "                \"Window\": f\"{start}-{end}\",\n",
        "                \"WindowIdx\": int(w_idx + 1),\n",
        "                \"Prefix\": prefix,\n",
        "                \"PPO_Portfolio\": round(final_value, 2),\n",
        "                \"BuyHold\": round(hold_value, 2),\n",
        "                \"Sharpe\": round(sharpe, 3),\n",
        "                \"Drawdown_%\": round(drawdown, 2),\n",
        "                \"Winner\": \"PPO\" if final_value > hold_value else \"Buy & Hold\",\n",
        "                \"Action_Threshold\": round(thr, 4),\n",
        "                \"Accuracy\": step_accuracy,\n",
        "                \"Trade_Count\": trade_count,\n",
        "                \"Signal_Trade_Count\": int(signal_trade_count),\n",
        "                \"Signal_Trade_Count_Dyn\": int(signal_trade_count_dyn),\n",
        "                \"Executed_Trade_Count\": int(executed_trade_count),\n",
        "                \"Precision_Long\": round(precision_long, 4),\n",
        "                \"Precision_Short\": round(precision_short, 4),\n",
        "                \"Precision_Trades\": round(precision_trades, 4),\n",
        "            }\n",
        "\n",
        "            results.append(result_row)\n",
        "\n",
        "            meta = {\n",
        "                \"result\": result_row,\n",
        "                \"features\": df_window.columns.tolist(),\n",
        "                \"prefix\": prefix,\n",
        "                \"model_path\": model_path,\n",
        "                \"vecnorm_path\": vecnorm_path,\n",
        "            }\n",
        "\n",
        "            item = (result_row[\"Sharpe\"], prefix, meta)\n",
        "            if len(top_heap) < TOP_N_WINDOWS:\n",
        "                heapq.heappush(top_heap, item)\n",
        "            else:\n",
        "                if item[0] > top_heap[0][0]:\n",
        "                    heapq.heapreplace(top_heap, item)\n",
        "\n",
        "            logging.info(\n",
        "                f\"{ticker} | Window {w_idx+1} runtime: \"\n",
        "                f\"{round(time.time() - window_start_time, 2)}s\"\n",
        "            )\n",
        "        finally:\n",
        "            try:\n",
        "                env.close()\n",
        "            except Exception:\n",
        "                pass\n",
        "            del env\n",
        "            try:\n",
        "                del model\n",
        "            except Exception:\n",
        "                pass\n",
        "            gc.collect()\n",
        "            try:\n",
        "                torch.cuda.empty_cache()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    if skipped_windows:\n",
        "        logging.info(\n",
        "            f\"{ticker} skipped windows (already complete): {', '.join(skipped_windows)}\"\n",
        "        )\n",
        "        record_skips_global(\n",
        "            ticker,\n",
        "            skipped_windows=skipped_windows,\n",
        "            total_windows=len(windows),\n",
        "            fully_skipped=False,\n",
        "        )\n",
        "\n",
        "    # Save top-N QC-compatible\n",
        "    top_list = sorted(top_heap, key=lambda t: t[0], reverse=True)\n",
        "    for _, _, meta in top_list:\n",
        "        artifact_for_save = {\n",
        "            \"model\": None,  # we're copying from disk, not re-saving an in-memory model\n",
        "            \"model_path\": meta[\"model_path\"],\n",
        "            \"vecnorm_path\": meta[\"vecnorm_path\"],\n",
        "            \"features\": meta[\"features\"],\n",
        "            \"result\": meta[\"result\"],\n",
        "            \"prefix\": meta[\"prefix\"],\n",
        "        }\n",
        "        save_quantconnect_model(artifact_for_save, meta[\"prefix\"], QC_TOP_DIR)\n",
        "\n",
        "    return results\n",
        "\n",
        "def process_ticker(ticker):\n",
        "    try:\n",
        "        hp = pick_params(ticker)\n",
        "        return walkforward_ppo(\n",
        "            df[df[\"Symbol\"] == ticker].copy(),\n",
        "            ticker,\n",
        "            window_size=WINDOW_SIZE,\n",
        "            step_size=STEP_SIZE,\n",
        "            timesteps=TIMESTEPS,\n",
        "            learning_rate=hp[\"lr\"],\n",
        "            ppo_overrides=hp,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logging.error(f\"{ticker}: training failed with {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def run_parallel_tickers(tickers,\n",
        "                         out_path=os.path.join(RUN_RESULTS_DIR, \"summary.csv\"),\n",
        "                         max_workers=8):\n",
        "    results = []\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
        "        for res in ex.map(process_ticker, tickers):\n",
        "            if res:\n",
        "                results.extend(res)\n",
        "\n",
        "    if results:\n",
        "        pd.DataFrame(results).to_csv(out_path, index=False)\n",
        "        logging.info(f\"Saved summary to {out_path}\")\n",
        "    else:\n",
        "        logging.warning(\"No results produced; summary not written.\")\n",
        "\n",
        "    logging.info(\"All tickers processed.\")\n",
        "    return results\n",
        "\n",
        "def build_ppo_selector():\n",
        "    \"\"\"Aggregate all summary*.csv across runs and build selector JSON.\"\"\"\n",
        "    summary_files = glob.glob(\n",
        "        os.path.join(BASE_RESULTS_DIR, \"ppo_walkforward_results_*\", \"summary*.csv\")\n",
        "    )\n",
        "    all_summaries = []\n",
        "\n",
        "    for p in summary_files:\n",
        "        try:\n",
        "            tmp = pd.read_csv(p)\n",
        "            tmp[\"RunFolder\"] = os.path.dirname(p)\n",
        "            all_summaries.append(tmp)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Skipping {p} due to error: {e}\")\n",
        "\n",
        "    if not all_summaries:\n",
        "        logging.warning(\"No PPO summaries found across walkforward results folders.\")\n",
        "        return\n",
        "\n",
        "    combo = pd.concat(all_summaries, ignore_index=True)\n",
        "    if \"Sharpe\" in combo.columns:\n",
        "    combo[\"Sharpe\"] = pd.to_numeric(combo[\"Sharpe\"], errors=\"coerce\")\n",
        "    combo = combo.dropna(subset=[\"Sharpe\"])\n",
        "\n",
        "    # Ensure key columns exist for robust ratios\n",
        "    if \"BuyHold\" not in combo.columns:\n",
        "        combo[\"BuyHold\"] = np.nan\n",
        "    if \"PPO_Portfolio\" not in combo.columns:\n",
        "        combo[\"PPO_Portfolio\"] = np.nan\n",
        "\n",
        "    # parse Window \"start-end\" to WindowStart\n",
        "    def _parse_window_start(w):\n",
        "        if pd.isna(w):\n",
        "            return None\n",
        "        if isinstance(w, (int, float)):\n",
        "            return int(w)\n",
        "        parts = str(w).split(\"-\")\n",
        "        try:\n",
        "            return int(parts[0])\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    combo[\"WindowStart\"] = combo[\"Window\"].apply(_parse_window_start)\n",
        "\n",
        "    combo = combo.sort_values([\"Ticker\", \"WindowStart\"]).reset_index(drop=True)\n",
        "    combo[\"WindowIdx\"] = combo.groupby(\"Ticker\").cumcount() + 1\n",
        "\n",
        "    combo = combo.drop_duplicates(subset=[\"Ticker\", \"WindowIdx\"], keep=\"last\")\n",
        "\n",
        "    best_by_symbol = (\n",
        "        combo\n",
        "        .sort_values(\"Sharpe\", ascending=False)\n",
        "        .groupby(\"Ticker\")\n",
        "        .first()\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # If Drawdown_% missing (older runs), create it so rename won't break\n",
        "    if \"Drawdown_%\" not in best_by_symbol.columns:\n",
        "        best_by_symbol[\"Drawdown_%\"] = np.nan\n",
        "\n",
        "    # Ensure precision cols exist\n",
        "    for col in [\"Precision_Long\", \"Precision_Short\", \"Precision_Trades\"]:\n",
        "        if col not in best_by_symbol.columns:\n",
        "            best_by_symbol[col] = None  # or np.nan if you prefer\n",
        "\n",
        "    # Rename columns so everything downstream uses consistent names\n",
        "    best_by_symbol = best_by_symbol.rename(columns={\n",
        "        \"Drawdown_%\": \"Drawdown\",\n",
        "        \"PPO_Portfolio\": \"Final_Portfolio\",\n",
        "    })\n",
        "\n",
        "    # Ensure Accuracy / Trade_Count exist\n",
        "    if \"Accuracy\" not in best_by_symbol.columns:\n",
        "        best_by_symbol[\"Accuracy\"] = 0.0\n",
        "    if \"Trade_Count\" not in best_by_symbol.columns:\n",
        "        best_by_symbol[\"Trade_Count\"] = None\n",
        "\n",
        "    best_by_symbol[\"Model\"] = MODEL_NAME\n",
        "\n",
        "    # PPO vs Buy & Hold ratio (safe division)\n",
        "    best_by_symbol[\"Rel_vs_BH\"] = best_by_symbol.apply(\n",
        "        lambda r: (r[\"Final_Portfolio\"] / r[\"BuyHold\"])\n",
        "        if (pd.notna(r[\"BuyHold\"]) and r[\"BuyHold\"] not in (0, 0.0)) else np.nan,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Save flat CSV for debugging\n",
        "    best_by_symbol.to_csv(SELECTOR_FULL_PATH, index=False)\n",
        "    print(f\"Aggregated PPO selector saved to → {SELECTOR_FULL_PATH}\")\n",
        "\n",
        "    # Safety filters (tune as needed)\n",
        "    df_sel = best_by_symbol.copy()\n",
        "    gates = (\n",
        "        (df_sel[\"Sharpe\"].fillna(-999) > 0.0) &\n",
        "        (df_sel[\"Drawdown\"].fillna(999) < 50.0) &\n",
        "        (df_sel[\"Final_Portfolio\"].fillna(0) > 80_000) &\n",
        "        (df_sel[\"Rel_vs_BH\"].fillna(0) >= 0.95)   # PPO ≥ 95% of B&H; change to >1.0 to enforce beat\n",
        "    )\n",
        "    df_sel = df_sel[gates].copy()\n",
        "\n",
        "    df_sel[\"prefix\"] = (\n",
        "        \"ppo_\"\n",
        "        + df_sel[\"Ticker\"].astype(str)\n",
        "        + \"_window\"\n",
        "        + df_sel[\"WindowIdx\"].astype(int).astype(str)\n",
        "    )\n",
        "\n",
        "    df_sel[\"artifact_path\"] = df_sel[\"prefix\"].apply(\n",
        "        lambda p: os.path.join(FINAL_MODEL_DIR, f\"{p}_model.zip\")\n",
        "    )\n",
        "    df_sel[\"vecnorm_path\"] = df_sel[\"prefix\"].apply(\n",
        "        lambda p: os.path.join(FINAL_MODEL_DIR, f\"{p}_vecnorm.pkl\")\n",
        "    )\n",
        "\n",
        "    EPS = 0.03  # 3% of top-sharpe for \"close enough\"\n",
        "    selected_models = {}\n",
        "\n",
        "    def safe_int(v, default=0):\n",
        "        if v is None:\n",
        "            return int(default)\n",
        "        try:\n",
        "            import math\n",
        "            if isinstance(v, float) and math.isnan(v):\n",
        "                return int(default)\n",
        "        except TypeError:\n",
        "            pass\n",
        "        try:\n",
        "            return int(v)\n",
        "        except (ValueError, TypeError):\n",
        "            return int(default)\n",
        "\n",
        "    def safe_float(v, default=0.0):\n",
        "        if v is None:\n",
        "            return float(default)\n",
        "        try:\n",
        "            import math\n",
        "            if isinstance(v, float) and math.isnan(v):\n",
        "                return float(default)\n",
        "        except TypeError:\n",
        "            pass\n",
        "        try:\n",
        "            return float(v)\n",
        "        except (ValueError, TypeError):\n",
        "            return float(default)\n",
        "\n",
        "    for ticker, group in df_sel.groupby(\"Ticker\"):\n",
        "        group_sorted = group.sort_values(\"Sharpe\", ascending=False)\n",
        "        top = group_sorted.iloc[0]\n",
        "        second = group_sorted.iloc[1] if len(group_sorted) > 1 else None\n",
        "\n",
        "        if (second is not None) and (\n",
        "            abs(top[\"Sharpe\"] - second[\"Sharpe\"]) <= abs(top[\"Sharpe\"]) * EPS\n",
        "        ):\n",
        "            mode = \"ensemble\"\n",
        "            primary, secondary = top[\"Model\"], second[\"Model\"]\n",
        "        else:\n",
        "            mode = \"single\"\n",
        "            primary, secondary = top[\"Model\"], None\n",
        "\n",
        "        selected_models[ticker] = {\n",
        "            \"model\": MODEL_NAME,\n",
        "            \"score\": round(safe_float(top[\"Sharpe\"]), 4),\n",
        "            \"return\": round(safe_float(top[\"Final_Portfolio\"]), 2),\n",
        "            \"sharpe\": round(safe_float(top[\"Sharpe\"]), 3),\n",
        "            \"drawdown\": round(safe_float(top[\"Drawdown\"]), 2),\n",
        "            \"sortino\": None,\n",
        "            \"turnover\": None,\n",
        "            \"trade_count\": safe_int(top.get(\"Trade_Count\", 0)),\n",
        "            \"precision\": {\n",
        "                \"long\":   safe_float(top.get(\"Precision_Long\", 0.0)),\n",
        "                \"short\":  safe_float(top.get(\"Precision_Short\", 0.0)),\n",
        "                \"trades\": safe_float(top.get(\"Precision_Trades\", 0.0)),\n",
        "            },\n",
        "            \"stability\": {},\n",
        "            \"regime\": \"unknown\",\n",
        "            \"rl_profile\": \"fast\",\n",
        "            \"artifact\": {\n",
        "                \"path\": top[\"artifact_path\"],\n",
        "                \"vecnorm\": top[\"vecnorm_path\"],\n",
        "                \"features\": None,\n",
        "                \"load_ms\": 180,\n",
        "                \"mem_mb\": 512,\n",
        "                \"exists\": os.path.exists(top[\"artifact_path\"]),\n",
        "            },\n",
        "            \"selection\": {\n",
        "                \"mode\": mode,\n",
        "                \"primary\": primary,\n",
        "                \"secondary\": secondary,\n",
        "            },\n",
        "        }\n",
        "\n",
        "    with open(SELECTOR_JSON_PATH, \"w\") as f:\n",
        "        json.dump(selected_models, f, indent=2)\n",
        "\n",
        "    print(f\"Final enhanced PPO selector JSON saved to → {SELECTOR_JSON_PATH}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.info(f\"RUN_RESULTS_DIR   = {RUN_RESULTS_DIR}\")\n",
        "    logging.info(f\"FINAL_MODEL_DIR  = {FINAL_MODEL_DIR}\")\n",
        "    logging.info(f\"BASE_RESULTS_DIR = {BASE_RESULTS_DIR}\")\n",
        "\n",
        "    min_rows = WINDOW_SIZE + 50  # small buffer so we have at least one window\n",
        "    all_symbols = df[\"Symbol\"].value_counts()\n",
        "    candidate_symbols = []\n",
        "\n",
        "    for sym, n in all_symbols.items():\n",
        "        if n >= min_rows:\n",
        "            candidate_symbols.append(sym)\n",
        "        else:\n",
        "            logging.warning(f\"Skipping {sym}: only {n} rows (< {min_rows} required)\")\n",
        "\n",
        "    if not candidate_symbols:\n",
        "        logging.error(\"No symbols have enough rows for the current WINDOW_SIZE. Nothing to train.\")\n",
        "    else:\n",
        "        logging.info(f\"Training candidate symbols: {candidate_symbols}\")\n",
        "\n",
        "    needed_cols = [\"Close\", \"Datetime\"]\n",
        "    if ENABLE_WAVELET:\n",
        "        needed_cols.append(\"Denoised_Close\")\n",
        "    if ENABLE_SENTIMENT:\n",
        "        needed_cols.append(\"SentimentScore\")\n",
        "\n",
        "    valid_symbols = []\n",
        "    for sym in candidate_symbols:\n",
        "        cols = set(df.loc[df[\"Symbol\"] == sym].columns)\n",
        "        missing = [c for c in needed_cols if c not in cols]\n",
        "        if missing:\n",
        "            logging.warning(f\"Skipping {sym}: missing required cols {missing}\")\n",
        "        else:\n",
        "            valid_symbols.append(sym)\n",
        "\n",
        "    if not valid_symbols:\n",
        "        logging.error(\"No symbols passed the feature/column checks. Nothing to train.\")\n",
        "    else:\n",
        "        logging.info(f\"Final training universe: {valid_symbols}\")\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    if test_mode:\n",
        "        # Optional: shrink timesteps and/or window size in test mode\n",
        "        TIMESTEPS = 100_000   # lighter test\n",
        "        # WINDOW_SIZE = 2000  # uncomment if you want faster test runs\n",
        "        # STEP_SIZE   = 500\n",
        "\n",
        "        test_stocks = [\"AAPL\", \"NVDA\", \"MSFT\"]\n",
        "        present = [s for s in test_stocks if s in valid_symbols]\n",
        "        if not present:\n",
        "            logging.warning(\"Test mode: none of ['AAPL','NVDA','MSFT'] present after filters.\")\n",
        "        else:\n",
        "            logging.info(f\"Test mode: running on {present}\")\n",
        "\n",
        "        for sym in present:\n",
        "            logging.info(f\">>> [TEST_MODE] Processing {sym}\")\n",
        "            res = process_ticker(sym)\n",
        "            logging.info(f\"{sym}: produced {len(res)} window summaries\")\n",
        "            if res:\n",
        "                all_results.extend(res)\n",
        "\n",
        "        summary_path = os.path.join(RUN_RESULTS_DIR, \"summary_test_mode.csv\")\n",
        "        if all_results:\n",
        "            pd.DataFrame(all_results).to_csv(summary_path, index=False)\n",
        "            logging.info(f\"Test-mode summary saved to {summary_path}\")\n",
        "        else:\n",
        "            logging.warning(\"Test mode finished but no results were generated (no windows, or all skipped).\")\n",
        "\n",
        "    else:\n",
        "        logging.info(\"Starting full parallel PPO walkforward run...\")\n",
        "        summary_results = run_parallel_tickers(valid_symbols)\n",
        "        if not summary_results:\n",
        "            logging.warning(\"No results generated in full run (check logs for skips/length issues).\")\n",
        "        else:\n",
        "            summary_path = os.path.join(RUN_RESULTS_DIR, \"summary.csv\")\n",
        "            pd.DataFrame(summary_results).to_csv(summary_path, index=False)\n",
        "            logging.info(f\"Summary saved to {summary_path}\")\n",
        "\n",
        "    try:\n",
        "        build_ppo_selector()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"build_ppo_selector failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5c480aa-48e3-414a-cb54-ddb8a0cd1751",
        "id": "NeWtjMHAisFM"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "2025-12-26 00:44:30,958 - INFO - RUN_RESULTS_DIR   = /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044\n",
            "2025-12-26 00:44:30,960 - INFO - FINAL_MODEL_DIR  = /content/drive/MyDrive/Results_May_2025/ppo_models_master\n",
            "2025-12-26 00:44:30,961 - INFO - BASE_RESULTS_DIR = /content/drive/MyDrive/Results_May_2025\n",
            "2025-12-26 00:44:31,078 - INFO - Training candidate symbols: ['AAPL', 'QCOM', 'COST', 'RTX', 'BRK-B', 'SBUX', 'TMO', 'BAC', 'TSLA', 'CRM', 'AVGO', 'TXN', 'UNH', 'ADBE', 'AMGN', 'UNP', 'AMD', 'ACN', 'AMZN', 'ABT', 'WMT', 'ABBV', 'XOM', 'PFE', 'PM', 'CSCO', 'PG', 'LLY', 'MA', 'MCD', 'META', 'MRK', 'JPM', 'MSFT', 'NVDA', 'NKE', 'INTC', 'LOW', 'CVX', 'PEP', 'HD', 'GOOGL', 'ORCL', 'GE', 'BMY', 'JNJ', 'NEE', 'LIN', 'KO', 'V', 'UPS', 'DHR', 'IBM']\n",
            "2025-12-26 00:44:36,459 - INFO - Final training universe: ['AAPL', 'QCOM', 'COST', 'RTX', 'BRK-B', 'SBUX', 'TMO', 'BAC', 'TSLA', 'CRM', 'AVGO', 'TXN', 'UNH', 'ADBE', 'AMGN', 'UNP', 'AMD', 'ACN', 'AMZN', 'ABT', 'WMT', 'ABBV', 'XOM', 'PFE', 'PM', 'CSCO', 'PG', 'LLY', 'MA', 'MCD', 'META', 'MRK', 'JPM', 'MSFT', 'NVDA', 'NKE', 'INTC', 'LOW', 'CVX', 'PEP', 'HD', 'GOOGL', 'ORCL', 'GE', 'BMY', 'JNJ', 'NEE', 'LIN', 'KO', 'V', 'UPS', 'DHR', 'IBM']\n",
            "2025-12-26 00:44:36,461 - INFO - Test mode: running on ['AAPL', 'NVDA', 'MSFT']\n",
            "2025-12-26 00:44:36,462 - INFO - >>> [TEST_MODE] Processing AAPL\n",
            "2025-12-26 00:44:37,024 - INFO - Will train AAPL | Window 1 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:44:48,713 - INFO - Training AAPL Window 1/3\n",
            "2025-12-26 00:47:22,129 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window1_predictions.csv\n",
            "2025-12-26 00:47:22,205 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window1_predictions_compat.csv\n",
            "2025-12-26 00:47:22,206 - INFO - AAPL | Window 1 runtime: 165.69s\n",
            "2025-12-26 00:47:23,014 - INFO - Will train AAPL | Window 2 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:47:23,024 - INFO - Training AAPL Window 2/3\n",
            "2025-12-26 00:49:38,624 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window2_predictions.csv\n",
            "2025-12-26 00:49:38,718 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window2_predictions_compat.csv\n",
            "2025-12-26 00:49:38,719 - INFO - AAPL | Window 2 runtime: 136.07s\n",
            "2025-12-26 00:49:39,501 - INFO - Will train AAPL | Window 3 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:49:39,509 - INFO - Training AAPL Window 3/3\n",
            "2025-12-26 00:51:55,863 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window3_predictions.csv\n",
            "2025-12-26 00:51:55,987 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window3_predictions_compat.csv\n",
            "2025-12-26 00:51:55,990 - INFO - AAPL | Window 3 runtime: 136.86s\n",
            "2025-12-26 00:51:56,549 - INFO - [QC SAVE] Saved QC artifacts for ppo_AAPL_window3\n",
            "2025-12-26 00:51:56,576 - INFO - [QC SAVE] Saved QC artifacts for ppo_AAPL_window2\n",
            "2025-12-26 00:51:56,603 - INFO - [QC SAVE] Saved QC artifacts for ppo_AAPL_window1\n",
            "2025-12-26 00:51:56,610 - INFO - AAPL: produced 3 window summaries\n",
            "2025-12-26 00:51:56,612 - INFO - >>> [TEST_MODE] Processing NVDA\n",
            "2025-12-26 00:51:57,183 - INFO - Will train NVDA | Window 1 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:51:57,193 - INFO - Training NVDA Window 1/3\n",
            "2025-12-26 00:54:20,438 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window1_predictions.csv\n",
            "2025-12-26 00:54:20,643 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window1_predictions_compat.csv\n",
            "2025-12-26 00:54:20,645 - INFO - NVDA | Window 1 runtime: 143.94s\n",
            "2025-12-26 00:54:22,250 - INFO - Will train NVDA | Window 2 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:54:22,263 - INFO - Training NVDA Window 2/3\n",
            "2025-12-26 00:56:48,511 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window2_predictions.csv\n",
            "2025-12-26 00:56:48,590 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window2_predictions_compat.csv\n",
            "2025-12-26 00:56:48,591 - INFO - NVDA | Window 2 runtime: 147.14s\n",
            "2025-12-26 00:56:49,383 - INFO - Will train NVDA | Window 3 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:56:49,391 - INFO - Training NVDA Window 3/3\n",
            "2025-12-26 00:59:11,463 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window3_predictions.csv\n",
            "2025-12-26 00:59:11,547 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window3_predictions_compat.csv\n",
            "2025-12-26 00:59:11,548 - INFO - NVDA | Window 3 runtime: 142.54s\n",
            "2025-12-26 00:59:12,044 - INFO - [QC SAVE] Saved QC artifacts for ppo_NVDA_window1\n",
            "2025-12-26 00:59:12,081 - INFO - [QC SAVE] Saved QC artifacts for ppo_NVDA_window2\n",
            "2025-12-26 00:59:12,120 - INFO - [QC SAVE] Saved QC artifacts for ppo_NVDA_window3\n",
            "2025-12-26 00:59:12,127 - INFO - NVDA: produced 3 window summaries\n",
            "2025-12-26 00:59:12,127 - INFO - >>> [TEST_MODE] Processing MSFT\n",
            "2025-12-26 00:59:12,587 - INFO - Will train MSFT | Window 1 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:59:12,597 - INFO - Training MSFT Window 1/3\n",
            "2025-12-26 01:01:29,748 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window1_predictions.csv\n",
            "2025-12-26 01:01:29,862 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window1_predictions_compat.csv\n",
            "2025-12-26 01:01:29,865 - INFO - MSFT | Window 1 runtime: 137.7s\n",
            "2025-12-26 01:01:30,841 - INFO - Will train MSFT | Window 2 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 01:01:30,852 - INFO - Training MSFT Window 2/3\n",
            "2025-12-26 01:03:53,747 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window2_predictions.csv\n",
            "2025-12-26 01:03:53,877 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window2_predictions_compat.csv\n",
            "2025-12-26 01:03:53,881 - INFO - MSFT | Window 2 runtime: 143.51s\n",
            "2025-12-26 01:03:55,045 - INFO - Will train MSFT | Window 3 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 01:03:55,057 - INFO - Training MSFT Window 3/3\n",
            "2025-12-26 01:06:09,439 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window3_predictions.csv\n",
            "2025-12-26 01:06:09,522 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window3_predictions_compat.csv\n",
            "2025-12-26 01:06:09,523 - INFO - MSFT | Window 3 runtime: 134.92s\n",
            "2025-12-26 01:06:09,985 - INFO - [QC SAVE] Saved QC artifacts for ppo_MSFT_window2\n",
            "2025-12-26 01:06:10,025 - INFO - [QC SAVE] Saved QC artifacts for ppo_MSFT_window1\n",
            "2025-12-26 01:06:10,086 - INFO - [QC SAVE] Saved QC artifacts for ppo_MSFT_window3\n",
            "2025-12-26 01:06:10,093 - INFO - MSFT: produced 3 window summaries\n",
            "2025-12-26 01:06:10,108 - INFO - Test-mode summary saved to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/summary_test_mode.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aggregated PPO selector saved to → /content/drive/MyDrive/Results_May_2025/ppo_model_selector_FULL.csv\n",
            "Final enhanced PPO selector JSON saved to → /content/drive/MyDrive/Results_May_2025/ppo_model_selector_final.json\n"
          ]
        }
      ],
      "source": [
        "# PPO walkforward training + selector\n",
        "import os, gc, time, json, logging, glob\n",
        "import shutil\n",
        "from threading import Lock\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt  # optional for ad-hoc plots\n",
        "\n",
        "import torch\n",
        "import gymnasium as gym\n",
        "from gymnasium.spaces import Box as GBox\n",
        "\n",
        "import yfinance as yf\n",
        "from gym_anytrading.envs import StocksEnv\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"gymnasium\")\n",
        "# ---- Sharpe annualization helper (intraday heuristic: 6.5 hrs * 252) ----\n",
        "def _annualization_factor(_df_like=None) -> float:\n",
        "    \"\"\"Annualization factor for intraday bars (6.5 trading hours × 252 days).\"\"\"\n",
        "    return np.sqrt(252 * 6.5)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"jupyter_client.session\")\n",
        "warnings.filterwarnings(\"ignore\", message=\".*Gym has been unmaintained.*\")\n",
        "\n",
        "try:\n",
        "    compute_enhanced_features  # type: ignore\n",
        "except NameError:\n",
        "    def compute_enhanced_features(df_in: pd.DataFrame) -> pd.DataFrame:\n",
        "        df_out = df_in.copy()\n",
        "        if \"Datetime\" in df_out.columns:\n",
        "            df_out[\"Datetime\"] = pd.to_datetime(df_out[\"Datetime\"])\n",
        "            df_out = df_out.sort_values(\"Datetime\").reset_index(drop=True)\n",
        "        if \"Close\" not in df_out.columns:\n",
        "            raise ValueError(\"compute_enhanced_features: missing required column 'Close'\")\n",
        "        return df_out\n",
        "\n",
        "set_random_seed(42)\n",
        "\n",
        "BASE_RESULTS_DIR = \"/content/drive/MyDrive/Results_May_2025\"\n",
        "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "RUN_RESULTS_DIR = os.path.join(BASE_RESULTS_DIR, f\"ppo_walkforward_results_{RUN_TAG}\")\n",
        "FINAL_MODEL_DIR = os.path.join(BASE_RESULTS_DIR, \"ppo_models_master\")\n",
        "QC_TOP_DIR      = os.path.join(BASE_RESULTS_DIR, \"ppo_models_QC_TOP\")\n",
        "\n",
        "os.makedirs(QC_TOP_DIR, exist_ok=True)\n",
        "os.makedirs(RUN_RESULTS_DIR, exist_ok=True)\n",
        "os.makedirs(FINAL_MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# Aggregated selector outputs\n",
        "SELECTOR_FULL_PATH = os.path.join(BASE_RESULTS_DIR, \"ppo_model_selector_FULL.csv\")\n",
        "SELECTOR_JSON_PATH = os.path.join(BASE_RESULTS_DIR, \"ppo_model_selector_final.json\")\n",
        "MODEL_NAME = \"PPO\"\n",
        "\n",
        "# Global skip aggregation (thread-safe)\n",
        "SKIP_AGG_PATH = os.path.join(RUN_RESULTS_DIR, \"skipped_windows_global.csv\")\n",
        "SKIP_LOCK = Lock()\n",
        "\n",
        "# Logging Setup\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    force=True\n",
        ")\n",
        "\n",
        "# Flags\n",
        "ENABLE_SENTIMENT = False\n",
        "ENABLE_SLO       = True\n",
        "ENABLE_WAVELET   = True\n",
        "test_mode        = True            # set False for full universe\n",
        "ENABLE_PLOTS     = False\n",
        "LIVE_MODE        = False           # set True to run simple live/paper loop\n",
        "SIM_LATENCY_MS   = 0               # broker latency simulation; 0 = off\n",
        "BROKER           = \"log\"           # \"log\" = do not place orders, just log\n",
        "\n",
        "# Global training settings\n",
        "WINDOW_SIZE = 3500\n",
        "STEP_SIZE   = 500\n",
        "TIMESTEPS   = 150_000  # overridden in test_mode block to smaller value\n",
        "\n",
        "\n",
        "DATA_PATH = \"multi_stock_feature_engineered_dataset.csv\"\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    raise FileNotFoundError(\"Required feature-engineered dataset not found!\")\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "df[\"Datetime\"] = pd.to_datetime(df[\"Datetime\"])\n",
        "\n",
        "# Wavelet fallback\n",
        "if ENABLE_WAVELET and \"Denoised_Close\" not in df.columns:\n",
        "    logging.warning(\"ENABLE_WAVELET=True but 'Denoised_Close' missing; \"\n",
        "                    \"falling back to Close->Denoised_Close.\")\n",
        "    df[\"Denoised_Close\"] = df[\"Close\"]\n",
        "\n",
        "\n",
        "def record_skips_global(ticker: str, skipped_windows: list,\n",
        "                        total_windows: int = None, fully_skipped: bool = False):\n",
        "    \"\"\"Append skipped windows to the global skip log.\"\"\"\n",
        "    if not skipped_windows and not fully_skipped:\n",
        "        return\n",
        "    import csv\n",
        "    with SKIP_LOCK:\n",
        "        new_file = not os.path.exists(SKIP_AGG_PATH)\n",
        "        with open(SKIP_AGG_PATH, \"a\", newline=\"\") as f:\n",
        "            w = csv.writer(f)\n",
        "            if new_file:\n",
        "                w.writerow([\"Ticker\", \"Window\", \"FullySkipped\", \"TotalWindows\"])\n",
        "            if fully_skipped:\n",
        "                w.writerow([ticker, \"ALL\", True, total_windows if total_windows is not None else \"\"])\n",
        "            else:\n",
        "                for wname in skipped_windows:\n",
        "                    try:\n",
        "                        _, win_str = wname.split(\"_window\")\n",
        "                        win = int(win_str)\n",
        "                    except Exception:\n",
        "                        win = \"\"\n",
        "                    w.writerow([ticker, win, False, total_windows if total_windows is not None else \"\"])\n",
        "\n",
        "\n",
        "ENV_KWARGS = dict(\n",
        "    window_size=10,\n",
        "    cost_rate=0.0002,\n",
        "    slip_rate=0.0003,\n",
        "\n",
        "    k_alpha=0.0,\n",
        "    k_mom=0.15,\n",
        "    k_sent=(0.01 if ENABLE_SENTIMENT else 0.0),\n",
        "    mom_source=\"denoised\",\n",
        "    mom_lookback=20,\n",
        "\n",
        "    min_trade_delta=0.08,\n",
        "    cooldown=10,\n",
        "\n",
        "    reward_clip=0.05,\n",
        "    k_vol=0.00,\n",
        "    k_dd=0.00,\n",
        ")\n",
        "\n",
        "\n",
        "class ContinuousPositionEnv(StocksEnv):\n",
        "    def __init__(self, df, frame_bound, **kwargs):\n",
        "        # Require window_size from ENV_KWARGS\n",
        "        if \"window_size\" not in kwargs:\n",
        "            raise ValueError(\"ContinuousPositionEnv requires window_size (pass via ENV_KWARGS).\")\n",
        "\n",
        "        window_size = int(kwargs.pop(\"window_size\"))\n",
        "\n",
        "        # Pull params (all defaults live in ENV_KWARGS; these are just safety fallbacks)\n",
        "        cost_rate       = float(kwargs.pop(\"cost_rate\", 0.0002))\n",
        "        slip_rate       = float(kwargs.pop(\"slip_rate\", 0.0003))\n",
        "        k_alpha         = float(kwargs.pop(\"k_alpha\", 0.0))\n",
        "        k_mom           = float(kwargs.pop(\"k_mom\", 0.15))\n",
        "        k_sent          = float(kwargs.pop(\"k_sent\", 0.0))\n",
        "        mom_source      = str(kwargs.pop(\"mom_source\", \"denoised\"))\n",
        "        mom_lookback    = int(kwargs.pop(\"mom_lookback\", 20))\n",
        "        min_trade_delta = float(kwargs.pop(\"min_trade_delta\", 0.04))\n",
        "        cooldown        = int(kwargs.pop(\"cooldown\", 6))\n",
        "        reward_clip     = float(kwargs.pop(\"reward_clip\", 0.05))\n",
        "        k_vol           = float(kwargs.pop(\"k_vol\", 0.0))\n",
        "        k_dd            = float(kwargs.pop(\"k_dd\", 0.0))\n",
        "\n",
        "        # Fail fast on unexpected env kwargs\n",
        "        if kwargs:\n",
        "            raise ValueError(f\"Unexpected env kwargs: {list(kwargs.keys())}\")\n",
        "\n",
        "        super().__init__(\n",
        "            df=df.reset_index(drop=True),\n",
        "            frame_bound=frame_bound,\n",
        "            window_size=window_size\n",
        "        )\n",
        "\n",
        "        if isinstance(self.observation_space, gym.spaces.Box):\n",
        "            self.observation_space = GBox(\n",
        "                low=self.observation_space.low,\n",
        "                high=self.observation_space.high,\n",
        "                shape=self.observation_space.shape,\n",
        "                dtype=self.observation_space.dtype,\n",
        "            )\n",
        "\n",
        "        self.k_vol = k_vol\n",
        "        self.k_dd  = k_dd\n",
        "\n",
        "        self.ret_history = []\n",
        "        self.nav_history = []\n",
        "        self.peak_nav    = 1.0\n",
        "        self.trade_count = 0\n",
        "\n",
        "        self.action_space = GBox(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)\n",
        "\n",
        "\n",
        "        self.cost_rate       = cost_rate\n",
        "        self.slip_rate       = slip_rate\n",
        "        self.k_alpha         = k_alpha\n",
        "        self.k_mom           = k_mom\n",
        "        self.k_sent          = k_sent\n",
        "        self.mom_source      = mom_source\n",
        "        self.mom_lookback    = mom_lookback\n",
        "        self.min_trade_delta = min_trade_delta\n",
        "        self.cooldown        = cooldown\n",
        "        self.reward_clip     = reward_clip\n",
        "\n",
        "        self.nav = 1.0\n",
        "        self.pos = 0.0\n",
        "        self._last_trade_step = -self.cooldown\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        out = super().reset(**kwargs)\n",
        "        if isinstance(out, tuple):\n",
        "            obs, info = out\n",
        "        else:\n",
        "            obs, info = out, {}\n",
        "\n",
        "        self.nav = 1.0\n",
        "        self.pos = 0.0\n",
        "        self._last_trade_step = -self.cooldown\n",
        "\n",
        "        self.trade_count = 0\n",
        "        self.ret_history = []\n",
        "        self.nav_history = [self.nav]\n",
        "        self.peak_nav    = self.nav\n",
        "\n",
        "        info = info or {}\n",
        "        info.update({\n",
        "            \"nav\": self.nav,\n",
        "            \"pos\": self.pos,\n",
        "            \"trade_count\": int(self.trade_count),\n",
        "\n",
        "        })\n",
        "        return obs, info\n",
        "\n",
        "    def _step_parent_hold(self):\n",
        "        step_result = super().step(2)\n",
        "        if len(step_result) == 5:\n",
        "            obs, _env_rew, terminated, truncated, info = step_result\n",
        "        else:\n",
        "            obs, _env_rew, done, info = step_result\n",
        "            terminated, truncated = bool(done), False\n",
        "        return obs, terminated, truncated, info\n",
        "\n",
        "    def _ret_t(self):\n",
        "        cur  = float(self.df.loc[self._current_tick, \"Close\"])\n",
        "        prev = float(self.df.loc[max(self._current_tick - 1, 0), \"Close\"])\n",
        "        return 0.0 if prev <= 0 else (cur - prev) / prev\n",
        "\n",
        "    def _mom_signal(self):\n",
        "        if self.mom_source == \"macd\" and \"MACD_Line\" in self.df.columns:\n",
        "            recent = self.df[\"MACD_Line\"].iloc[max(self._current_tick - 200, 0):self._current_tick + 1]\n",
        "            return float(np.tanh(\n",
        "                float(self.df.loc[self._current_tick, \"MACD_Line\"]) /\n",
        "                (1e-6 + float(recent.std()))\n",
        "            ))\n",
        "\n",
        "        if \"Denoised_Close\" in self.df.columns and self._current_tick - self.mom_lookback >= 0:\n",
        "            now  = float(self.df.loc[self._current_tick, \"Denoised_Close\"])\n",
        "            then = float(self.df.loc[self._current_tick - self.mom_lookback, \"Denoised_Close\"])\n",
        "            base = float(self.df.loc[max(self._current_tick - 1, 0), \"Close\"])\n",
        "            slope = (now - then) / max(self.mom_lookback, 1)\n",
        "            return float(np.tanh(10.0 * (slope / max(abs(base), 1e-6))))\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "    def step(self, action):\n",
        "        a = float(np.array(action).squeeze())\n",
        "        target_pos = float(np.clip(a, -1.0, 1.0))\n",
        "\n",
        "        r_t = self._ret_t()\n",
        "        base_ret = self.pos * r_t\n",
        "\n",
        "        changed = (\n",
        "            abs(target_pos - self.pos) >= self.min_trade_delta\n",
        "        ) and (\n",
        "            (self._current_tick - self._last_trade_step) >= self.cooldown\n",
        "        )\n",
        "\n",
        "        delta_pos = (target_pos - self.pos) if changed else 0.0\n",
        "        trade_cost = (self.cost_rate + self.slip_rate) * abs(delta_pos)\n",
        "\n",
        "        rel_alpha = base_ret - r_t\n",
        "        mom_term = self.pos * self._mom_signal()\n",
        "\n",
        "        alpha_term = self.k_alpha * rel_alpha\n",
        "\n",
        "        sent_term = 0.0\n",
        "        if ENABLE_SENTIMENT and \"SentimentScore\" in self.df.columns:\n",
        "            sent_term = self.k_sent * float(self.df.loc[self._current_tick, \"SentimentScore\"])\n",
        "\n",
        "        shaped = base_ret + alpha_term + (self.k_mom * mom_term) + sent_term - trade_cost\n",
        "        reward = float(np.clip(shaped, -self.reward_clip, self.reward_clip))\n",
        "\n",
        "\n",
        "        self.nav *= (1.0 + base_ret - trade_cost)\n",
        "        self.nav_history.append(self.nav)\n",
        "        self.peak_nav = max(self.peak_nav, self.nav)\n",
        "\n",
        "        executed_trade = False\n",
        "        if changed:\n",
        "            self.pos = target_pos\n",
        "            self._last_trade_step = self._current_tick\n",
        "            self.trade_count += 1\n",
        "            executed_trade = True\n",
        "\n",
        "        obs, terminated, truncated, info = self._step_parent_hold()\n",
        "        info = info or {}\n",
        "        info.update({\n",
        "            \"ret_t\": r_t,\n",
        "            \"nav\": self.nav,\n",
        "            \"pos\": self.pos,\n",
        "            \"trade_cost\": trade_cost,\n",
        "            \"base_ret\": base_ret,\n",
        "            \"rel_alpha\": rel_alpha,\n",
        "            \"mom\": mom_term,\n",
        "            \"changed\": bool(changed),\n",
        "            \"executed_trade\": bool(executed_trade),\n",
        "            \"trade_count\": int(self.trade_count),\n",
        "            \"delta_pos\": float(delta_pos),\n",
        "        })\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "def get_mu_sigma(model, obs):\n",
        "    \"\"\"SB3 v2-safe way to get Gaussian policy mean/std for continuous actions.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        obs_t, _ = model.policy.obs_to_tensor(obs)\n",
        "        features = model.policy.extract_features(obs_t)\n",
        "        latent_pi, _ = model.policy.mlp_extractor(features)\n",
        "        mean_actions = model.policy.action_net(latent_pi)\n",
        "        log_std = model.policy.log_std\n",
        "        mu = float(mean_actions.detach().cpu().numpy().squeeze())\n",
        "        sigma = float(log_std.exp().detach().cpu().numpy().squeeze())\n",
        "    return mu, sigma\n",
        "\n",
        "def get_walk_forward_windows(df_in, window_size=3500, step_size=500, min_len=1200):\n",
        "    return [\n",
        "        (start, start + window_size)\n",
        "        for start in range(0, len(df_in) - min_len, step_size)\n",
        "        if start + window_size < len(df_in)\n",
        "    ]\n",
        "\n",
        "def save_quantconnect_model(artifact, prefix, save_dir):\n",
        "    \"\"\"Save/copy QC-compatible artifacts into save_dir.\"\"\"\n",
        "    import shutil\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # --- Model zip: save or copy ---\n",
        "    model_dst = os.path.join(save_dir, f\"{prefix}_model.zip\")\n",
        "\n",
        "    model_obj = artifact.get(\"model\", None)\n",
        "    model_src = artifact.get(\"model_path\", None)\n",
        "\n",
        "    try:\n",
        "        if model_obj is not None:\n",
        "            # Save from in-memory SB3 model\n",
        "            if not os.path.exists(model_dst):\n",
        "                model_obj.save(model_dst)\n",
        "\n",
        "        else:\n",
        "            # Copy from an existing trained window model zip\n",
        "            if model_src and os.path.exists(model_src):\n",
        "                if os.path.abspath(model_src) != os.path.abspath(model_dst):\n",
        "                    shutil.copyfile(model_src, model_dst)\n",
        "            else:\n",
        "                # If neither provided, warn loudly\n",
        "                if not os.path.exists(model_dst):\n",
        "                    logging.warning(f\"[QC SAVE] Missing model for {prefix}: no model_obj and no valid model_path.\")\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[QC SAVE] Model handling issue for {prefix}: {e}\")\n",
        "\n",
        "    # --- VecNormalize: copy ---\n",
        "    vecnorm_src = artifact.get(\"vecnorm_path\")\n",
        "    if vecnorm_src and os.path.exists(vecnorm_src):\n",
        "        try:\n",
        "            vecnorm_dst = os.path.join(save_dir, f\"{prefix}_vecnorm.pkl\")\n",
        "            if os.path.abspath(vecnorm_src) != os.path.abspath(vecnorm_dst):\n",
        "                shutil.copyfile(vecnorm_src, vecnorm_dst)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"[QC SAVE] VecNormalize handling issue for {prefix}: {e}\")\n",
        "    else:\n",
        "        logging.warning(f\"[QC SAVE] VecNormalize missing for {prefix}: vecnorm_path not found.\")\n",
        "\n",
        "    # --- Features ---\n",
        "    try:\n",
        "        with open(os.path.join(save_dir, f\"{prefix}_features.json\"), \"w\") as f:\n",
        "            json.dump({\"features\": artifact.get(\"features\", [])}, f)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[QC SAVE] Could not write features.json for {prefix}: {e}\")\n",
        "\n",
        "    # --- Probability config ---\n",
        "    try:\n",
        "        thr = 0.2\n",
        "        try:\n",
        "            thr = float(artifact.get(\"result\", {}).get(\"Action_Threshold\", 0.2))\n",
        "        except Exception:\n",
        "            thr = 0.2\n",
        "\n",
        "        with open(os.path.join(save_dir, f\"{prefix}_probability_config.json\"), \"w\") as f:\n",
        "            json.dump(\n",
        "                {\"threshold\": thr, \"use_confidence\": True, \"inference_mode\": \"deterministic\"},\n",
        "                f\n",
        "            )\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[QC SAVE] Could not write probability_config.json for {prefix}: {e}\")\n",
        "\n",
        "\n",
        "    # --- Model info ---\n",
        "    try:\n",
        "        r = artifact.get(\"result\", {})\n",
        "        with open(os.path.join(save_dir, f\"{prefix}_model_info.json\"), \"w\") as f:\n",
        "            json.dump({\n",
        "                \"model\": \"PPO\",\n",
        "                \"ticker\": r.get(\"Ticker\"),\n",
        "                \"window\": r.get(\"Window\"),\n",
        "                \"date_trained\": datetime.today().strftime(\"%Y-%m-%d\"),\n",
        "                \"framework\": \"stable-baselines3\",\n",
        "                \"input_features\": artifact.get(\"features\", []),\n",
        "                \"final_portfolio\": r.get(\"PPO_Portfolio\"),\n",
        "                \"buy_hold\": r.get(\"BuyHold\"),\n",
        "                \"sharpe\": r.get(\"Sharpe\"),\n",
        "            }, f)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[QC SAVE] Could not write model_info.json for {prefix}: {e}\")\n",
        "\n",
        "    logging.info(f\"[QC SAVE] Saved QC artifacts for {prefix}\")\n",
        "\n",
        "def load_model_and_env(prefix):\n",
        "    \"\"\"Load a trained PPO and create a factory to build a matching env window.\"\"\"\n",
        "    model_path = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_model.zip\")\n",
        "    vec_path   = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_vecnorm.pkl\")\n",
        "    model = PPO.load(model_path, device=\"cpu\")\n",
        "\n",
        "    def make_env(df_window):\n",
        "        frame_bound = (50, len(df_window) - 3)\n",
        "        e = DummyVecEnv([lambda: ContinuousPositionEnv(\n",
        "            df=df_window, frame_bound=frame_bound, **ENV_KWARGS\n",
        "        )])\n",
        "        if os.path.exists(vec_path):\n",
        "            e = VecNormalize.load(vec_path, e)\n",
        "        e.training = False\n",
        "        e.norm_reward = False\n",
        "        return e\n",
        "\n",
        "    return model, make_env\n",
        "\n",
        "def latest_df_for_symbol(symbol, horizon_days=5, interval=\"1m\"):\n",
        "    \"\"\"Fetch fresh bars and rebuild features exactly like training.\"\"\"\n",
        "    end = datetime.utcnow()\n",
        "    start = end - timedelta(days=horizon_days)\n",
        "    df_live = yf.download(\n",
        "        symbol,\n",
        "        start=start.strftime(\"%Y-%m-%d\"),\n",
        "        end=end.strftime(\"%Y-%m-%d\"),\n",
        "        interval=interval,\n",
        "        progress=False,\n",
        "        auto_adjust=False,\n",
        "    )\n",
        "    if df_live is None or df_live.empty:\n",
        "        return None\n",
        "    df_live = df_live.reset_index()\n",
        "    df_live[\"Symbol\"] = symbol\n",
        "    df_live = compute_enhanced_features(df_live)\n",
        "    if ENABLE_WAVELET and \"Denoised_Close\" not in df_live.columns:\n",
        "        df_live[\"Denoised_Close\"] = df_live[\"Close\"]\n",
        "    return df_live\n",
        "\n",
        "def predict_latest(symbol, prefix):\n",
        "    \"\"\"Build last window, fast-forward env, call model.predict(), return a signal.\"\"\"\n",
        "    # --- load per-model threshold ---\n",
        "    cfg_path = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_probability_config.json\")\n",
        "    thr = 0.2\n",
        "    if os.path.exists(cfg_path):\n",
        "        try:\n",
        "            with open(cfg_path, \"r\") as f:\n",
        "                thr = float(json.load(f).get(\"threshold\", 0.2))\n",
        "        except Exception:\n",
        "            thr = 0.2\n",
        "\n",
        "    model, make_env = load_model_and_env(prefix)\n",
        "    live_df = latest_df_for_symbol(symbol)\n",
        "    if live_df is None or len(live_df) < 100:\n",
        "        logging.warning(\"No fresh data yet for live inference.\")\n",
        "        return None\n",
        "\n",
        "    df_window = live_df.iloc[-2500:].reset_index(drop=True) if len(live_df) > 2500 else live_df.copy()\n",
        "\n",
        "    env = make_env(df_window)\n",
        "    obs = env.reset()\n",
        "    if isinstance(obs, tuple):\n",
        "        obs, _ = obs\n",
        "\n",
        "    # fast-forward with HOLD\n",
        "    for _ in range(len(df_window) - 1):\n",
        "        obs, _, dones, _ = env.step([np.array([0.0], dtype=np.float32)])\n",
        "        if isinstance(dones, (np.ndarray, list, tuple)) and len(dones) and dones[0]:\n",
        "            break\n",
        "\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    mu, sigma = get_mu_sigma(model, obs)\n",
        "\n",
        "    a = float(np.array(action).squeeze())\n",
        "\n",
        "    # --- thresholded signal using loaded thr ---\n",
        "    if a > thr:\n",
        "        signal = \"BUY\"\n",
        "    elif a < -thr:\n",
        "        signal = \"SELL\"\n",
        "    else:\n",
        "        signal = \"HOLD\"\n",
        "\n",
        "    conf = abs(a)\n",
        "    ts = df_window[\"Datetime\"].iloc[-1] if \"Datetime\" in df_window.columns else None\n",
        "    price = float(df_window[\"Close\"].iloc[-1])\n",
        "\n",
        "    return dict(\n",
        "        signal=signal,\n",
        "        confidence=conf,\n",
        "        action=a,\n",
        "        threshold=thr,\n",
        "        ts=ts,\n",
        "        price=price,\n",
        "        mu=mu,\n",
        "        sigma=sigma,\n",
        "    )\n",
        "\n",
        "def place_order(signal, qty=1):\n",
        "    \"\"\"Stub broker router with latency simulation; logs in Colab.\"\"\"\n",
        "    if SIM_LATENCY_MS > 0:\n",
        "        time.sleep(SIM_LATENCY_MS / 1000.0)\n",
        "    if BROKER == \"log\":\n",
        "        logging.info(f\"[PAPER] {signal} x{qty}\")\n",
        "    else:\n",
        "        logging.info(f\"[BROKER={BROKER}] {signal} x{qty} (not implemented)\")\n",
        "\n",
        "def live_loop(symbol, best_prefix):\n",
        "    \"\"\"Simple polling loop—set LIVE_MODE=True to run.\"\"\"\n",
        "    while LIVE_MODE:\n",
        "        try:\n",
        "            pred = predict_latest(symbol, best_prefix)\n",
        "            if pred:\n",
        "                logging.info(\n",
        "                    f\"{symbol} {pred['ts']} | {pred['signal']} \"\n",
        "                    f\"@ {pred['price']:.2f} (conf {pred['confidence']:.2f})\"\n",
        "                )\n",
        "                place_order(pred[\"signal\"], qty=1)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Live loop error: {e}\")\n",
        "        time.sleep(60)  # Poll each minute\n",
        "\n",
        "TOP_N_WINDOWS = 3\n",
        "\n",
        "FAST = {\n",
        "    \"lr\": 8e-5,\n",
        "    \"n_steps\": 3072,\n",
        "    \"batch\": 512,\n",
        "    \"clip\": 0.2,\n",
        "    \"ent\": 0.01,\n",
        "}\n",
        "\n",
        "SLOW = {\n",
        "    \"lr\": 3e-5,\n",
        "    \"n_steps\": 3072,\n",
        "    \"batch\": 512,\n",
        "    \"clip\": 0.16,\n",
        "    \"ent\": 0.005,\n",
        "}\n",
        "\n",
        "fast_names = {\n",
        "    \"TSLA\",\"NVDA\",\"AMD\",\"AVGO\",\"AAPL\",\"MSFT\",\"AMZN\",\"GOOGL\",\"META\",\"ADBE\",\"CRM\",\n",
        "    \"INTC\",\"QCOM\",\"TXN\",\"ORCL\",\"NEE\",\"GE\",\"XOM\",\"CVX\",\"LLY\",\"NKE\",\"SBUX\"\n",
        "}\n",
        "slow_names = {\n",
        "    \"BRK-B\",\"JPM\",\"BAC\",\"JNJ\",\"UNH\",\"MRK\",\"PFE\",\"ABBV\",\"ABT\",\"AMGN\",\"PG\",\"PEP\",\"KO\",\n",
        "    \"V\",\"MA\",\"WMT\",\"MCD\",\"TMO\",\"DHR\",\"ACN\",\"IBM\",\"LIN\",\"PM\",\"RTX\",\"UPS\",\"UNP\",\"COST\",\"HD\",\"LOW\"\n",
        "}\n",
        "\n",
        "def pick_params(symbol: str):\n",
        "    return FAST if symbol in fast_names else SLOW\n",
        "\n",
        "def export_qc_top_from_existing(ticker: str, top_n: int = 3):\n",
        "    \"\"\"\n",
        "    If a ticker is fully skipped (models already exist), still populate QC_TOP_DIR.\n",
        "    Uses existing summary CSVs to pick top Sharpe windows, then copies artifacts from FINAL_MODEL_DIR.\n",
        "    Prefers using 'Prefix' from summaries (robust). Falls back to WindowIdx reconstruction.\n",
        "    \"\"\"\n",
        "    summary_files = glob.glob(os.path.join(BASE_RESULTS_DIR, \"ppo_walkforward_results_*\", \"summary*.csv\"))\n",
        "    if not summary_files:\n",
        "        logging.warning(f\"[QC_TOP] No summary files found; cannot export QC_TOP for {ticker}.\")\n",
        "        return\n",
        "\n",
        "    frames = []\n",
        "    for p in summary_files:\n",
        "        try:\n",
        "            tmp = pd.read_csv(p)\n",
        "            frames.append(tmp)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"[QC_TOP] Could not read {p}: {e}\")\n",
        "\n",
        "    if not frames:\n",
        "        logging.warning(f\"[QC_TOP] Could not read any summary files; cannot export QC_TOP for {ticker}.\")\n",
        "        return\n",
        "\n",
        "    combo = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "    if \"Ticker\" not in combo.columns:\n",
        "        logging.warning(\"[QC_TOP] Summary files missing 'Ticker' column; cannot export.\")\n",
        "        return\n",
        "\n",
        "    combo = combo[combo[\"Ticker\"] == ticker].copy()\n",
        "    if combo.empty or \"Sharpe\" not in combo.columns:\n",
        "        logging.warning(f\"[QC_TOP] No rows for {ticker} in summaries (or missing Sharpe); cannot export QC_TOP.\")\n",
        "        return\n",
        "\n",
        "    # Ensure Sharpe is numeric so sorting works reliably\n",
        "    combo[\"Sharpe\"] = pd.to_numeric(combo[\"Sharpe\"], errors=\"coerce\")\n",
        "    combo = combo.dropna(subset=[\"Sharpe\"])\n",
        "    if combo.empty:\n",
        "        logging.warning(f\"[QC_TOP] All Sharpe values were non-numeric for {ticker}; cannot export.\")\n",
        "        return\n",
        "\n",
        "    use_prefix = (\"Prefix\" in combo.columns) and combo[\"Prefix\"].notna().any()\n",
        "\n",
        "    if use_prefix:\n",
        "        # Robust path: use saved Prefix directly\n",
        "        top = combo.sort_values(\"Sharpe\", ascending=False).head(top_n).copy()\n",
        "        top[\"__prefix__\"] = top[\"Prefix\"].astype(str)\n",
        "    else:\n",
        "        # Fallback: reconstruct WindowIdx (less robust)\n",
        "        def _window_start(w):\n",
        "            try:\n",
        "                s = str(w)\n",
        "                return int(s.split(\"-\")[0]) if \"-\" in s else np.nan\n",
        "            except Exception:\n",
        "                return np.nan\n",
        "\n",
        "        combo[\"WindowStart\"] = combo[\"Window\"].apply(_window_start)\n",
        "        combo = combo.sort_values([\"WindowStart\"]).reset_index(drop=True)\n",
        "        combo[\"WindowIdx\"] = combo.groupby(\"Ticker\").cumcount() + 1\n",
        "\n",
        "        top = combo.sort_values(\"Sharpe\", ascending=False).head(top_n).copy()\n",
        "        top[\"__prefix__\"] = top[\"WindowIdx\"].apply(lambda widx: f\"ppo_{ticker}_window{int(widx)}\")\n",
        "\n",
        "    exported = 0\n",
        "\n",
        "    for _, r in top.iterrows():\n",
        "        prefix = str(r[\"__prefix__\"])\n",
        "\n",
        "        model_path = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_model.zip\")\n",
        "        vec_path   = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_vecnorm.pkl\")\n",
        "\n",
        "        if not (os.path.exists(model_path) and os.path.exists(vec_path)):\n",
        "            logging.warning(f\"[QC_TOP] Missing model/vecnorm for {prefix}; cannot export.\")\n",
        "            continue\n",
        "\n",
        "        artifact_for_save = {\n",
        "            \"model\": None,\n",
        "            \"model_path\": model_path,\n",
        "            \"vecnorm_path\": vec_path,\n",
        "            \"features\": [],         # ok if unknown; QC can load features elsewhere\n",
        "            \"result\": r.to_dict(),  # includes Sharpe, Action_Threshold, etc if present\n",
        "            \"prefix\": prefix,\n",
        "        }\n",
        "        save_quantconnect_model(artifact_for_save, prefix, QC_TOP_DIR)\n",
        "        exported += 1\n",
        "\n",
        "    logging.info(f\"[QC_TOP] Exported {exported}/{len(top)} QC artifacts for {ticker}.\")\n",
        "\n",
        "def walkforward_ppo(df_sym, ticker,\n",
        "                    window_size=3500, step_size=500,\n",
        "                    timesteps=150_000, learning_rate=1e-4,\n",
        "                    ppo_overrides=None):\n",
        "    import heapq\n",
        "\n",
        "    if ppo_overrides is None:\n",
        "        ppo_overrides = {}\n",
        "\n",
        "    if len(df_sym) < window_size:\n",
        "        logging.warning(\n",
        "            f\"Skipping {ticker}: only {len(df_sym)} rows (min required: {window_size})\"\n",
        "        )\n",
        "        return []\n",
        "\n",
        "    results = []\n",
        "    windows = get_walk_forward_windows(df_sym, window_size, step_size)\n",
        "    top_heap = []\n",
        "    skipped_windows = []\n",
        "\n",
        "    # quick check: all windows already have model+vecnorm?\n",
        "    all_done = True\n",
        "    for idx in range(len(windows)):\n",
        "        prefix = f\"ppo_{ticker}_window{idx+1}\"\n",
        "        model_ok   = os.path.exists(os.path.join(FINAL_MODEL_DIR, f\"{prefix}_model.zip\"))\n",
        "        vecnorm_ok = os.path.exists(os.path.join(FINAL_MODEL_DIR, f\"{prefix}_vecnorm.pkl\"))\n",
        "        if not (model_ok and vecnorm_ok):\n",
        "            all_done = False\n",
        "            break\n",
        "\n",
        "    if all_done:\n",
        "        logging.info(f\"Ticker {ticker} fully skipped (all {len(windows)} windows already complete).\")\n",
        "        record_skips_global(ticker, skipped_windows=[], total_windows=len(windows), fully_skipped=True)\n",
        "\n",
        "        export_qc_top_from_existing(ticker, top_n=TOP_N_WINDOWS)\n",
        "        return []\n",
        "\n",
        "\n",
        "\n",
        "    for w_idx, (start, end) in enumerate(windows):\n",
        "        window_start_time = time.time()\n",
        "        gc.collect()\n",
        "\n",
        "        prefix = f\"ppo_{ticker}_window{w_idx+1}\"\n",
        "        model_path   = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_model.zip\")\n",
        "        vecnorm_path = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_vecnorm.pkl\")\n",
        "\n",
        "        if os.path.exists(model_path) and os.path.exists(vecnorm_path):\n",
        "            logging.info(f\"Skipping {ticker} | Window {w_idx+1}, already trained.\")\n",
        "            skipped_windows.append(f\"{ticker}_window{w_idx+1}\")\n",
        "            continue\n",
        "\n",
        "        missing = []\n",
        "        if not os.path.exists(model_path):   missing.append(\"model.zip\")\n",
        "        if not os.path.exists(vecnorm_path): missing.append(\"vecnorm.pkl\")\n",
        "        logging.info(\n",
        "            f\"Will train {ticker} | Window {w_idx+1} because missing: {', '.join(missing)}\"\n",
        "        )\n",
        "\n",
        "        df_window = df_sym.iloc[start:end].reset_index(drop=True)\n",
        "        if len(df_window) <= 52 or len(df_window) % 2 != 0:\n",
        "            df_window = df_window.iloc[:-1]\n",
        "\n",
        "        frame_bound = (50, len(df_window) - 3)\n",
        "\n",
        "        env = DummyVecEnv([lambda: ContinuousPositionEnv(\n",
        "          df=df_window, frame_bound=frame_bound, **ENV_KWARGS\n",
        "        )])\n",
        "\n",
        "        env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
        "\n",
        "        try:\n",
        "            model = PPO(\n",
        "                \"MlpPolicy\",\n",
        "                env,\n",
        "                verbose=0,\n",
        "                device=(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "                learning_rate=ppo_overrides.get(\"lr\", learning_rate),\n",
        "                n_steps=ppo_overrides.get(\"n_steps\", 256),\n",
        "                batch_size=ppo_overrides.get(\"batch\", 64),\n",
        "                n_epochs=5,\n",
        "                gamma=0.99,\n",
        "                gae_lambda=0.95,\n",
        "                clip_range=ppo_overrides.get(\"clip\", 0.2),\n",
        "                ent_coef=ppo_overrides.get(\"ent\", 0.005),\n",
        "                policy_kwargs=dict(net_arch=[64, 64]),\n",
        "            )\n",
        "\n",
        "            logging.info(f\"Training {ticker} Window {w_idx+1}/{len(windows)}\")\n",
        "            model.learn(total_timesteps=timesteps)\n",
        "\n",
        "            # Evaluation pass\n",
        "            env.training = False\n",
        "            env.norm_reward = False\n",
        "            obs = env.reset()\n",
        "            if isinstance(obs, tuple):\n",
        "                obs, _ = obs\n",
        "\n",
        "            nav_track = [1.0]\n",
        "            bh_track  = [1.0]\n",
        "            step_log  = []\n",
        "            executed_trade_count = 0\n",
        "            signal_trade_count   = 0\n",
        "            signal_trade_count_dyn   = 0   # dynamic-threshold diagnostic\n",
        "\n",
        "            DIAG_THR = 0.2\n",
        "            for i in range(len(df_window) - 1):\n",
        "                action, _ = model.predict(obs, deterministic=True)\n",
        "                mu, sigma = get_mu_sigma(model, obs)\n",
        "\n",
        "                obs, rew, dones, infos = env.step(action)\n",
        "                # VecEnv returns list/tuple of infos; otherwise it may be a dict\n",
        "                if isinstance(infos, (list, tuple)):\n",
        "                    info = infos[0] if len(infos) else {}\n",
        "                elif isinstance(infos, dict):\n",
        "                    info = infos\n",
        "                else:\n",
        "                    info = {}\n",
        "\n",
        "\n",
        "                nav_track.append(float(info.get(\"nav\", nav_track[-1])))\n",
        "                bh_track.append(\n",
        "                    bh_track[-1] * (1.0 + float(info.get(\"ret_t\", 0.0)))\n",
        "                )\n",
        "\n",
        "                a = float(np.array(action).squeeze())\n",
        "                dt_val = df_window[\"Datetime\"].iloc[i+1] if \"Datetime\" in df_window.columns else None\n",
        "                px     = float(df_window[\"Close\"].iloc[i+1]) if \"Close\" in df_window.columns else np.nan\n",
        "                #“signal” trades (threshold-based) — diagnostic only\n",
        "                if a > DIAG_THR or a < -DIAG_THR:\n",
        "                    signal_trade_count += 1\n",
        "                #real trades executed by env friction logic\n",
        "                if bool(info.get(\"executed_trade\", False)):\n",
        "                    executed_trade_count += 1\n",
        "\n",
        "                # next-bar return (to score BUY/SELL vs the *next* move)\n",
        "                if i + 2 < len(df_window):\n",
        "                    p0 = float(df_window[\"Close\"].iloc[i+1])\n",
        "                    p1 = float(df_window[\"Close\"].iloc[i+2])\n",
        "                    next_ret = 0.0 if p0 <= 0 else (p1 - p0) / p0\n",
        "                else:\n",
        "                    next_ret = 0.0\n",
        "\n",
        "                # reward scalar (VecEnv returns arrays)\n",
        "                rew_val = float(rew[0]) if isinstance(rew, (list, tuple, np.ndarray)) else float(rew)\n",
        "\n",
        "                step_log.append({\n",
        "                    \"Index\": i+1,\n",
        "                    \"Datetime\": dt_val,\n",
        "                    \"Close\": px,\n",
        "                    \"Action\": a,\n",
        "                    \"mu\": mu,\n",
        "                    \"sigma\": sigma,\n",
        "                    \"nav\": nav_track[-1],\n",
        "                    \"ret_t\": float(info.get(\"ret_t\", 0.0)),\n",
        "                    \"next_ret\": float(next_ret),\n",
        "                    \"reward\": rew_val,\n",
        "                    \"pos\": float(info.get(\"pos\", 0.0)),\n",
        "                    \"trade_cost\": float(info.get(\"trade_cost\", 0.0)),\n",
        "                    \"base_ret\": float(info.get(\"base_ret\", 0.0)),\n",
        "                    \"rel_alpha\": float(info.get(\"rel_alpha\", 0.0)),\n",
        "                    \"mom\": float(info.get(\"mom\", 0.0)),\n",
        "                })\n",
        "\n",
        "                # done handling (VecEnv)\n",
        "                if isinstance(dones, (np.ndarray, list, tuple)):\n",
        "                    if dones[0]:\n",
        "                        break\n",
        "                elif dones:\n",
        "                    break\n",
        "\n",
        "\n",
        "            # --- Metrics ---\n",
        "            final_value = float(nav_track[-1]) * 100_000.0\n",
        "            hold_value  = float(bh_track[-1])  * 100_000.0\n",
        "\n",
        "            #dynamic action threshold for this window (prevents “no signals” windows)\n",
        "            abs_actions = np.array([abs(float(r[\"Action\"])) for r in step_log], dtype=float)\n",
        "            if len(abs_actions) > 0:\n",
        "                thr = float(np.quantile(abs_actions, 0.70))  # 70th percentile\n",
        "                thr = float(np.clip(thr, 0.08, 0.30))\n",
        "            else:\n",
        "                thr = 0.2\n",
        "\n",
        "            # Dynamic signal trade count (post-hoc diagnostic)\n",
        "            signal_trade_count_dyn = int(np.sum(abs_actions > thr)) if len(abs_actions) > 0 else 0\n",
        "\n",
        "\n",
        "            returns = pd.Series(nav_track).pct_change().fillna(0.0)\n",
        "            sharpe  = float((returns.mean() / (returns.std() + 1e-9)) * _annualization_factor(df_window))\n",
        "            drawdown = float(\n",
        "                ((pd.Series(nav_track).cummax() - pd.Series(nav_track)) /\n",
        "                pd.Series(nav_track).cummax()).max() * 100.0\n",
        "            )\n",
        "\n",
        "            # Classification stats (now using thr)\n",
        "            correct = 0\n",
        "            total   = 0\n",
        "            tp_buy = fp_buy = 0\n",
        "            tp_sell = fp_sell = 0\n",
        "\n",
        "            for r in step_log:\n",
        "                a = float(r[\"Action\"])\n",
        "                ret_t = float(r.get(\"next_ret\", 0.0))\n",
        "\n",
        "                if a > thr:\n",
        "                    sig = \"BUY\"\n",
        "                elif a < -thr:\n",
        "                    sig = \"SELL\"\n",
        "                else:\n",
        "                    sig = \"HOLD\"\n",
        "\n",
        "                if sig == \"BUY\":\n",
        "                    if ret_t > 0:\n",
        "                        tp_buy += 1; correct += 1\n",
        "                    else:\n",
        "                        fp_buy += 1\n",
        "                    total += 1\n",
        "                elif sig == \"SELL\":\n",
        "                    if ret_t < 0:\n",
        "                        tp_sell += 1; correct += 1\n",
        "                    else:\n",
        "                        fp_sell += 1\n",
        "                    total += 1\n",
        "                # HOLD not counted\n",
        "\n",
        "            precision_long  = tp_buy  / (tp_buy  + fp_buy  + 1e-9)\n",
        "            precision_short = tp_sell / (tp_sell + fp_sell + 1e-9)\n",
        "            precision_trades = (tp_buy + tp_sell) / (\n",
        "                (tp_buy + tp_sell) + (fp_buy + fp_sell) + 1e-9\n",
        "            )\n",
        "            step_accuracy = round(correct / total, 4) if total > 0 else 0.0\n",
        "            #Trade_count reflect REAL executed trades (cooldown/min_trade_delta)\n",
        "            trade_count = int(executed_trade_count)\n",
        "\n",
        "            # Save VecNormalize\n",
        "            try:\n",
        "                env.save(vecnorm_path)\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"Could not save VecNormalize for {ticker} {start}-{end}: {e}\")\n",
        "                vecnorm_path = None\n",
        "\n",
        "            # Save model\n",
        "            model.save(model_path)\n",
        "\n",
        "            # Save detailed predictions\n",
        "            pred_path = os.path.join(RUN_RESULTS_DIR, f\"{prefix}_predictions.csv\")\n",
        "            pd.DataFrame(step_log).to_csv(pred_path, index=False)\n",
        "            logging.info(f\"Saved predictions to {pred_path}\")\n",
        "\n",
        "            # Save compat predictions with same thresholds as metrics\n",
        "            compat_rows = []\n",
        "            for r in step_log:\n",
        "                a = r[\"Action\"]\n",
        "\n",
        "                if a > thr:\n",
        "                    signal = \"BUY\"\n",
        "                elif a < -thr:\n",
        "                    signal = \"SELL\"\n",
        "                else:\n",
        "                    signal = \"HOLD\"\n",
        "                compat_rows.append({\n",
        "                    \"Index\": r[\"Index\"],\n",
        "                    \"Datetime\": r[\"Datetime\"],\n",
        "                    \"Close\": r[\"Close\"],\n",
        "                    \"Action\": a,\n",
        "                    \"Signal\": signal,\n",
        "                    \"PortfolioValue\": r[\"nav\"],\n",
        "                    \"Reward\": r.get(\"reward\", np.nan),\n",
        "                })\n",
        "            compat_path = os.path.join(RUN_RESULTS_DIR, f\"{prefix}_predictions_compat.csv\")\n",
        "            pd.DataFrame(compat_rows).to_csv(compat_path, index=False)\n",
        "            logging.info(f\"Saved compatibility predictions to {compat_path}\")\n",
        "\n",
        "            # Summary row\n",
        "            result_row = {\n",
        "                \"Ticker\": ticker,\n",
        "                \"Window\": f\"{start}-{end}\",\n",
        "                \"WindowIdx\": int(w_idx + 1),\n",
        "                \"Prefix\": prefix,\n",
        "                \"PPO_Portfolio\": round(final_value, 2),\n",
        "                \"BuyHold\": round(hold_value, 2),\n",
        "                \"Sharpe\": round(sharpe, 3),\n",
        "                \"Drawdown_%\": round(drawdown, 2),\n",
        "                \"Winner\": \"PPO\" if final_value > hold_value else \"Buy & Hold\",\n",
        "                \"Action_Threshold\": round(thr, 4),\n",
        "                \"Accuracy\": step_accuracy,\n",
        "                \"Trade_Count\": trade_count,\n",
        "                \"Signal_Trade_Count\": int(signal_trade_count),\n",
        "                \"Signal_Trade_Count_Dyn\": int(signal_trade_count_dyn),\n",
        "                \"Executed_Trade_Count\": int(executed_trade_count),\n",
        "                \"Precision_Long\": round(precision_long, 4),\n",
        "                \"Precision_Short\": round(precision_short, 4),\n",
        "                \"Precision_Trades\": round(precision_trades, 4),\n",
        "            }\n",
        "\n",
        "            results.append(result_row)\n",
        "\n",
        "            meta = {\n",
        "                \"result\": result_row,\n",
        "                \"features\": df_window.columns.tolist(),\n",
        "                \"prefix\": prefix,\n",
        "                \"model_path\": model_path,\n",
        "                \"vecnorm_path\": vecnorm_path,\n",
        "            }\n",
        "\n",
        "            item = (result_row[\"Sharpe\"], prefix, meta)\n",
        "            if len(top_heap) < TOP_N_WINDOWS:\n",
        "                heapq.heappush(top_heap, item)\n",
        "            else:\n",
        "                if item[0] > top_heap[0][0]:\n",
        "                    heapq.heapreplace(top_heap, item)\n",
        "\n",
        "            logging.info(\n",
        "                f\"{ticker} | Window {w_idx+1} runtime: \"\n",
        "                f\"{round(time.time() - window_start_time, 2)}s\"\n",
        "            )\n",
        "        finally:\n",
        "            try:\n",
        "                env.close()\n",
        "            except Exception:\n",
        "                pass\n",
        "            del env\n",
        "            try:\n",
        "                del model\n",
        "            except Exception:\n",
        "                pass\n",
        "            gc.collect()\n",
        "            try:\n",
        "                torch.cuda.empty_cache()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    if skipped_windows:\n",
        "        logging.info(\n",
        "            f\"{ticker} skipped windows (already complete): {', '.join(skipped_windows)}\"\n",
        "        )\n",
        "        record_skips_global(\n",
        "            ticker,\n",
        "            skipped_windows=skipped_windows,\n",
        "            total_windows=len(windows),\n",
        "            fully_skipped=False,\n",
        "        )\n",
        "\n",
        "    # Save top-N QC-compatible\n",
        "    top_list = sorted(top_heap, key=lambda t: t[0], reverse=True)\n",
        "    for _, _, meta in top_list:\n",
        "        artifact_for_save = {\n",
        "            \"model\": None,  # we're copying from disk, not re-saving an in-memory model\n",
        "            \"model_path\": meta[\"model_path\"],\n",
        "            \"vecnorm_path\": meta[\"vecnorm_path\"],\n",
        "            \"features\": meta[\"features\"],\n",
        "            \"result\": meta[\"result\"],\n",
        "            \"prefix\": meta[\"prefix\"],\n",
        "        }\n",
        "        save_quantconnect_model(artifact_for_save, meta[\"prefix\"], QC_TOP_DIR)\n",
        "\n",
        "    return results\n",
        "\n",
        "def process_ticker(ticker):\n",
        "    try:\n",
        "        hp = pick_params(ticker)\n",
        "        return walkforward_ppo(\n",
        "            df[df[\"Symbol\"] == ticker].copy(),\n",
        "            ticker,\n",
        "            window_size=WINDOW_SIZE,\n",
        "            step_size=STEP_SIZE,\n",
        "            timesteps=TIMESTEPS,\n",
        "            learning_rate=hp[\"lr\"],\n",
        "            ppo_overrides=hp,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logging.error(f\"{ticker}: training failed with {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def run_parallel_tickers(tickers,\n",
        "                         out_path=os.path.join(RUN_RESULTS_DIR, \"summary.csv\"),\n",
        "                         max_workers=8):\n",
        "    results = []\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
        "        for res in ex.map(process_ticker, tickers):\n",
        "            if res:\n",
        "                results.extend(res)\n",
        "\n",
        "    if results:\n",
        "        pd.DataFrame(results).to_csv(out_path, index=False)\n",
        "        logging.info(f\"Saved summary to {out_path}\")\n",
        "    else:\n",
        "        logging.warning(\"No results produced; summary not written.\")\n",
        "\n",
        "    logging.info(\"All tickers processed.\")\n",
        "    return results\n",
        "\n",
        "def build_ppo_selector():\n",
        "    \"\"\"Aggregate all summary*.csv across runs and build selector JSON.\"\"\"\n",
        "    summary_files = glob.glob(\n",
        "        os.path.join(BASE_RESULTS_DIR, \"ppo_walkforward_results_*\", \"summary*.csv\")\n",
        "    )\n",
        "    all_summaries = []\n",
        "\n",
        "    for p in summary_files:\n",
        "        try:\n",
        "            tmp = pd.read_csv(p)\n",
        "            tmp[\"RunFolder\"] = os.path.dirname(p)\n",
        "            all_summaries.append(tmp)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Skipping {p} due to error: {e}\")\n",
        "\n",
        "    if not all_summaries:\n",
        "        logging.warning(\"No PPO summaries found across walkforward results folders.\")\n",
        "        return\n",
        "\n",
        "    combo = pd.concat(all_summaries, ignore_index=True)\n",
        "    if \"Sharpe\" in combo.columns:\n",
        "    combo[\"Sharpe\"] = pd.to_numeric(combo[\"Sharpe\"], errors=\"coerce\")\n",
        "    combo = combo.dropna(subset=[\"Sharpe\"])\n",
        "\n",
        "    # Ensure key columns exist for robust ratios\n",
        "    if \"BuyHold\" not in combo.columns:\n",
        "        combo[\"BuyHold\"] = np.nan\n",
        "    if \"PPO_Portfolio\" not in combo.columns:\n",
        "        combo[\"PPO_Portfolio\"] = np.nan\n",
        "\n",
        "    # parse Window \"start-end\" to WindowStart\n",
        "    def _parse_window_start(w):\n",
        "        if pd.isna(w):\n",
        "            return None\n",
        "        if isinstance(w, (int, float)):\n",
        "            return int(w)\n",
        "        parts = str(w).split(\"-\")\n",
        "        try:\n",
        "            return int(parts[0])\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    combo[\"WindowStart\"] = combo[\"Window\"].apply(_parse_window_start)\n",
        "\n",
        "    combo = combo.sort_values([\"Ticker\", \"WindowStart\"]).reset_index(drop=True)\n",
        "    combo[\"WindowIdx\"] = combo.groupby(\"Ticker\").cumcount() + 1\n",
        "\n",
        "    combo = combo.drop_duplicates(subset=[\"Ticker\", \"WindowIdx\"], keep=\"last\")\n",
        "\n",
        "    best_by_symbol = (\n",
        "        combo\n",
        "        .sort_values(\"Sharpe\", ascending=False)\n",
        "        .groupby(\"Ticker\")\n",
        "        .first()\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # If Drawdown_% missing (older runs), create it so rename won't break\n",
        "    if \"Drawdown_%\" not in best_by_symbol.columns:\n",
        "        best_by_symbol[\"Drawdown_%\"] = np.nan\n",
        "\n",
        "    # Ensure precision cols exist\n",
        "    for col in [\"Precision_Long\", \"Precision_Short\", \"Precision_Trades\"]:\n",
        "        if col not in best_by_symbol.columns:\n",
        "            best_by_symbol[col] = None  # or np.nan if you prefer\n",
        "\n",
        "    # Rename columns so everything downstream uses consistent names\n",
        "    best_by_symbol = best_by_symbol.rename(columns={\n",
        "        \"Drawdown_%\": \"Drawdown\",\n",
        "        \"PPO_Portfolio\": \"Final_Portfolio\",\n",
        "    })\n",
        "\n",
        "    # Ensure Accuracy / Trade_Count exist\n",
        "    if \"Accuracy\" not in best_by_symbol.columns:\n",
        "        best_by_symbol[\"Accuracy\"] = 0.0\n",
        "    if \"Trade_Count\" not in best_by_symbol.columns:\n",
        "        best_by_symbol[\"Trade_Count\"] = None\n",
        "\n",
        "    best_by_symbol[\"Model\"] = MODEL_NAME\n",
        "\n",
        "    # PPO vs Buy & Hold ratio (safe division)\n",
        "    best_by_symbol[\"Rel_vs_BH\"] = best_by_symbol.apply(\n",
        "        lambda r: (r[\"Final_Portfolio\"] / r[\"BuyHold\"])\n",
        "        if (pd.notna(r[\"BuyHold\"]) and r[\"BuyHold\"] not in (0, 0.0)) else np.nan,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Save flat CSV for debugging\n",
        "    best_by_symbol.to_csv(SELECTOR_FULL_PATH, index=False)\n",
        "    print(f\"Aggregated PPO selector saved to → {SELECTOR_FULL_PATH}\")\n",
        "\n",
        "    # Safety filters (tune as needed)\n",
        "    df_sel = best_by_symbol.copy()\n",
        "    gates = (\n",
        "        (df_sel[\"Sharpe\"].fillna(-999) > 0.0) &\n",
        "        (df_sel[\"Drawdown\"].fillna(999) < 50.0) &\n",
        "        (df_sel[\"Final_Portfolio\"].fillna(0) > 80_000) &\n",
        "        (df_sel[\"Rel_vs_BH\"].fillna(0) >= 0.95)   # PPO ≥ 95% of B&H; change to >1.0 to enforce beat\n",
        "    )\n",
        "    df_sel = df_sel[gates].copy()\n",
        "\n",
        "    df_sel[\"prefix\"] = (\n",
        "        \"ppo_\"\n",
        "        + df_sel[\"Ticker\"].astype(str)\n",
        "        + \"_window\"\n",
        "        + df_sel[\"WindowIdx\"].astype(int).astype(str)\n",
        "    )\n",
        "\n",
        "    df_sel[\"artifact_path\"] = df_sel[\"prefix\"].apply(\n",
        "        lambda p: os.path.join(FINAL_MODEL_DIR, f\"{p}_model.zip\")\n",
        "    )\n",
        "    df_sel[\"vecnorm_path\"] = df_sel[\"prefix\"].apply(\n",
        "        lambda p: os.path.join(FINAL_MODEL_DIR, f\"{p}_vecnorm.pkl\")\n",
        "    )\n",
        "\n",
        "    EPS = 0.03  # 3% of top-sharpe for \"close enough\"\n",
        "    selected_models = {}\n",
        "\n",
        "    def safe_int(v, default=0):\n",
        "        if v is None:\n",
        "            return int(default)\n",
        "        try:\n",
        "            import math\n",
        "            if isinstance(v, float) and math.isnan(v):\n",
        "                return int(default)\n",
        "        except TypeError:\n",
        "            pass\n",
        "        try:\n",
        "            return int(v)\n",
        "        except (ValueError, TypeError):\n",
        "            return int(default)\n",
        "\n",
        "    def safe_float(v, default=0.0):\n",
        "        if v is None:\n",
        "            return float(default)\n",
        "        try:\n",
        "            import math\n",
        "            if isinstance(v, float) and math.isnan(v):\n",
        "                return float(default)\n",
        "        except TypeError:\n",
        "            pass\n",
        "        try:\n",
        "            return float(v)\n",
        "        except (ValueError, TypeError):\n",
        "            return float(default)\n",
        "\n",
        "    for ticker, group in df_sel.groupby(\"Ticker\"):\n",
        "        group_sorted = group.sort_values(\"Sharpe\", ascending=False)\n",
        "        top = group_sorted.iloc[0]\n",
        "        second = group_sorted.iloc[1] if len(group_sorted) > 1 else None\n",
        "\n",
        "        if (second is not None) and (\n",
        "            abs(top[\"Sharpe\"] - second[\"Sharpe\"]) <= abs(top[\"Sharpe\"]) * EPS\n",
        "        ):\n",
        "            mode = \"ensemble\"\n",
        "            primary, secondary = top[\"Model\"], second[\"Model\"]\n",
        "        else:\n",
        "            mode = \"single\"\n",
        "            primary, secondary = top[\"Model\"], None\n",
        "\n",
        "        selected_models[ticker] = {\n",
        "            \"model\": MODEL_NAME,\n",
        "            \"score\": round(safe_float(top[\"Sharpe\"]), 4),\n",
        "            \"return\": round(safe_float(top[\"Final_Portfolio\"]), 2),\n",
        "            \"sharpe\": round(safe_float(top[\"Sharpe\"]), 3),\n",
        "            \"drawdown\": round(safe_float(top[\"Drawdown\"]), 2),\n",
        "            \"sortino\": None,\n",
        "            \"turnover\": None,\n",
        "            \"trade_count\": safe_int(top.get(\"Trade_Count\", 0)),\n",
        "            \"precision\": {\n",
        "                \"long\":   safe_float(top.get(\"Precision_Long\", 0.0)),\n",
        "                \"short\":  safe_float(top.get(\"Precision_Short\", 0.0)),\n",
        "                \"trades\": safe_float(top.get(\"Precision_Trades\", 0.0)),\n",
        "            },\n",
        "            \"stability\": {},\n",
        "            \"regime\": \"unknown\",\n",
        "            \"rl_profile\": \"fast\",\n",
        "            \"artifact\": {\n",
        "                \"path\": top[\"artifact_path\"],\n",
        "                \"vecnorm\": top[\"vecnorm_path\"],\n",
        "                \"features\": None,\n",
        "                \"load_ms\": 180,\n",
        "                \"mem_mb\": 512,\n",
        "                \"exists\": os.path.exists(top[\"artifact_path\"]),\n",
        "            },\n",
        "            \"selection\": {\n",
        "                \"mode\": mode,\n",
        "                \"primary\": primary,\n",
        "                \"secondary\": secondary,\n",
        "            },\n",
        "        }\n",
        "\n",
        "    with open(SELECTOR_JSON_PATH, \"w\") as f:\n",
        "        json.dump(selected_models, f, indent=2)\n",
        "\n",
        "    print(f\"Final enhanced PPO selector JSON saved to → {SELECTOR_JSON_PATH}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.info(f\"RUN_RESULTS_DIR   = {RUN_RESULTS_DIR}\")\n",
        "    logging.info(f\"FINAL_MODEL_DIR  = {FINAL_MODEL_DIR}\")\n",
        "    logging.info(f\"BASE_RESULTS_DIR = {BASE_RESULTS_DIR}\")\n",
        "\n",
        "    min_rows = WINDOW_SIZE + 50  # small buffer so we have at least one window\n",
        "    all_symbols = df[\"Symbol\"].value_counts()\n",
        "    candidate_symbols = []\n",
        "\n",
        "    for sym, n in all_symbols.items():\n",
        "        if n >= min_rows:\n",
        "            candidate_symbols.append(sym)\n",
        "        else:\n",
        "            logging.warning(f\"Skipping {sym}: only {n} rows (< {min_rows} required)\")\n",
        "\n",
        "    if not candidate_symbols:\n",
        "        logging.error(\"No symbols have enough rows for the current WINDOW_SIZE. Nothing to train.\")\n",
        "    else:\n",
        "        logging.info(f\"Training candidate symbols: {candidate_symbols}\")\n",
        "\n",
        "    needed_cols = [\"Close\", \"Datetime\"]\n",
        "    if ENABLE_WAVELET:\n",
        "        needed_cols.append(\"Denoised_Close\")\n",
        "    if ENABLE_SENTIMENT:\n",
        "        needed_cols.append(\"SentimentScore\")\n",
        "\n",
        "    valid_symbols = []\n",
        "    for sym in candidate_symbols:\n",
        "        cols = set(df.loc[df[\"Symbol\"] == sym].columns)\n",
        "        missing = [c for c in needed_cols if c not in cols]\n",
        "        if missing:\n",
        "            logging.warning(f\"Skipping {sym}: missing required cols {missing}\")\n",
        "        else:\n",
        "            valid_symbols.append(sym)\n",
        "\n",
        "    if not valid_symbols:\n",
        "        logging.error(\"No symbols passed the feature/column checks. Nothing to train.\")\n",
        "    else:\n",
        "        logging.info(f\"Final training universe: {valid_symbols}\")\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    if test_mode:\n",
        "        # Optional: shrink timesteps and/or window size in test mode\n",
        "        TIMESTEPS = 100_000   # lighter test\n",
        "        # WINDOW_SIZE = 2000  # uncomment if you want faster test runs\n",
        "        # STEP_SIZE   = 500\n",
        "\n",
        "        test_stocks = [\"AAPL\", \"NVDA\", \"MSFT\"]\n",
        "        present = [s for s in test_stocks if s in valid_symbols]\n",
        "        if not present:\n",
        "            logging.warning(\"Test mode: none of ['AAPL','NVDA','MSFT'] present after filters.\")\n",
        "        else:\n",
        "            logging.info(f\"Test mode: running on {present}\")\n",
        "\n",
        "        for sym in present:\n",
        "            logging.info(f\">>> [TEST_MODE] Processing {sym}\")\n",
        "            res = process_ticker(sym)\n",
        "            logging.info(f\"{sym}: produced {len(res)} window summaries\")\n",
        "            if res:\n",
        "                all_results.extend(res)\n",
        "\n",
        "        summary_path = os.path.join(RUN_RESULTS_DIR, \"summary_test_mode.csv\")\n",
        "        if all_results:\n",
        "            pd.DataFrame(all_results).to_csv(summary_path, index=False)\n",
        "            logging.info(f\"Test-mode summary saved to {summary_path}\")\n",
        "        else:\n",
        "            logging.warning(\"Test mode finished but no results were generated (no windows, or all skipped).\")\n",
        "\n",
        "    else:\n",
        "        logging.info(\"Starting full parallel PPO walkforward run...\")\n",
        "        summary_results = run_parallel_tickers(valid_symbols)\n",
        "        if not summary_results:\n",
        "            logging.warning(\"No results generated in full run (check logs for skips/length issues).\")\n",
        "        else:\n",
        "            summary_path = os.path.join(RUN_RESULTS_DIR, \"summary.csv\")\n",
        "            pd.DataFrame(summary_results).to_csv(summary_path, index=False)\n",
        "            logging.info(f\"Summary saved to {summary_path}\")\n",
        "\n",
        "    try:\n",
        "        build_ppo_selector()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"build_ppo_selector failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5c480aa-48e3-414a-cb54-ddb8a0cd1751",
        "id": "Gnbme3naiwx0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "2025-12-26 00:44:30,958 - INFO - RUN_RESULTS_DIR   = /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044\n",
            "2025-12-26 00:44:30,960 - INFO - FINAL_MODEL_DIR  = /content/drive/MyDrive/Results_May_2025/ppo_models_master\n",
            "2025-12-26 00:44:30,961 - INFO - BASE_RESULTS_DIR = /content/drive/MyDrive/Results_May_2025\n",
            "2025-12-26 00:44:31,078 - INFO - Training candidate symbols: ['AAPL', 'QCOM', 'COST', 'RTX', 'BRK-B', 'SBUX', 'TMO', 'BAC', 'TSLA', 'CRM', 'AVGO', 'TXN', 'UNH', 'ADBE', 'AMGN', 'UNP', 'AMD', 'ACN', 'AMZN', 'ABT', 'WMT', 'ABBV', 'XOM', 'PFE', 'PM', 'CSCO', 'PG', 'LLY', 'MA', 'MCD', 'META', 'MRK', 'JPM', 'MSFT', 'NVDA', 'NKE', 'INTC', 'LOW', 'CVX', 'PEP', 'HD', 'GOOGL', 'ORCL', 'GE', 'BMY', 'JNJ', 'NEE', 'LIN', 'KO', 'V', 'UPS', 'DHR', 'IBM']\n",
            "2025-12-26 00:44:36,459 - INFO - Final training universe: ['AAPL', 'QCOM', 'COST', 'RTX', 'BRK-B', 'SBUX', 'TMO', 'BAC', 'TSLA', 'CRM', 'AVGO', 'TXN', 'UNH', 'ADBE', 'AMGN', 'UNP', 'AMD', 'ACN', 'AMZN', 'ABT', 'WMT', 'ABBV', 'XOM', 'PFE', 'PM', 'CSCO', 'PG', 'LLY', 'MA', 'MCD', 'META', 'MRK', 'JPM', 'MSFT', 'NVDA', 'NKE', 'INTC', 'LOW', 'CVX', 'PEP', 'HD', 'GOOGL', 'ORCL', 'GE', 'BMY', 'JNJ', 'NEE', 'LIN', 'KO', 'V', 'UPS', 'DHR', 'IBM']\n",
            "2025-12-26 00:44:36,461 - INFO - Test mode: running on ['AAPL', 'NVDA', 'MSFT']\n",
            "2025-12-26 00:44:36,462 - INFO - >>> [TEST_MODE] Processing AAPL\n",
            "2025-12-26 00:44:37,024 - INFO - Will train AAPL | Window 1 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:44:48,713 - INFO - Training AAPL Window 1/3\n",
            "2025-12-26 00:47:22,129 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window1_predictions.csv\n",
            "2025-12-26 00:47:22,205 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window1_predictions_compat.csv\n",
            "2025-12-26 00:47:22,206 - INFO - AAPL | Window 1 runtime: 165.69s\n",
            "2025-12-26 00:47:23,014 - INFO - Will train AAPL | Window 2 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:47:23,024 - INFO - Training AAPL Window 2/3\n",
            "2025-12-26 00:49:38,624 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window2_predictions.csv\n",
            "2025-12-26 00:49:38,718 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window2_predictions_compat.csv\n",
            "2025-12-26 00:49:38,719 - INFO - AAPL | Window 2 runtime: 136.07s\n",
            "2025-12-26 00:49:39,501 - INFO - Will train AAPL | Window 3 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:49:39,509 - INFO - Training AAPL Window 3/3\n",
            "2025-12-26 00:51:55,863 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window3_predictions.csv\n",
            "2025-12-26 00:51:55,987 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_AAPL_window3_predictions_compat.csv\n",
            "2025-12-26 00:51:55,990 - INFO - AAPL | Window 3 runtime: 136.86s\n",
            "2025-12-26 00:51:56,549 - INFO - [QC SAVE] Saved QC artifacts for ppo_AAPL_window3\n",
            "2025-12-26 00:51:56,576 - INFO - [QC SAVE] Saved QC artifacts for ppo_AAPL_window2\n",
            "2025-12-26 00:51:56,603 - INFO - [QC SAVE] Saved QC artifacts for ppo_AAPL_window1\n",
            "2025-12-26 00:51:56,610 - INFO - AAPL: produced 3 window summaries\n",
            "2025-12-26 00:51:56,612 - INFO - >>> [TEST_MODE] Processing NVDA\n",
            "2025-12-26 00:51:57,183 - INFO - Will train NVDA | Window 1 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:51:57,193 - INFO - Training NVDA Window 1/3\n",
            "2025-12-26 00:54:20,438 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window1_predictions.csv\n",
            "2025-12-26 00:54:20,643 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window1_predictions_compat.csv\n",
            "2025-12-26 00:54:20,645 - INFO - NVDA | Window 1 runtime: 143.94s\n",
            "2025-12-26 00:54:22,250 - INFO - Will train NVDA | Window 2 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:54:22,263 - INFO - Training NVDA Window 2/3\n",
            "2025-12-26 00:56:48,511 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window2_predictions.csv\n",
            "2025-12-26 00:56:48,590 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window2_predictions_compat.csv\n",
            "2025-12-26 00:56:48,591 - INFO - NVDA | Window 2 runtime: 147.14s\n",
            "2025-12-26 00:56:49,383 - INFO - Will train NVDA | Window 3 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:56:49,391 - INFO - Training NVDA Window 3/3\n",
            "2025-12-26 00:59:11,463 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window3_predictions.csv\n",
            "2025-12-26 00:59:11,547 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_NVDA_window3_predictions_compat.csv\n",
            "2025-12-26 00:59:11,548 - INFO - NVDA | Window 3 runtime: 142.54s\n",
            "2025-12-26 00:59:12,044 - INFO - [QC SAVE] Saved QC artifacts for ppo_NVDA_window1\n",
            "2025-12-26 00:59:12,081 - INFO - [QC SAVE] Saved QC artifacts for ppo_NVDA_window2\n",
            "2025-12-26 00:59:12,120 - INFO - [QC SAVE] Saved QC artifacts for ppo_NVDA_window3\n",
            "2025-12-26 00:59:12,127 - INFO - NVDA: produced 3 window summaries\n",
            "2025-12-26 00:59:12,127 - INFO - >>> [TEST_MODE] Processing MSFT\n",
            "2025-12-26 00:59:12,587 - INFO - Will train MSFT | Window 1 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 00:59:12,597 - INFO - Training MSFT Window 1/3\n",
            "2025-12-26 01:01:29,748 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window1_predictions.csv\n",
            "2025-12-26 01:01:29,862 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window1_predictions_compat.csv\n",
            "2025-12-26 01:01:29,865 - INFO - MSFT | Window 1 runtime: 137.7s\n",
            "2025-12-26 01:01:30,841 - INFO - Will train MSFT | Window 2 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 01:01:30,852 - INFO - Training MSFT Window 2/3\n",
            "2025-12-26 01:03:53,747 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window2_predictions.csv\n",
            "2025-12-26 01:03:53,877 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window2_predictions_compat.csv\n",
            "2025-12-26 01:03:53,881 - INFO - MSFT | Window 2 runtime: 143.51s\n",
            "2025-12-26 01:03:55,045 - INFO - Will train MSFT | Window 3 because missing: model.zip, vecnorm.pkl\n",
            "2025-12-26 01:03:55,057 - INFO - Training MSFT Window 3/3\n",
            "2025-12-26 01:06:09,439 - INFO - Saved predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window3_predictions.csv\n",
            "2025-12-26 01:06:09,522 - INFO - Saved compatibility predictions to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/ppo_MSFT_window3_predictions_compat.csv\n",
            "2025-12-26 01:06:09,523 - INFO - MSFT | Window 3 runtime: 134.92s\n",
            "2025-12-26 01:06:09,985 - INFO - [QC SAVE] Saved QC artifacts for ppo_MSFT_window2\n",
            "2025-12-26 01:06:10,025 - INFO - [QC SAVE] Saved QC artifacts for ppo_MSFT_window1\n",
            "2025-12-26 01:06:10,086 - INFO - [QC SAVE] Saved QC artifacts for ppo_MSFT_window3\n",
            "2025-12-26 01:06:10,093 - INFO - MSFT: produced 3 window summaries\n",
            "2025-12-26 01:06:10,108 - INFO - Test-mode summary saved to /content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_20251226_0044/summary_test_mode.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aggregated PPO selector saved to → /content/drive/MyDrive/Results_May_2025/ppo_model_selector_FULL.csv\n",
            "Final enhanced PPO selector JSON saved to → /content/drive/MyDrive/Results_May_2025/ppo_model_selector_final.json\n"
          ]
        }
      ],
      "source": [
        "# PPO walkforward training + selector\n",
        "import os, gc, time, json, logging, glob\n",
        "import shutil\n",
        "from threading import Lock\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt  # optional for ad-hoc plots\n",
        "\n",
        "import torch\n",
        "import gymnasium as gym\n",
        "from gymnasium.spaces import Box as GBox\n",
        "\n",
        "import yfinance as yf\n",
        "from gym_anytrading.envs import StocksEnv\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"gymnasium\")\n",
        "# ---- Sharpe annualization helper (intraday heuristic: 6.5 hrs * 252) ----\n",
        "def _annualization_factor(_df_like=None) -> float:\n",
        "    \"\"\"Annualization factor for intraday bars (6.5 trading hours × 252 days).\"\"\"\n",
        "    return np.sqrt(252 * 6.5)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"jupyter_client.session\")\n",
        "warnings.filterwarnings(\"ignore\", message=\".*Gym has been unmaintained.*\")\n",
        "\n",
        "try:\n",
        "    compute_enhanced_features  # type: ignore\n",
        "except NameError:\n",
        "    def compute_enhanced_features(df_in: pd.DataFrame) -> pd.DataFrame:\n",
        "        df_out = df_in.copy()\n",
        "        if \"Datetime\" in df_out.columns:\n",
        "            df_out[\"Datetime\"] = pd.to_datetime(df_out[\"Datetime\"])\n",
        "            df_out = df_out.sort_values(\"Datetime\").reset_index(drop=True)\n",
        "        if \"Close\" not in df_out.columns:\n",
        "            raise ValueError(\"compute_enhanced_features: missing required column 'Close'\")\n",
        "        return df_out\n",
        "\n",
        "set_random_seed(42)\n",
        "\n",
        "BASE_RESULTS_DIR = \"/content/drive/MyDrive/Results_May_2025\"\n",
        "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "RUN_RESULTS_DIR = os.path.join(BASE_RESULTS_DIR, f\"ppo_walkforward_results_{RUN_TAG}\")\n",
        "FINAL_MODEL_DIR = os.path.join(BASE_RESULTS_DIR, \"ppo_models_master\")\n",
        "QC_TOP_DIR      = os.path.join(BASE_RESULTS_DIR, \"ppo_models_QC_TOP\")\n",
        "\n",
        "os.makedirs(QC_TOP_DIR, exist_ok=True)\n",
        "os.makedirs(RUN_RESULTS_DIR, exist_ok=True)\n",
        "os.makedirs(FINAL_MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# Aggregated selector outputs\n",
        "SELECTOR_FULL_PATH = os.path.join(BASE_RESULTS_DIR, \"ppo_model_selector_FULL.csv\")\n",
        "SELECTOR_JSON_PATH = os.path.join(BASE_RESULTS_DIR, \"ppo_model_selector_final.json\")\n",
        "MODEL_NAME = \"PPO\"\n",
        "\n",
        "# Global skip aggregation (thread-safe)\n",
        "SKIP_AGG_PATH = os.path.join(RUN_RESULTS_DIR, \"skipped_windows_global.csv\")\n",
        "SKIP_LOCK = Lock()\n",
        "\n",
        "# Logging Setup\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    force=True\n",
        ")\n",
        "\n",
        "# Flags\n",
        "ENABLE_SENTIMENT = False\n",
        "ENABLE_SLO       = True\n",
        "ENABLE_WAVELET   = True\n",
        "test_mode        = True            # set False for full universe\n",
        "ENABLE_PLOTS     = False\n",
        "LIVE_MODE        = False           # set True to run simple live/paper loop\n",
        "SIM_LATENCY_MS   = 0               # broker latency simulation; 0 = off\n",
        "BROKER           = \"log\"           # \"log\" = do not place orders, just log\n",
        "\n",
        "# Global training settings\n",
        "WINDOW_SIZE = 3500\n",
        "STEP_SIZE   = 500\n",
        "TIMESTEPS   = 150_000  # overridden in test_mode block to smaller value\n",
        "\n",
        "\n",
        "DATA_PATH = \"multi_stock_feature_engineered_dataset.csv\"\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    raise FileNotFoundError(\"Required feature-engineered dataset not found!\")\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "df[\"Datetime\"] = pd.to_datetime(df[\"Datetime\"])\n",
        "\n",
        "# Wavelet fallback\n",
        "if ENABLE_WAVELET and \"Denoised_Close\" not in df.columns:\n",
        "    logging.warning(\"ENABLE_WAVELET=True but 'Denoised_Close' missing; \"\n",
        "                    \"falling back to Close->Denoised_Close.\")\n",
        "    df[\"Denoised_Close\"] = df[\"Close\"]\n",
        "\n",
        "\n",
        "def record_skips_global(ticker: str, skipped_windows: list,\n",
        "                        total_windows: int = None, fully_skipped: bool = False):\n",
        "    \"\"\"Append skipped windows to the global skip log.\"\"\"\n",
        "    if not skipped_windows and not fully_skipped:\n",
        "        return\n",
        "    import csv\n",
        "    with SKIP_LOCK:\n",
        "        new_file = not os.path.exists(SKIP_AGG_PATH)\n",
        "        with open(SKIP_AGG_PATH, \"a\", newline=\"\") as f:\n",
        "            w = csv.writer(f)\n",
        "            if new_file:\n",
        "                w.writerow([\"Ticker\", \"Window\", \"FullySkipped\", \"TotalWindows\"])\n",
        "            if fully_skipped:\n",
        "                w.writerow([ticker, \"ALL\", True, total_windows if total_windows is not None else \"\"])\n",
        "            else:\n",
        "                for wname in skipped_windows:\n",
        "                    try:\n",
        "                        _, win_str = wname.split(\"_window\")\n",
        "                        win = int(win_str)\n",
        "                    except Exception:\n",
        "                        win = \"\"\n",
        "                    w.writerow([ticker, win, False, total_windows if total_windows is not None else \"\"])\n",
        "\n",
        "\n",
        "ENV_KWARGS = dict(\n",
        "    window_size=10,\n",
        "    cost_rate=0.0002,\n",
        "    slip_rate=0.0003,\n",
        "\n",
        "    k_alpha=0.0,\n",
        "    k_mom=0.15,\n",
        "    k_sent=(0.01 if ENABLE_SENTIMENT else 0.0),\n",
        "    mom_source=\"denoised\",\n",
        "    mom_lookback=20,\n",
        "\n",
        "    min_trade_delta=0.08,\n",
        "    cooldown=10,\n",
        "\n",
        "    reward_clip=0.05,\n",
        "    k_vol=0.00,\n",
        "    k_dd=0.00,\n",
        ")\n",
        "\n",
        "\n",
        "class ContinuousPositionEnv(StocksEnv):\n",
        "    def __init__(self, df, frame_bound, **kwargs):\n",
        "        # Require window_size from ENV_KWARGS\n",
        "        if \"window_size\" not in kwargs:\n",
        "            raise ValueError(\"ContinuousPositionEnv requires window_size (pass via ENV_KWARGS).\")\n",
        "\n",
        "        window_size = int(kwargs.pop(\"window_size\"))\n",
        "\n",
        "        # Pull params (all defaults live in ENV_KWARGS; these are just safety fallbacks)\n",
        "        cost_rate       = float(kwargs.pop(\"cost_rate\", 0.0002))\n",
        "        slip_rate       = float(kwargs.pop(\"slip_rate\", 0.0003))\n",
        "        k_alpha         = float(kwargs.pop(\"k_alpha\", 0.0))\n",
        "        k_mom           = float(kwargs.pop(\"k_mom\", 0.15))\n",
        "        k_sent          = float(kwargs.pop(\"k_sent\", 0.0))\n",
        "        mom_source      = str(kwargs.pop(\"mom_source\", \"denoised\"))\n",
        "        mom_lookback    = int(kwargs.pop(\"mom_lookback\", 20))\n",
        "        min_trade_delta = float(kwargs.pop(\"min_trade_delta\", 0.04))\n",
        "        cooldown        = int(kwargs.pop(\"cooldown\", 6))\n",
        "        reward_clip     = float(kwargs.pop(\"reward_clip\", 0.05))\n",
        "        k_vol           = float(kwargs.pop(\"k_vol\", 0.0))\n",
        "        k_dd            = float(kwargs.pop(\"k_dd\", 0.0))\n",
        "\n",
        "        # Fail fast on unexpected env kwargs\n",
        "        if kwargs:\n",
        "            raise ValueError(f\"Unexpected env kwargs: {list(kwargs.keys())}\")\n",
        "\n",
        "        super().__init__(\n",
        "            df=df.reset_index(drop=True),\n",
        "            frame_bound=frame_bound,\n",
        "            window_size=window_size\n",
        "        )\n",
        "\n",
        "        if isinstance(self.observation_space, gym.spaces.Box):\n",
        "            self.observation_space = GBox(\n",
        "                low=self.observation_space.low,\n",
        "                high=self.observation_space.high,\n",
        "                shape=self.observation_space.shape,\n",
        "                dtype=self.observation_space.dtype,\n",
        "            )\n",
        "\n",
        "        self.k_vol = k_vol\n",
        "        self.k_dd  = k_dd\n",
        "\n",
        "        self.ret_history = []\n",
        "        self.nav_history = []\n",
        "        self.peak_nav    = 1.0\n",
        "        self.trade_count = 0\n",
        "\n",
        "        self.action_space = GBox(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)\n",
        "\n",
        "\n",
        "        self.cost_rate       = cost_rate\n",
        "        self.slip_rate       = slip_rate\n",
        "        self.k_alpha         = k_alpha\n",
        "        self.k_mom           = k_mom\n",
        "        self.k_sent          = k_sent\n",
        "        self.mom_source      = mom_source\n",
        "        self.mom_lookback    = mom_lookback\n",
        "        self.min_trade_delta = min_trade_delta\n",
        "        self.cooldown        = cooldown\n",
        "        self.reward_clip     = reward_clip\n",
        "\n",
        "        self.nav = 1.0\n",
        "        self.pos = 0.0\n",
        "        self._last_trade_step = -self.cooldown\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        out = super().reset(**kwargs)\n",
        "        if isinstance(out, tuple):\n",
        "            obs, info = out\n",
        "        else:\n",
        "            obs, info = out, {}\n",
        "\n",
        "        self.nav = 1.0\n",
        "        self.pos = 0.0\n",
        "        self._last_trade_step = -self.cooldown\n",
        "\n",
        "        self.trade_count = 0\n",
        "        self.ret_history = []\n",
        "        self.nav_history = [self.nav]\n",
        "        self.peak_nav    = self.nav\n",
        "\n",
        "        info = info or {}\n",
        "        info.update({\n",
        "            \"nav\": self.nav,\n",
        "            \"pos\": self.pos,\n",
        "            \"trade_count\": int(self.trade_count),\n",
        "\n",
        "        })\n",
        "        return obs, info\n",
        "\n",
        "    def _step_parent_hold(self):\n",
        "        step_result = super().step(2)\n",
        "        if len(step_result) == 5:\n",
        "            obs, _env_rew, terminated, truncated, info = step_result\n",
        "        else:\n",
        "            obs, _env_rew, done, info = step_result\n",
        "            terminated, truncated = bool(done), False\n",
        "        return obs, terminated, truncated, info\n",
        "\n",
        "    def _ret_t(self):\n",
        "        cur  = float(self.df.loc[self._current_tick, \"Close\"])\n",
        "        prev = float(self.df.loc[max(self._current_tick - 1, 0), \"Close\"])\n",
        "        return 0.0 if prev <= 0 else (cur - prev) / prev\n",
        "\n",
        "    def _mom_signal(self):\n",
        "        if self.mom_source == \"macd\" and \"MACD_Line\" in self.df.columns:\n",
        "            recent = self.df[\"MACD_Line\"].iloc[max(self._current_tick - 200, 0):self._current_tick + 1]\n",
        "            return float(np.tanh(\n",
        "                float(self.df.loc[self._current_tick, \"MACD_Line\"]) /\n",
        "                (1e-6 + float(recent.std()))\n",
        "            ))\n",
        "\n",
        "        if \"Denoised_Close\" in self.df.columns and self._current_tick - self.mom_lookback >= 0:\n",
        "            now  = float(self.df.loc[self._current_tick, \"Denoised_Close\"])\n",
        "            then = float(self.df.loc[self._current_tick - self.mom_lookback, \"Denoised_Close\"])\n",
        "            base = float(self.df.loc[max(self._current_tick - 1, 0), \"Close\"])\n",
        "            slope = (now - then) / max(self.mom_lookback, 1)\n",
        "            return float(np.tanh(10.0 * (slope / max(abs(base), 1e-6))))\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "    def step(self, action):\n",
        "        a = float(np.array(action).squeeze())\n",
        "        target_pos = float(np.clip(a, -1.0, 1.0))\n",
        "\n",
        "        r_t = self._ret_t()\n",
        "        base_ret = self.pos * r_t\n",
        "\n",
        "        changed = (\n",
        "            abs(target_pos - self.pos) >= self.min_trade_delta\n",
        "        ) and (\n",
        "            (self._current_tick - self._last_trade_step) >= self.cooldown\n",
        "        )\n",
        "\n",
        "        delta_pos = (target_pos - self.pos) if changed else 0.0\n",
        "        trade_cost = (self.cost_rate + self.slip_rate) * abs(delta_pos)\n",
        "\n",
        "        rel_alpha = base_ret - r_t\n",
        "        mom_term = self.pos * self._mom_signal()\n",
        "\n",
        "        alpha_term = self.k_alpha * rel_alpha\n",
        "\n",
        "        sent_term = 0.0\n",
        "        if ENABLE_SENTIMENT and \"SentimentScore\" in self.df.columns:\n",
        "            sent_term = self.k_sent * float(self.df.loc[self._current_tick, \"SentimentScore\"])\n",
        "\n",
        "        shaped = base_ret + alpha_term + (self.k_mom * mom_term) + sent_term - trade_cost\n",
        "        reward = float(np.clip(shaped, -self.reward_clip, self.reward_clip))\n",
        "\n",
        "\n",
        "        self.nav *= (1.0 + base_ret - trade_cost)\n",
        "        self.nav_history.append(self.nav)\n",
        "        self.peak_nav = max(self.peak_nav, self.nav)\n",
        "\n",
        "        executed_trade = False\n",
        "        if changed:\n",
        "            self.pos = target_pos\n",
        "            self._last_trade_step = self._current_tick\n",
        "            self.trade_count += 1\n",
        "            executed_trade = True\n",
        "\n",
        "        obs, terminated, truncated, info = self._step_parent_hold()\n",
        "        info = info or {}\n",
        "        info.update({\n",
        "            \"ret_t\": r_t,\n",
        "            \"nav\": self.nav,\n",
        "            \"pos\": self.pos,\n",
        "            \"trade_cost\": trade_cost,\n",
        "            \"base_ret\": base_ret,\n",
        "            \"rel_alpha\": rel_alpha,\n",
        "            \"mom\": mom_term,\n",
        "            \"changed\": bool(changed),\n",
        "            \"executed_trade\": bool(executed_trade),\n",
        "            \"trade_count\": int(self.trade_count),\n",
        "            \"delta_pos\": float(delta_pos),\n",
        "        })\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "def get_mu_sigma(model, obs):\n",
        "    \"\"\"SB3 v2-safe way to get Gaussian policy mean/std for continuous actions.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        obs_t, _ = model.policy.obs_to_tensor(obs)\n",
        "        features = model.policy.extract_features(obs_t)\n",
        "        latent_pi, _ = model.policy.mlp_extractor(features)\n",
        "        mean_actions = model.policy.action_net(latent_pi)\n",
        "        log_std = model.policy.log_std\n",
        "        mu = float(mean_actions.detach().cpu().numpy().squeeze())\n",
        "        sigma = float(log_std.exp().detach().cpu().numpy().squeeze())\n",
        "    return mu, sigma\n",
        "\n",
        "def get_walk_forward_windows(df_in, window_size=3500, step_size=500, min_len=1200):\n",
        "    return [\n",
        "        (start, start + window_size)\n",
        "        for start in range(0, len(df_in) - min_len, step_size)\n",
        "        if start + window_size < len(df_in)\n",
        "    ]\n",
        "\n",
        "def save_quantconnect_model(artifact, prefix, save_dir):\n",
        "    \"\"\"Save/copy QC-compatible artifacts into save_dir.\"\"\"\n",
        "    import shutil\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # --- Model zip: save or copy ---\n",
        "    model_dst = os.path.join(save_dir, f\"{prefix}_model.zip\")\n",
        "\n",
        "    model_obj = artifact.get(\"model\", None)\n",
        "    model_src = artifact.get(\"model_path\", None)\n",
        "\n",
        "    try:\n",
        "        if model_obj is not None:\n",
        "            # Save from in-memory SB3 model\n",
        "            if not os.path.exists(model_dst):\n",
        "                model_obj.save(model_dst)\n",
        "\n",
        "        else:\n",
        "            # Copy from an existing trained window model zip\n",
        "            if model_src and os.path.exists(model_src):\n",
        "                if os.path.abspath(model_src) != os.path.abspath(model_dst):\n",
        "                    shutil.copyfile(model_src, model_dst)\n",
        "            else:\n",
        "                # If neither provided, warn loudly\n",
        "                if not os.path.exists(model_dst):\n",
        "                    logging.warning(f\"[QC SAVE] Missing model for {prefix}: no model_obj and no valid model_path.\")\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[QC SAVE] Model handling issue for {prefix}: {e}\")\n",
        "\n",
        "    # --- VecNormalize: copy ---\n",
        "    vecnorm_src = artifact.get(\"vecnorm_path\")\n",
        "    if vecnorm_src and os.path.exists(vecnorm_src):\n",
        "        try:\n",
        "            vecnorm_dst = os.path.join(save_dir, f\"{prefix}_vecnorm.pkl\")\n",
        "            if os.path.abspath(vecnorm_src) != os.path.abspath(vecnorm_dst):\n",
        "                shutil.copyfile(vecnorm_src, vecnorm_dst)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"[QC SAVE] VecNormalize handling issue for {prefix}: {e}\")\n",
        "    else:\n",
        "        logging.warning(f\"[QC SAVE] VecNormalize missing for {prefix}: vecnorm_path not found.\")\n",
        "\n",
        "    # --- Features ---\n",
        "    try:\n",
        "        with open(os.path.join(save_dir, f\"{prefix}_features.json\"), \"w\") as f:\n",
        "            json.dump({\"features\": artifact.get(\"features\", [])}, f)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[QC SAVE] Could not write features.json for {prefix}: {e}\")\n",
        "\n",
        "    # --- Probability config ---\n",
        "    try:\n",
        "        thr = 0.2\n",
        "        try:\n",
        "            thr = float(artifact.get(\"result\", {}).get(\"Action_Threshold\", 0.2))\n",
        "        except Exception:\n",
        "            thr = 0.2\n",
        "\n",
        "        with open(os.path.join(save_dir, f\"{prefix}_probability_config.json\"), \"w\") as f:\n",
        "            json.dump(\n",
        "                {\"threshold\": thr, \"use_confidence\": True, \"inference_mode\": \"deterministic\"},\n",
        "                f\n",
        "            )\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[QC SAVE] Could not write probability_config.json for {prefix}: {e}\")\n",
        "\n",
        "\n",
        "    # --- Model info ---\n",
        "    try:\n",
        "        r = artifact.get(\"result\", {})\n",
        "        with open(os.path.join(save_dir, f\"{prefix}_model_info.json\"), \"w\") as f:\n",
        "            json.dump({\n",
        "                \"model\": \"PPO\",\n",
        "                \"ticker\": r.get(\"Ticker\"),\n",
        "                \"window\": r.get(\"Window\"),\n",
        "                \"date_trained\": datetime.today().strftime(\"%Y-%m-%d\"),\n",
        "                \"framework\": \"stable-baselines3\",\n",
        "                \"input_features\": artifact.get(\"features\", []),\n",
        "                \"final_portfolio\": r.get(\"PPO_Portfolio\"),\n",
        "                \"buy_hold\": r.get(\"BuyHold\"),\n",
        "                \"sharpe\": r.get(\"Sharpe\"),\n",
        "            }, f)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"[QC SAVE] Could not write model_info.json for {prefix}: {e}\")\n",
        "\n",
        "    logging.info(f\"[QC SAVE] Saved QC artifacts for {prefix}\")\n",
        "\n",
        "def load_model_and_env(prefix):\n",
        "    \"\"\"Load a trained PPO and create a factory to build a matching env window.\"\"\"\n",
        "    model_path = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_model.zip\")\n",
        "    vec_path   = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_vecnorm.pkl\")\n",
        "    model = PPO.load(model_path, device=\"cpu\")\n",
        "\n",
        "    def make_env(df_window):\n",
        "        frame_bound = (50, len(df_window) - 3)\n",
        "        e = DummyVecEnv([lambda: ContinuousPositionEnv(\n",
        "            df=df_window, frame_bound=frame_bound, **ENV_KWARGS\n",
        "        )])\n",
        "        if os.path.exists(vec_path):\n",
        "            e = VecNormalize.load(vec_path, e)\n",
        "        e.training = False\n",
        "        e.norm_reward = False\n",
        "        return e\n",
        "\n",
        "    return model, make_env\n",
        "\n",
        "def latest_df_for_symbol(symbol, horizon_days=5, interval=\"1m\"):\n",
        "    \"\"\"Fetch fresh bars and rebuild features exactly like training.\"\"\"\n",
        "    end = datetime.utcnow()\n",
        "    start = end - timedelta(days=horizon_days)\n",
        "    df_live = yf.download(\n",
        "        symbol,\n",
        "        start=start.strftime(\"%Y-%m-%d\"),\n",
        "        end=end.strftime(\"%Y-%m-%d\"),\n",
        "        interval=interval,\n",
        "        progress=False,\n",
        "        auto_adjust=False,\n",
        "    )\n",
        "    if df_live is None or df_live.empty:\n",
        "        return None\n",
        "    df_live = df_live.reset_index()\n",
        "    df_live[\"Symbol\"] = symbol\n",
        "    df_live = compute_enhanced_features(df_live)\n",
        "    if ENABLE_WAVELET and \"Denoised_Close\" not in df_live.columns:\n",
        "        df_live[\"Denoised_Close\"] = df_live[\"Close\"]\n",
        "    return df_live\n",
        "\n",
        "def predict_latest(symbol, prefix):\n",
        "    \"\"\"Build last window, fast-forward env, call model.predict(), return a signal.\"\"\"\n",
        "    # --- load per-model threshold ---\n",
        "    cfg_path = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_probability_config.json\")\n",
        "    thr = 0.2\n",
        "    if os.path.exists(cfg_path):\n",
        "        try:\n",
        "            with open(cfg_path, \"r\") as f:\n",
        "                thr = float(json.load(f).get(\"threshold\", 0.2))\n",
        "        except Exception:\n",
        "            thr = 0.2\n",
        "\n",
        "    model, make_env = load_model_and_env(prefix)\n",
        "    live_df = latest_df_for_symbol(symbol)\n",
        "    if live_df is None or len(live_df) < 100:\n",
        "        logging.warning(\"No fresh data yet for live inference.\")\n",
        "        return None\n",
        "\n",
        "    df_window = live_df.iloc[-2500:].reset_index(drop=True) if len(live_df) > 2500 else live_df.copy()\n",
        "\n",
        "    env = make_env(df_window)\n",
        "    obs = env.reset()\n",
        "    if isinstance(obs, tuple):\n",
        "        obs, _ = obs\n",
        "\n",
        "    # fast-forward with HOLD\n",
        "    for _ in range(len(df_window) - 1):\n",
        "        obs, _, dones, _ = env.step([np.array([0.0], dtype=np.float32)])\n",
        "        if isinstance(dones, (np.ndarray, list, tuple)) and len(dones) and dones[0]:\n",
        "            break\n",
        "\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    mu, sigma = get_mu_sigma(model, obs)\n",
        "\n",
        "    a = float(np.array(action).squeeze())\n",
        "\n",
        "    # --- thresholded signal using loaded thr ---\n",
        "    if a > thr:\n",
        "        signal = \"BUY\"\n",
        "    elif a < -thr:\n",
        "        signal = \"SELL\"\n",
        "    else:\n",
        "        signal = \"HOLD\"\n",
        "\n",
        "    conf = abs(a)\n",
        "    ts = df_window[\"Datetime\"].iloc[-1] if \"Datetime\" in df_window.columns else None\n",
        "    price = float(df_window[\"Close\"].iloc[-1])\n",
        "\n",
        "    return dict(\n",
        "        signal=signal,\n",
        "        confidence=conf,\n",
        "        action=a,\n",
        "        threshold=thr,\n",
        "        ts=ts,\n",
        "        price=price,\n",
        "        mu=mu,\n",
        "        sigma=sigma,\n",
        "    )\n",
        "\n",
        "def place_order(signal, qty=1):\n",
        "    \"\"\"Stub broker router with latency simulation; logs in Colab.\"\"\"\n",
        "    if SIM_LATENCY_MS > 0:\n",
        "        time.sleep(SIM_LATENCY_MS / 1000.0)\n",
        "    if BROKER == \"log\":\n",
        "        logging.info(f\"[PAPER] {signal} x{qty}\")\n",
        "    else:\n",
        "        logging.info(f\"[BROKER={BROKER}] {signal} x{qty} (not implemented)\")\n",
        "\n",
        "def live_loop(symbol, best_prefix):\n",
        "    \"\"\"Simple polling loop—set LIVE_MODE=True to run.\"\"\"\n",
        "    while LIVE_MODE:\n",
        "        try:\n",
        "            pred = predict_latest(symbol, best_prefix)\n",
        "            if pred:\n",
        "                logging.info(\n",
        "                    f\"{symbol} {pred['ts']} | {pred['signal']} \"\n",
        "                    f\"@ {pred['price']:.2f} (conf {pred['confidence']:.2f})\"\n",
        "                )\n",
        "                place_order(pred[\"signal\"], qty=1)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Live loop error: {e}\")\n",
        "        time.sleep(60)  # Poll each minute\n",
        "\n",
        "TOP_N_WINDOWS = 3\n",
        "\n",
        "FAST = {\n",
        "    \"lr\": 8e-5,\n",
        "    \"n_steps\": 3072,\n",
        "    \"batch\": 512,\n",
        "    \"clip\": 0.2,\n",
        "    \"ent\": 0.01,\n",
        "}\n",
        "\n",
        "SLOW = {\n",
        "    \"lr\": 3e-5,\n",
        "    \"n_steps\": 3072,\n",
        "    \"batch\": 512,\n",
        "    \"clip\": 0.16,\n",
        "    \"ent\": 0.005,\n",
        "}\n",
        "\n",
        "fast_names = {\n",
        "    \"TSLA\",\"NVDA\",\"AMD\",\"AVGO\",\"AAPL\",\"MSFT\",\"AMZN\",\"GOOGL\",\"META\",\"ADBE\",\"CRM\",\n",
        "    \"INTC\",\"QCOM\",\"TXN\",\"ORCL\",\"NEE\",\"GE\",\"XOM\",\"CVX\",\"LLY\",\"NKE\",\"SBUX\"\n",
        "}\n",
        "slow_names = {\n",
        "    \"BRK-B\",\"JPM\",\"BAC\",\"JNJ\",\"UNH\",\"MRK\",\"PFE\",\"ABBV\",\"ABT\",\"AMGN\",\"PG\",\"PEP\",\"KO\",\n",
        "    \"V\",\"MA\",\"WMT\",\"MCD\",\"TMO\",\"DHR\",\"ACN\",\"IBM\",\"LIN\",\"PM\",\"RTX\",\"UPS\",\"UNP\",\"COST\",\"HD\",\"LOW\"\n",
        "}\n",
        "\n",
        "def pick_params(symbol: str):\n",
        "    return FAST if symbol in fast_names else SLOW\n",
        "\n",
        "def export_qc_top_from_existing(ticker: str, top_n: int = 3):\n",
        "    \"\"\"\n",
        "    If a ticker is fully skipped (models already exist), still populate QC_TOP_DIR.\n",
        "    Uses existing summary CSVs to pick top Sharpe windows, then copies artifacts from FINAL_MODEL_DIR.\n",
        "    Prefers using 'Prefix' from summaries (robust). Falls back to WindowIdx reconstruction.\n",
        "    \"\"\"\n",
        "    summary_files = glob.glob(os.path.join(BASE_RESULTS_DIR, \"ppo_walkforward_results_*\", \"summary*.csv\"))\n",
        "    if not summary_files:\n",
        "        logging.warning(f\"[QC_TOP] No summary files found; cannot export QC_TOP for {ticker}.\")\n",
        "        return\n",
        "\n",
        "    frames = []\n",
        "    for p in summary_files:\n",
        "        try:\n",
        "            tmp = pd.read_csv(p)\n",
        "            frames.append(tmp)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"[QC_TOP] Could not read {p}: {e}\")\n",
        "\n",
        "    if not frames:\n",
        "        logging.warning(f\"[QC_TOP] Could not read any summary files; cannot export QC_TOP for {ticker}.\")\n",
        "        return\n",
        "\n",
        "    combo = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "    if \"Ticker\" not in combo.columns:\n",
        "        logging.warning(\"[QC_TOP] Summary files missing 'Ticker' column; cannot export.\")\n",
        "        return\n",
        "\n",
        "    combo = combo[combo[\"Ticker\"] == ticker].copy()\n",
        "    if combo.empty or \"Sharpe\" not in combo.columns:\n",
        "        logging.warning(f\"[QC_TOP] No rows for {ticker} in summaries (or missing Sharpe); cannot export QC_TOP.\")\n",
        "        return\n",
        "\n",
        "    # Ensure Sharpe is numeric so sorting works reliably\n",
        "    combo[\"Sharpe\"] = pd.to_numeric(combo[\"Sharpe\"], errors=\"coerce\")\n",
        "    combo = combo.dropna(subset=[\"Sharpe\"])\n",
        "    if combo.empty:\n",
        "        logging.warning(f\"[QC_TOP] All Sharpe values were non-numeric for {ticker}; cannot export.\")\n",
        "        return\n",
        "\n",
        "    use_prefix = (\"Prefix\" in combo.columns) and combo[\"Prefix\"].notna().any()\n",
        "\n",
        "    if use_prefix:\n",
        "        # Robust path: use saved Prefix directly\n",
        "        top = combo.sort_values(\"Sharpe\", ascending=False).head(top_n).copy()\n",
        "        top[\"__prefix__\"] = top[\"Prefix\"].astype(str)\n",
        "    else:\n",
        "        # Fallback: reconstruct WindowIdx (less robust)\n",
        "        def _window_start(w):\n",
        "            try:\n",
        "                s = str(w)\n",
        "                return int(s.split(\"-\")[0]) if \"-\" in s else np.nan\n",
        "            except Exception:\n",
        "                return np.nan\n",
        "\n",
        "        combo[\"WindowStart\"] = combo[\"Window\"].apply(_window_start)\n",
        "        combo = combo.sort_values([\"WindowStart\"]).reset_index(drop=True)\n",
        "        combo[\"WindowIdx\"] = combo.groupby(\"Ticker\").cumcount() + 1\n",
        "\n",
        "        top = combo.sort_values(\"Sharpe\", ascending=False).head(top_n).copy()\n",
        "        top[\"__prefix__\"] = top[\"WindowIdx\"].apply(lambda widx: f\"ppo_{ticker}_window{int(widx)}\")\n",
        "\n",
        "    exported = 0\n",
        "\n",
        "    for _, r in top.iterrows():\n",
        "        prefix = str(r[\"__prefix__\"])\n",
        "\n",
        "        model_path = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_model.zip\")\n",
        "        vec_path   = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_vecnorm.pkl\")\n",
        "\n",
        "        if not (os.path.exists(model_path) and os.path.exists(vec_path)):\n",
        "            logging.warning(f\"[QC_TOP] Missing model/vecnorm for {prefix}; cannot export.\")\n",
        "            continue\n",
        "\n",
        "        artifact_for_save = {\n",
        "            \"model\": None,\n",
        "            \"model_path\": model_path,\n",
        "            \"vecnorm_path\": vec_path,\n",
        "            \"features\": [],         # ok if unknown; QC can load features elsewhere\n",
        "            \"result\": r.to_dict(),  # includes Sharpe, Action_Threshold, etc if present\n",
        "            \"prefix\": prefix,\n",
        "        }\n",
        "        save_quantconnect_model(artifact_for_save, prefix, QC_TOP_DIR)\n",
        "        exported += 1\n",
        "\n",
        "    logging.info(f\"[QC_TOP] Exported {exported}/{len(top)} QC artifacts for {ticker}.\")\n",
        "\n",
        "def walkforward_ppo(df_sym, ticker,\n",
        "                    window_size=3500, step_size=500,\n",
        "                    timesteps=150_000, learning_rate=1e-4,\n",
        "                    ppo_overrides=None):\n",
        "    import heapq\n",
        "\n",
        "    if ppo_overrides is None:\n",
        "        ppo_overrides = {}\n",
        "\n",
        "    if len(df_sym) < window_size:\n",
        "        logging.warning(\n",
        "            f\"Skipping {ticker}: only {len(df_sym)} rows (min required: {window_size})\"\n",
        "        )\n",
        "        return []\n",
        "\n",
        "    results = []\n",
        "    windows = get_walk_forward_windows(df_sym, window_size, step_size)\n",
        "    top_heap = []\n",
        "    skipped_windows = []\n",
        "\n",
        "    # quick check: all windows already have model+vecnorm?\n",
        "    all_done = True\n",
        "    for idx in range(len(windows)):\n",
        "        prefix = f\"ppo_{ticker}_window{idx+1}\"\n",
        "        model_ok   = os.path.exists(os.path.join(FINAL_MODEL_DIR, f\"{prefix}_model.zip\"))\n",
        "        vecnorm_ok = os.path.exists(os.path.join(FINAL_MODEL_DIR, f\"{prefix}_vecnorm.pkl\"))\n",
        "        if not (model_ok and vecnorm_ok):\n",
        "            all_done = False\n",
        "            break\n",
        "\n",
        "    if all_done:\n",
        "        logging.info(f\"Ticker {ticker} fully skipped (all {len(windows)} windows already complete).\")\n",
        "        record_skips_global(ticker, skipped_windows=[], total_windows=len(windows), fully_skipped=True)\n",
        "\n",
        "        export_qc_top_from_existing(ticker, top_n=TOP_N_WINDOWS)\n",
        "        return []\n",
        "\n",
        "\n",
        "\n",
        "    for w_idx, (start, end) in enumerate(windows):\n",
        "        window_start_time = time.time()\n",
        "        gc.collect()\n",
        "\n",
        "        prefix = f\"ppo_{ticker}_window{w_idx+1}\"\n",
        "        model_path   = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_model.zip\")\n",
        "        vecnorm_path = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_vecnorm.pkl\")\n",
        "\n",
        "        if os.path.exists(model_path) and os.path.exists(vecnorm_path):\n",
        "            logging.info(f\"Skipping {ticker} | Window {w_idx+1}, already trained.\")\n",
        "            skipped_windows.append(f\"{ticker}_window{w_idx+1}\")\n",
        "            continue\n",
        "\n",
        "        missing = []\n",
        "        if not os.path.exists(model_path):   missing.append(\"model.zip\")\n",
        "        if not os.path.exists(vecnorm_path): missing.append(\"vecnorm.pkl\")\n",
        "        logging.info(\n",
        "            f\"Will train {ticker} | Window {w_idx+1} because missing: {', '.join(missing)}\"\n",
        "        )\n",
        "\n",
        "        df_window = df_sym.iloc[start:end].reset_index(drop=True)\n",
        "        if len(df_window) <= 52 or len(df_window) % 2 != 0:\n",
        "            df_window = df_window.iloc[:-1]\n",
        "\n",
        "        frame_bound = (50, len(df_window) - 3)\n",
        "\n",
        "        env = DummyVecEnv([lambda: ContinuousPositionEnv(\n",
        "          df=df_window, frame_bound=frame_bound, **ENV_KWARGS\n",
        "        )])\n",
        "\n",
        "        env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
        "\n",
        "        try:\n",
        "            model = PPO(\n",
        "                \"MlpPolicy\",\n",
        "                env,\n",
        "                verbose=0,\n",
        "                device=(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "                learning_rate=ppo_overrides.get(\"lr\", learning_rate),\n",
        "                n_steps=ppo_overrides.get(\"n_steps\", 256),\n",
        "                batch_size=ppo_overrides.get(\"batch\", 64),\n",
        "                n_epochs=5,\n",
        "                gamma=0.99,\n",
        "                gae_lambda=0.95,\n",
        "                clip_range=ppo_overrides.get(\"clip\", 0.2),\n",
        "                ent_coef=ppo_overrides.get(\"ent\", 0.005),\n",
        "                policy_kwargs=dict(net_arch=[64, 64]),\n",
        "            )\n",
        "\n",
        "            logging.info(f\"Training {ticker} Window {w_idx+1}/{len(windows)}\")\n",
        "            model.learn(total_timesteps=timesteps)\n",
        "\n",
        "            # Evaluation pass\n",
        "            env.training = False\n",
        "            env.norm_reward = False\n",
        "            obs = env.reset()\n",
        "            if isinstance(obs, tuple):\n",
        "                obs, _ = obs\n",
        "\n",
        "            nav_track = [1.0]\n",
        "            bh_track  = [1.0]\n",
        "            step_log  = []\n",
        "            executed_trade_count = 0\n",
        "            signal_trade_count   = 0\n",
        "            signal_trade_count_dyn   = 0   # dynamic-threshold diagnostic\n",
        "\n",
        "            DIAG_THR = 0.2\n",
        "            for i in range(len(df_window) - 1):\n",
        "                action, _ = model.predict(obs, deterministic=True)\n",
        "                mu, sigma = get_mu_sigma(model, obs)\n",
        "\n",
        "                obs, rew, dones, infos = env.step(action)\n",
        "                # VecEnv returns list/tuple of infos; otherwise it may be a dict\n",
        "                if isinstance(infos, (list, tuple)):\n",
        "                    info = infos[0] if len(infos) else {}\n",
        "                elif isinstance(infos, dict):\n",
        "                    info = infos\n",
        "                else:\n",
        "                    info = {}\n",
        "\n",
        "\n",
        "                nav_track.append(float(info.get(\"nav\", nav_track[-1])))\n",
        "                bh_track.append(\n",
        "                    bh_track[-1] * (1.0 + float(info.get(\"ret_t\", 0.0)))\n",
        "                )\n",
        "\n",
        "                a = float(np.array(action).squeeze())\n",
        "                dt_val = df_window[\"Datetime\"].iloc[i+1] if \"Datetime\" in df_window.columns else None\n",
        "                px     = float(df_window[\"Close\"].iloc[i+1]) if \"Close\" in df_window.columns else np.nan\n",
        "                #“signal” trades (threshold-based) — diagnostic only\n",
        "                if a > DIAG_THR or a < -DIAG_THR:\n",
        "                    signal_trade_count += 1\n",
        "                #real trades executed by env friction logic\n",
        "                if bool(info.get(\"executed_trade\", False)):\n",
        "                    executed_trade_count += 1\n",
        "\n",
        "                # next-bar return (to score BUY/SELL vs the *next* move)\n",
        "                if i + 2 < len(df_window):\n",
        "                    p0 = float(df_window[\"Close\"].iloc[i+1])\n",
        "                    p1 = float(df_window[\"Close\"].iloc[i+2])\n",
        "                    next_ret = 0.0 if p0 <= 0 else (p1 - p0) / p0\n",
        "                else:\n",
        "                    next_ret = 0.0\n",
        "\n",
        "                # reward scalar (VecEnv returns arrays)\n",
        "                rew_val = float(rew[0]) if isinstance(rew, (list, tuple, np.ndarray)) else float(rew)\n",
        "\n",
        "                step_log.append({\n",
        "                    \"Index\": i+1,\n",
        "                    \"Datetime\": dt_val,\n",
        "                    \"Close\": px,\n",
        "                    \"Action\": a,\n",
        "                    \"mu\": mu,\n",
        "                    \"sigma\": sigma,\n",
        "                    \"nav\": nav_track[-1],\n",
        "                    \"ret_t\": float(info.get(\"ret_t\", 0.0)),\n",
        "                    \"next_ret\": float(next_ret),\n",
        "                    \"reward\": rew_val,\n",
        "                    \"pos\": float(info.get(\"pos\", 0.0)),\n",
        "                    \"trade_cost\": float(info.get(\"trade_cost\", 0.0)),\n",
        "                    \"base_ret\": float(info.get(\"base_ret\", 0.0)),\n",
        "                    \"rel_alpha\": float(info.get(\"rel_alpha\", 0.0)),\n",
        "                    \"mom\": float(info.get(\"mom\", 0.0)),\n",
        "                })\n",
        "\n",
        "                # done handling (VecEnv)\n",
        "                if isinstance(dones, (np.ndarray, list, tuple)):\n",
        "                    if dones[0]:\n",
        "                        break\n",
        "                elif dones:\n",
        "                    break\n",
        "\n",
        "\n",
        "            # --- Metrics ---\n",
        "            final_value = float(nav_track[-1]) * 100_000.0\n",
        "            hold_value  = float(bh_track[-1])  * 100_000.0\n",
        "\n",
        "            #dynamic action threshold for this window (prevents “no signals” windows)\n",
        "            abs_actions = np.array([abs(float(r[\"Action\"])) for r in step_log], dtype=float)\n",
        "            if len(abs_actions) > 0:\n",
        "                thr = float(np.quantile(abs_actions, 0.70))  # 70th percentile\n",
        "                thr = float(np.clip(thr, 0.08, 0.30))\n",
        "            else:\n",
        "                thr = 0.2\n",
        "\n",
        "            # Dynamic signal trade count (post-hoc diagnostic)\n",
        "            signal_trade_count_dyn = int(np.sum(abs_actions > thr)) if len(abs_actions) > 0 else 0\n",
        "\n",
        "\n",
        "            returns = pd.Series(nav_track).pct_change().fillna(0.0)\n",
        "            sharpe  = float((returns.mean() / (returns.std() + 1e-9)) * _annualization_factor(df_window))\n",
        "            drawdown = float(\n",
        "                ((pd.Series(nav_track).cummax() - pd.Series(nav_track)) /\n",
        "                pd.Series(nav_track).cummax()).max() * 100.0\n",
        "            )\n",
        "\n",
        "            # Classification stats (now using thr)\n",
        "            correct = 0\n",
        "            total   = 0\n",
        "            tp_buy = fp_buy = 0\n",
        "            tp_sell = fp_sell = 0\n",
        "\n",
        "            for r in step_log:\n",
        "                a = float(r[\"Action\"])\n",
        "                ret_t = float(r.get(\"next_ret\", 0.0))\n",
        "\n",
        "                if a > thr:\n",
        "                    sig = \"BUY\"\n",
        "                elif a < -thr:\n",
        "                    sig = \"SELL\"\n",
        "                else:\n",
        "                    sig = \"HOLD\"\n",
        "\n",
        "                if sig == \"BUY\":\n",
        "                    if ret_t > 0:\n",
        "                        tp_buy += 1; correct += 1\n",
        "                    else:\n",
        "                        fp_buy += 1\n",
        "                    total += 1\n",
        "                elif sig == \"SELL\":\n",
        "                    if ret_t < 0:\n",
        "                        tp_sell += 1; correct += 1\n",
        "                    else:\n",
        "                        fp_sell += 1\n",
        "                    total += 1\n",
        "                # HOLD not counted\n",
        "\n",
        "            precision_long  = tp_buy  / (tp_buy  + fp_buy  + 1e-9)\n",
        "            precision_short = tp_sell / (tp_sell + fp_sell + 1e-9)\n",
        "            precision_trades = (tp_buy + tp_sell) / (\n",
        "                (tp_buy + tp_sell) + (fp_buy + fp_sell) + 1e-9\n",
        "            )\n",
        "            step_accuracy = round(correct / total, 4) if total > 0 else 0.0\n",
        "            #Trade_count reflect REAL executed trades (cooldown/min_trade_delta)\n",
        "            trade_count = int(executed_trade_count)\n",
        "\n",
        "            # Save VecNormalize\n",
        "            try:\n",
        "                env.save(vecnorm_path)\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"Could not save VecNormalize for {ticker} {start}-{end}: {e}\")\n",
        "                vecnorm_path = None\n",
        "\n",
        "            # Save model\n",
        "            model.save(model_path)\n",
        "\n",
        "            # Save detailed predictions\n",
        "            pred_path = os.path.join(RUN_RESULTS_DIR, f\"{prefix}_predictions.csv\")\n",
        "            pd.DataFrame(step_log).to_csv(pred_path, index=False)\n",
        "            logging.info(f\"Saved predictions to {pred_path}\")\n",
        "\n",
        "            # Save compat predictions with same thresholds as metrics\n",
        "            compat_rows = []\n",
        "            for r in step_log:\n",
        "                a = r[\"Action\"]\n",
        "\n",
        "                if a > thr:\n",
        "                    signal = \"BUY\"\n",
        "                elif a < -thr:\n",
        "                    signal = \"SELL\"\n",
        "                else:\n",
        "                    signal = \"HOLD\"\n",
        "                compat_rows.append({\n",
        "                    \"Index\": r[\"Index\"],\n",
        "                    \"Datetime\": r[\"Datetime\"],\n",
        "                    \"Close\": r[\"Close\"],\n",
        "                    \"Action\": a,\n",
        "                    \"Signal\": signal,\n",
        "                    \"PortfolioValue\": r[\"nav\"],\n",
        "                    \"Reward\": r.get(\"reward\", np.nan),\n",
        "                })\n",
        "            compat_path = os.path.join(RUN_RESULTS_DIR, f\"{prefix}_predictions_compat.csv\")\n",
        "            pd.DataFrame(compat_rows).to_csv(compat_path, index=False)\n",
        "            logging.info(f\"Saved compatibility predictions to {compat_path}\")\n",
        "\n",
        "            # Summary row\n",
        "            result_row = {\n",
        "                \"Ticker\": ticker,\n",
        "                \"Window\": f\"{start}-{end}\",\n",
        "                \"WindowIdx\": int(w_idx + 1),\n",
        "                \"Prefix\": prefix,\n",
        "                \"PPO_Portfolio\": round(final_value, 2),\n",
        "                \"BuyHold\": round(hold_value, 2),\n",
        "                \"Sharpe\": round(sharpe, 3),\n",
        "                \"Drawdown_%\": round(drawdown, 2),\n",
        "                \"Winner\": \"PPO\" if final_value > hold_value else \"Buy & Hold\",\n",
        "                \"Action_Threshold\": round(thr, 4),\n",
        "                \"Accuracy\": step_accuracy,\n",
        "                \"Trade_Count\": trade_count,\n",
        "                \"Signal_Trade_Count\": int(signal_trade_count),\n",
        "                \"Signal_Trade_Count_Dyn\": int(signal_trade_count_dyn),\n",
        "                \"Executed_Trade_Count\": int(executed_trade_count),\n",
        "                \"Precision_Long\": round(precision_long, 4),\n",
        "                \"Precision_Short\": round(precision_short, 4),\n",
        "                \"Precision_Trades\": round(precision_trades, 4),\n",
        "            }\n",
        "\n",
        "            results.append(result_row)\n",
        "\n",
        "            meta = {\n",
        "                \"result\": result_row,\n",
        "                \"features\": df_window.columns.tolist(),\n",
        "                \"prefix\": prefix,\n",
        "                \"model_path\": model_path,\n",
        "                \"vecnorm_path\": vecnorm_path,\n",
        "            }\n",
        "\n",
        "            item = (result_row[\"Sharpe\"], prefix, meta)\n",
        "            if len(top_heap) < TOP_N_WINDOWS:\n",
        "                heapq.heappush(top_heap, item)\n",
        "            else:\n",
        "                if item[0] > top_heap[0][0]:\n",
        "                    heapq.heapreplace(top_heap, item)\n",
        "\n",
        "            logging.info(\n",
        "                f\"{ticker} | Window {w_idx+1} runtime: \"\n",
        "                f\"{round(time.time() - window_start_time, 2)}s\"\n",
        "            )\n",
        "        finally:\n",
        "            try:\n",
        "                env.close()\n",
        "            except Exception:\n",
        "                pass\n",
        "            del env\n",
        "            try:\n",
        "                del model\n",
        "            except Exception:\n",
        "                pass\n",
        "            gc.collect()\n",
        "            try:\n",
        "                torch.cuda.empty_cache()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    if skipped_windows:\n",
        "        logging.info(\n",
        "            f\"{ticker} skipped windows (already complete): {', '.join(skipped_windows)}\"\n",
        "        )\n",
        "        record_skips_global(\n",
        "            ticker,\n",
        "            skipped_windows=skipped_windows,\n",
        "            total_windows=len(windows),\n",
        "            fully_skipped=False,\n",
        "        )\n",
        "\n",
        "    # Save top-N QC-compatible\n",
        "    top_list = sorted(top_heap, key=lambda t: t[0], reverse=True)\n",
        "    for _, _, meta in top_list:\n",
        "        artifact_for_save = {\n",
        "            \"model\": None,  # we're copying from disk, not re-saving an in-memory model\n",
        "            \"model_path\": meta[\"model_path\"],\n",
        "            \"vecnorm_path\": meta[\"vecnorm_path\"],\n",
        "            \"features\": meta[\"features\"],\n",
        "            \"result\": meta[\"result\"],\n",
        "            \"prefix\": meta[\"prefix\"],\n",
        "        }\n",
        "        save_quantconnect_model(artifact_for_save, meta[\"prefix\"], QC_TOP_DIR)\n",
        "\n",
        "    return results\n",
        "\n",
        "def process_ticker(ticker):\n",
        "    try:\n",
        "        hp = pick_params(ticker)\n",
        "        return walkforward_ppo(\n",
        "            df[df[\"Symbol\"] == ticker].copy(),\n",
        "            ticker,\n",
        "            window_size=WINDOW_SIZE,\n",
        "            step_size=STEP_SIZE,\n",
        "            timesteps=TIMESTEPS,\n",
        "            learning_rate=hp[\"lr\"],\n",
        "            ppo_overrides=hp,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logging.error(f\"{ticker}: training failed with {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def run_parallel_tickers(tickers,\n",
        "                         out_path=os.path.join(RUN_RESULTS_DIR, \"summary.csv\"),\n",
        "                         max_workers=8):\n",
        "    results = []\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
        "        for res in ex.map(process_ticker, tickers):\n",
        "            if res:\n",
        "                results.extend(res)\n",
        "\n",
        "    if results:\n",
        "        pd.DataFrame(results).to_csv(out_path, index=False)\n",
        "        logging.info(f\"Saved summary to {out_path}\")\n",
        "    else:\n",
        "        logging.warning(\"No results produced; summary not written.\")\n",
        "\n",
        "    logging.info(\"All tickers processed.\")\n",
        "    return results\n",
        "\n",
        "def build_ppo_selector():\n",
        "    \"\"\"Aggregate all summary*.csv across runs and build selector JSON.\"\"\"\n",
        "    summary_files = glob.glob(\n",
        "        os.path.join(BASE_RESULTS_DIR, \"ppo_walkforward_results_*\", \"summary*.csv\")\n",
        "    )\n",
        "    all_summaries = []\n",
        "\n",
        "    for p in summary_files:\n",
        "        try:\n",
        "            tmp = pd.read_csv(p)\n",
        "            tmp[\"RunFolder\"] = os.path.dirname(p)\n",
        "            all_summaries.append(tmp)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Skipping {p} due to error: {e}\")\n",
        "\n",
        "    if not all_summaries:\n",
        "        logging.warning(\"No PPO summaries found across walkforward results folders.\")\n",
        "        return\n",
        "\n",
        "    combo = pd.concat(all_summaries, ignore_index=True)\n",
        "    if \"Sharpe\" in combo.columns:\n",
        "    combo[\"Sharpe\"] = pd.to_numeric(combo[\"Sharpe\"], errors=\"coerce\")\n",
        "    combo = combo.dropna(subset=[\"Sharpe\"])\n",
        "\n",
        "    # Ensure key columns exist for robust ratios\n",
        "    if \"BuyHold\" not in combo.columns:\n",
        "        combo[\"BuyHold\"] = np.nan\n",
        "    if \"PPO_Portfolio\" not in combo.columns:\n",
        "        combo[\"PPO_Portfolio\"] = np.nan\n",
        "\n",
        "    # parse Window \"start-end\" to WindowStart\n",
        "    def _parse_window_start(w):\n",
        "        if pd.isna(w):\n",
        "            return None\n",
        "        if isinstance(w, (int, float)):\n",
        "            return int(w)\n",
        "        parts = str(w).split(\"-\")\n",
        "        try:\n",
        "            return int(parts[0])\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    combo[\"WindowStart\"] = combo[\"Window\"].apply(_parse_window_start)\n",
        "\n",
        "    combo = combo.sort_values([\"Ticker\", \"WindowStart\"]).reset_index(drop=True)\n",
        "    combo[\"WindowIdx\"] = combo.groupby(\"Ticker\").cumcount() + 1\n",
        "\n",
        "    combo = combo.drop_duplicates(subset=[\"Ticker\", \"WindowIdx\"], keep=\"last\")\n",
        "\n",
        "    best_by_symbol = (\n",
        "        combo\n",
        "        .sort_values(\"Sharpe\", ascending=False)\n",
        "        .groupby(\"Ticker\")\n",
        "        .first()\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # If Drawdown_% missing (older runs), create it so rename won't break\n",
        "    if \"Drawdown_%\" not in best_by_symbol.columns:\n",
        "        best_by_symbol[\"Drawdown_%\"] = np.nan\n",
        "\n",
        "    # Ensure precision cols exist\n",
        "    for col in [\"Precision_Long\", \"Precision_Short\", \"Precision_Trades\"]:\n",
        "        if col not in best_by_symbol.columns:\n",
        "            best_by_symbol[col] = None  # or np.nan if you prefer\n",
        "\n",
        "    # Rename columns so everything downstream uses consistent names\n",
        "    best_by_symbol = best_by_symbol.rename(columns={\n",
        "        \"Drawdown_%\": \"Drawdown\",\n",
        "        \"PPO_Portfolio\": \"Final_Portfolio\",\n",
        "    })\n",
        "\n",
        "    # Ensure Accuracy / Trade_Count exist\n",
        "    if \"Accuracy\" not in best_by_symbol.columns:\n",
        "        best_by_symbol[\"Accuracy\"] = 0.0\n",
        "    if \"Trade_Count\" not in best_by_symbol.columns:\n",
        "        best_by_symbol[\"Trade_Count\"] = None\n",
        "\n",
        "    best_by_symbol[\"Model\"] = MODEL_NAME\n",
        "\n",
        "    # PPO vs Buy & Hold ratio (safe division)\n",
        "    best_by_symbol[\"Rel_vs_BH\"] = best_by_symbol.apply(\n",
        "        lambda r: (r[\"Final_Portfolio\"] / r[\"BuyHold\"])\n",
        "        if (pd.notna(r[\"BuyHold\"]) and r[\"BuyHold\"] not in (0, 0.0)) else np.nan,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Save flat CSV for debugging\n",
        "    best_by_symbol.to_csv(SELECTOR_FULL_PATH, index=False)\n",
        "    print(f\"Aggregated PPO selector saved to → {SELECTOR_FULL_PATH}\")\n",
        "\n",
        "    # Safety filters (tune as needed)\n",
        "    df_sel = best_by_symbol.copy()\n",
        "    gates = (\n",
        "        (df_sel[\"Sharpe\"].fillna(-999) > 0.0) &\n",
        "        (df_sel[\"Drawdown\"].fillna(999) < 50.0) &\n",
        "        (df_sel[\"Final_Portfolio\"].fillna(0) > 80_000) &\n",
        "        (df_sel[\"Rel_vs_BH\"].fillna(0) >= 0.95)   # PPO ≥ 95% of B&H; change to >1.0 to enforce beat\n",
        "    )\n",
        "    df_sel = df_sel[gates].copy()\n",
        "\n",
        "    df_sel[\"prefix\"] = (\n",
        "        \"ppo_\"\n",
        "        + df_sel[\"Ticker\"].astype(str)\n",
        "        + \"_window\"\n",
        "        + df_sel[\"WindowIdx\"].astype(int).astype(str)\n",
        "    )\n",
        "\n",
        "    df_sel[\"artifact_path\"] = df_sel[\"prefix\"].apply(\n",
        "        lambda p: os.path.join(FINAL_MODEL_DIR, f\"{p}_model.zip\")\n",
        "    )\n",
        "    df_sel[\"vecnorm_path\"] = df_sel[\"prefix\"].apply(\n",
        "        lambda p: os.path.join(FINAL_MODEL_DIR, f\"{p}_vecnorm.pkl\")\n",
        "    )\n",
        "\n",
        "    EPS = 0.03  # 3% of top-sharpe for \"close enough\"\n",
        "    selected_models = {}\n",
        "\n",
        "    def safe_int(v, default=0):\n",
        "        if v is None:\n",
        "            return int(default)\n",
        "        try:\n",
        "            import math\n",
        "            if isinstance(v, float) and math.isnan(v):\n",
        "                return int(default)\n",
        "        except TypeError:\n",
        "            pass\n",
        "        try:\n",
        "            return int(v)\n",
        "        except (ValueError, TypeError):\n",
        "            return int(default)\n",
        "\n",
        "    def safe_float(v, default=0.0):\n",
        "        if v is None:\n",
        "            return float(default)\n",
        "        try:\n",
        "            import math\n",
        "            if isinstance(v, float) and math.isnan(v):\n",
        "                return float(default)\n",
        "        except TypeError:\n",
        "            pass\n",
        "        try:\n",
        "            return float(v)\n",
        "        except (ValueError, TypeError):\n",
        "            return float(default)\n",
        "\n",
        "    for ticker, group in df_sel.groupby(\"Ticker\"):\n",
        "        group_sorted = group.sort_values(\"Sharpe\", ascending=False)\n",
        "        top = group_sorted.iloc[0]\n",
        "        second = group_sorted.iloc[1] if len(group_sorted) > 1 else None\n",
        "\n",
        "        if (second is not None) and (\n",
        "            abs(top[\"Sharpe\"] - second[\"Sharpe\"]) <= abs(top[\"Sharpe\"]) * EPS\n",
        "        ):\n",
        "            mode = \"ensemble\"\n",
        "            primary, secondary = top[\"Model\"], second[\"Model\"]\n",
        "        else:\n",
        "            mode = \"single\"\n",
        "            primary, secondary = top[\"Model\"], None\n",
        "\n",
        "        selected_models[ticker] = {\n",
        "            \"model\": MODEL_NAME,\n",
        "            \"score\": round(safe_float(top[\"Sharpe\"]), 4),\n",
        "            \"return\": round(safe_float(top[\"Final_Portfolio\"]), 2),\n",
        "            \"sharpe\": round(safe_float(top[\"Sharpe\"]), 3),\n",
        "            \"drawdown\": round(safe_float(top[\"Drawdown\"]), 2),\n",
        "            \"sortino\": None,\n",
        "            \"turnover\": None,\n",
        "            \"trade_count\": safe_int(top.get(\"Trade_Count\", 0)),\n",
        "            \"precision\": {\n",
        "                \"long\":   safe_float(top.get(\"Precision_Long\", 0.0)),\n",
        "                \"short\":  safe_float(top.get(\"Precision_Short\", 0.0)),\n",
        "                \"trades\": safe_float(top.get(\"Precision_Trades\", 0.0)),\n",
        "            },\n",
        "            \"stability\": {},\n",
        "            \"regime\": \"unknown\",\n",
        "            \"rl_profile\": \"fast\",\n",
        "            \"artifact\": {\n",
        "                \"path\": top[\"artifact_path\"],\n",
        "                \"vecnorm\": top[\"vecnorm_path\"],\n",
        "                \"features\": None,\n",
        "                \"load_ms\": 180,\n",
        "                \"mem_mb\": 512,\n",
        "                \"exists\": os.path.exists(top[\"artifact_path\"]),\n",
        "            },\n",
        "            \"selection\": {\n",
        "                \"mode\": mode,\n",
        "                \"primary\": primary,\n",
        "                \"secondary\": secondary,\n",
        "            },\n",
        "        }\n",
        "\n",
        "    with open(SELECTOR_JSON_PATH, \"w\") as f:\n",
        "        json.dump(selected_models, f, indent=2)\n",
        "\n",
        "    print(f\"Final enhanced PPO selector JSON saved to → {SELECTOR_JSON_PATH}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.info(f\"RUN_RESULTS_DIR   = {RUN_RESULTS_DIR}\")\n",
        "    logging.info(f\"FINAL_MODEL_DIR  = {FINAL_MODEL_DIR}\")\n",
        "    logging.info(f\"BASE_RESULTS_DIR = {BASE_RESULTS_DIR}\")\n",
        "\n",
        "    min_rows = WINDOW_SIZE + 50  # small buffer so we have at least one window\n",
        "    all_symbols = df[\"Symbol\"].value_counts()\n",
        "    candidate_symbols = []\n",
        "\n",
        "    for sym, n in all_symbols.items():\n",
        "        if n >= min_rows:\n",
        "            candidate_symbols.append(sym)\n",
        "        else:\n",
        "            logging.warning(f\"Skipping {sym}: only {n} rows (< {min_rows} required)\")\n",
        "\n",
        "    if not candidate_symbols:\n",
        "        logging.error(\"No symbols have enough rows for the current WINDOW_SIZE. Nothing to train.\")\n",
        "    else:\n",
        "        logging.info(f\"Training candidate symbols: {candidate_symbols}\")\n",
        "\n",
        "    needed_cols = [\"Close\", \"Datetime\"]\n",
        "    if ENABLE_WAVELET:\n",
        "        needed_cols.append(\"Denoised_Close\")\n",
        "    if ENABLE_SENTIMENT:\n",
        "        needed_cols.append(\"SentimentScore\")\n",
        "\n",
        "    valid_symbols = []\n",
        "    for sym in candidate_symbols:\n",
        "        cols = set(df.loc[df[\"Symbol\"] == sym].columns)\n",
        "        missing = [c for c in needed_cols if c not in cols]\n",
        "        if missing:\n",
        "            logging.warning(f\"Skipping {sym}: missing required cols {missing}\")\n",
        "        else:\n",
        "            valid_symbols.append(sym)\n",
        "\n",
        "    if not valid_symbols:\n",
        "        logging.error(\"No symbols passed the feature/column checks. Nothing to train.\")\n",
        "    else:\n",
        "        logging.info(f\"Final training universe: {valid_symbols}\")\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    if test_mode:\n",
        "        # Optional: shrink timesteps and/or window size in test mode\n",
        "        TIMESTEPS = 100_000   # lighter test\n",
        "        # WINDOW_SIZE = 2000  # uncomment if you want faster test runs\n",
        "        # STEP_SIZE   = 500\n",
        "\n",
        "        test_stocks = [\"AAPL\", \"NVDA\", \"MSFT\"]\n",
        "        present = [s for s in test_stocks if s in valid_symbols]\n",
        "        if not present:\n",
        "            logging.warning(\"Test mode: none of ['AAPL','NVDA','MSFT'] present after filters.\")\n",
        "        else:\n",
        "            logging.info(f\"Test mode: running on {present}\")\n",
        "\n",
        "        for sym in present:\n",
        "            logging.info(f\">>> [TEST_MODE] Processing {sym}\")\n",
        "            res = process_ticker(sym)\n",
        "            logging.info(f\"{sym}: produced {len(res)} window summaries\")\n",
        "            if res:\n",
        "                all_results.extend(res)\n",
        "\n",
        "        summary_path = os.path.join(RUN_RESULTS_DIR, \"summary_test_mode.csv\")\n",
        "        if all_results:\n",
        "            pd.DataFrame(all_results).to_csv(summary_path, index=False)\n",
        "            logging.info(f\"Test-mode summary saved to {summary_path}\")\n",
        "        else:\n",
        "            logging.warning(\"Test mode finished but no results were generated (no windows, or all skipped).\")\n",
        "\n",
        "    else:\n",
        "        logging.info(\"Starting full parallel PPO walkforward run...\")\n",
        "        summary_results = run_parallel_tickers(valid_symbols)\n",
        "        if not summary_results:\n",
        "            logging.warning(\"No results generated in full run (check logs for skips/length issues).\")\n",
        "        else:\n",
        "            summary_path = os.path.join(RUN_RESULTS_DIR, \"summary.csv\")\n",
        "            pd.DataFrame(summary_results).to_csv(summary_path, index=False)\n",
        "            logging.info(f\"Summary saved to {summary_path}\")\n",
        "\n",
        "    try:\n",
        "        build_ppo_selector()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"build_ppo_selector failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5X68r85om00"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "RESULTS_DIR = Path(globals().get(\"RESULTS_DIR\", os.getenv(\"RESULTS_DIR\", \".\")))\n",
        "LATEST_DIR  = Path(globals().get(\"LATEST_DIR\",  os.getenv(\"LATEST_DIR\",  str(RESULTS_DIR))))\n",
        "\n",
        "eq_candidates = [\n",
        "    globals().get(\"EQUITY_LOG_CSV\"),\n",
        "    globals().get(\"EQUITY_LOG_LATEST\"),\n",
        "    RESULTS_DIR / \"equity_log.csv\",\n",
        "    LATEST_DIR / \"equity_log.csv\",\n",
        "]\n",
        "\n",
        "def _first_existing(paths):\n",
        "    for p in paths:\n",
        "        if p:\n",
        "            p = Path(p)\n",
        "            if p.exists() and p.is_file():\n",
        "                return p\n",
        "    return None\n",
        "\n",
        "eq_path = _first_existing(eq_candidates)\n",
        "if eq_path is None:\n",
        "    all_eq = list(RESULTS_DIR.glob(\"equity_log*.csv\")) + list(LATEST_DIR.glob(\"equity_log*.csv\"))\n",
        "    eq_path = max(all_eq, key=lambda p: p.stat().st_mtime, default=None)\n",
        "\n",
        "if eq_path and eq_path.exists():\n",
        "    print(f\"[equity source] {eq_path}\")\n",
        "    try:\n",
        "        eq = pd.read_csv(eq_path, parse_dates=[\"datetime_utc\"]).sort_values(\"datetime_utc\")\n",
        "        if not eq.empty:\n",
        "            r = eq[\"equity\"].pct_change().dropna()\n",
        "            sharpe_h = (r.mean() / (r.std() + 1e-12)) * np.sqrt(252 * 6.5) if len(r) else float(\"nan\")\n",
        "            print(\n",
        "                f\"\\nEquity summary — last: ${eq['equity'].iloc[-1]:,.2f} | \"\n",
        "                f\"n={len(eq)} pts | Sharpe(h): {sharpe_h:.2f} | src={eq_path}\"\n",
        "            )\n",
        "        else:\n",
        "            print(f\"No rows in equity log: {eq_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not summarize equity ({eq_path}): {e}\")\n",
        "else:\n",
        "    print(\"No equity_log*.csv found in RESULTS_DIR/LATEST_DIR.\")\n",
        "\n",
        "def _resolve_tickers():\n",
        "    g = globals().get(\"TICKERS\", None)\n",
        "    if isinstance(g, (list, tuple, set)):\n",
        "        base = [str(x).upper() for x in g]\n",
        "    else:\n",
        "        env_val = os.getenv(\"TICKERS\", (g if isinstance(g, str) else \"\"))\n",
        "        base = [t.strip().upper() for t in str(env_val).split(\",\") if t.strip()]\n",
        "\n",
        "    #Also include symbols with existing logs on disk\n",
        "    discovered = [\n",
        "        p.stem.replace(\"trade_log_\", \"\").upper()\n",
        "        for p in list(RESULTS_DIR.glob(\"trade_log_*.csv\")) + list(LATEST_DIR.glob(\"trade_log_*.csv\"))\n",
        "    ]\n",
        "    ticks = sorted(set(base) | set(discovered))\n",
        "    return ticks if ticks else [\"UNH\", \"GE\"]\n",
        "\n",
        "tickers_to_report = _resolve_tickers()\n",
        "print(\"Tickers to report:\", tickers_to_report)\n",
        "\n",
        "print(\"\\nTrade Summary:\")\n",
        "for ticker in tickers_to_report:\n",
        "    trade_candidates = [\n",
        "        RESULTS_DIR / f\"trade_log_{ticker}.csv\",\n",
        "        LATEST_DIR / f\"trade_log_{ticker}.csv\",\n",
        "    ]\n",
        "    log_path = _first_existing(trade_candidates)\n",
        "    if not log_path:\n",
        "        #Tolerate Drive duplicates like \"trade_log_XYZ (1).csv\"\n",
        "        any_logs = list(RESULTS_DIR.glob(f\"trade_log_{ticker}*.csv\")) + \\\n",
        "                   list(LATEST_DIR.glob(f\"trade_log_{ticker}*.csv\"))\n",
        "        log_path = max(any_logs, key=lambda p: p.stat().st_mtime, default=None)\n",
        "\n",
        "    if not log_path or not log_path.exists():\n",
        "        print(f\"{ticker}: no trades logged yet.\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(\n",
        "            log_path,\n",
        "            on_bad_lines=\"skip\",\n",
        "            engine=\"python\",\n",
        "            parse_dates=[\"log_time\", \"bar_time\"],\n",
        "        )\n",
        "        key = \"signal\" if \"signal\" in df.columns else (\"action\" if \"action\" in df.columns else None)\n",
        "        if key:\n",
        "            counts = df[key].value_counts(dropna=False).to_dict()\n",
        "            print(f\"{ticker}: {counts} | src={log_path.name}\")\n",
        "        else:\n",
        "            print(f\"{ticker}: log present but missing 'signal'/'action' columns. src={log_path.name}\")\n",
        "\n",
        "        if \"confidence\" in df.columns and df[\"confidence\"].notna().any():\n",
        "            plt.figure(figsize=(8, 3.5))\n",
        "            df[\"confidence\"].dropna().plot(kind=\"hist\", bins=10, edgecolor=\"black\")\n",
        "            plt.title(f\"{ticker} - Confidence Distribution\")\n",
        "            plt.xlabel(\"confidence\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        for col in [\"weight\", \"raw_action\"]:\n",
        "            if col in df.columns and df[col].notna().any():\n",
        "                s = df[col].dropna()\n",
        "                print(\n",
        "                    f\"{ticker} {col}: mean={s.mean():.3f}, std={s.std():.3f}, \"\n",
        "                    f\"min={s.min():.3f}, max={s.max():.3f}\"\n",
        "                )\n",
        "    except Exception as e:\n",
        "        print(f\"{ticker}: could not summarize trades ({log_path}): {e}\")\n",
        "\n",
        "try:\n",
        "    if \"api\" not in globals():\n",
        "        api = init_alpaca()\n",
        "    positions = api.list_positions()\n",
        "    total_market_value = 0.0\n",
        "    print(\"\\nPosition Summary:\")\n",
        "    for p in positions:\n",
        "        mv = float(p.market_value)\n",
        "        total_market_value += mv\n",
        "        print(f\"  {p.symbol}: {p.qty} shares @ ${float(p.current_price):.2f} | Value: ${mv:,.2f}\")\n",
        "    print(f\"\\nTotal Market Value: ${total_market_value:,.2f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not summarize positions: {e}\")\n",
        "\n",
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "def count_filled_orders_since(api, symbol: str, days: int = 14) -> int:\n",
        "    after = (datetime.now(timezone.utc) - timedelta(days=days)).isoformat()\n",
        "    orders = api.list_orders(status=\"all\", after=after, nested=True)\n",
        "    return sum(1 for o in orders if o.symbol == symbol and o.status in (\"filled\", \"partially_filled\"))\n",
        "\n",
        "try:\n",
        "    api_chk = api if \"api\" in globals() else init_alpaca()\n",
        "    for sym in tickers_to_report:\n",
        "        n = count_filled_orders_since(api_chk, sym, days=14)\n",
        "        print(f\"{sym}: {n} filled trades in last 14 days\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not fetch filled orders: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaKYNmKdOwOt"
      },
      "outputs": [],
      "source": [
        "#--- Export locally & download to your computer (Colab) ---\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "from google.colab import files   #<-- NEW: for browser download\n",
        "import shutil, time, pandas as pd\n",
        "\n",
        "#Drive root (same as before, to read your results)\n",
        "ROOT = Path(\"/content/drive/MyDrive/AlpacaPaper\")\n",
        "TODAY = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "#Original sources in Drive (unchanged)\n",
        "SRC_RESULTS = ROOT / \"results\" / TODAY         #e.g., /.../results/2025-10-13\n",
        "SRC_EXPORT  = ROOT / \"results_export\" / TODAY  #rescue export folder (if used)\n",
        "\n",
        "#=== CHANGE: write/export to LOCAL staging (in Colab VM), not Drive ===\n",
        "DEST = Path(\"/content\") / \"exports\" / f\"{TODAY}_export\"\n",
        "DEST.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def copy_all(src_dir, dest_dir):\n",
        "    if src_dir.exists():\n",
        "        for p in src_dir.glob(\"*\"):\n",
        "            if p.is_file():\n",
        "                shutil.copy2(p, dest_dir / p.name)\n",
        "                print(\"Copied:\", p.name, \"from\", src_dir.name)\n",
        "    else:\n",
        "        print(\"Missing source:\", src_dir)\n",
        "\n",
        "#Copy from both possible sources into local /content/exports/<today>_export\n",
        "copy_all(SRC_RESULTS, DEST)\n",
        "copy_all(SRC_EXPORT, DEST)\n",
        "\n",
        "#Build/refresh trade_log_master.csv from per-symbol logs (in LOCAL DEST)\n",
        "sym_logs = list(DEST.glob(\"trade_log_*.csv\"))\n",
        "if sym_logs:\n",
        "    frames = []\n",
        "    for p in sym_logs:\n",
        "        try:\n",
        "            df = pd.read_csv(p)\n",
        "            df[\"symbol_file\"] = p.stem.replace(\"trade_log_\", \"\")\n",
        "            frames.append(df)\n",
        "        except Exception as e:\n",
        "            print(\"Skip\", p.name, \"->\", e)\n",
        "    if frames:\n",
        "        master = pd.concat(frames, ignore_index=True, sort=False)\n",
        "        master_path = DEST / \"trade_log_master.csv\"\n",
        "        master.to_csv(master_path, index=False)\n",
        "        print(\"Wrote:\", master_path)\n",
        "\n",
        "#Zip LOCALLY under /content and trigger a browser download\n",
        "zip_base = Path(\"/content\") / f\"results_{TODAY}_{int(time.time())}\"\n",
        "archive_path = shutil.make_archive(str(zip_base), \"zip\", DEST)\n",
        "archive_path = str(Path(archive_path))  #ensure string for files.download\n",
        "\n",
        "print(\"ZIP ->\", archive_path)\n",
        "\n",
        "#OPTIONAL: also keep a copy in Drive (uncomment if wanted)\n",
        "#shutil.copy2(archive_path, ROOT / \"results\" / Path(archive_path).name)\n",
        "\n",
        "#Prompt download to your computer\n",
        "files.download(archive_path)\n",
        "\n",
        "#Show what's in the LOCAL export folder\n",
        "print(\"\\nLocal export now contains:\")\n",
        "for p in sorted(DEST.iterdir()):\n",
        "    print(\" -\", p.name)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}